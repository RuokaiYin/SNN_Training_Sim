{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251e3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b07e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33e21ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PoissonGen(inp, rescale_fac=2.0):\n",
    "    rand_inp = torch.rand_like(inp)\n",
    "    return torch.mul(torch.le(rand_inp * rescale_fac, torch.abs(inp)).float(), torch.sign(inp))\n",
    "\n",
    "def spike_function(x, k):\n",
    "    x[x>0] = 1\n",
    "    x[x<=0] = 0\n",
    "    return x\n",
    "\n",
    "def de_func(U,th):\n",
    "    alpha = 0.4\n",
    "    U = alpha*(1.0 - abs((U-th)/th))\n",
    "    U[U<0]=0\n",
    "    return U\n",
    "\n",
    "def test(toy):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    toy = toy.cuda()\n",
    "    for data, target in test_loader:\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "        output = toy(data)\n",
    "        test_loss +=F.cross_entropy(output, target, size_average=False).item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "def quant(input, k):\n",
    "    size = input.size()\n",
    "    #mean = torch.mean(input.abs(), 1, keepdim=True)\n",
    "    x = input\n",
    "    #print(x)\n",
    "    xmax = x.abs().max()\n",
    "    num_bits=k\n",
    "    v0 = 1\n",
    "    v1 = 2\n",
    "    v2 = -0.5\n",
    "    y = k #2.**num_bits - 1.\n",
    "    #print(y)\n",
    "    x = x.add(v0).div(v1)\n",
    "    #print(x)\n",
    "    x = x.mul(y).round_()\n",
    "    #print(x)\n",
    "    x = x.div(y)\n",
    "    #print(x)\n",
    "    x = x.add(v2)\n",
    "    #print(x)\n",
    "    x = x.mul(v1)\n",
    "    #print(x)\n",
    "    input = x\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d766c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, time_step,leak):\n",
    "        super(model, self).__init__()\n",
    "        \n",
    "       \n",
    "        self.fc_1 = nn.Linear(28*28,256,bias=False)\n",
    "        self.fc_2 = nn.Linear(256,256,bias=False)\n",
    "        self.fc_out = nn.Linear(256,10,bias=False)\n",
    "        \n",
    "        self.lif1 = LIF(time_step,leak)\n",
    "        self.lif2 = LIF(time_step,leak)\n",
    "        self.time_step = time_step\n",
    "        self.s_regs_inp = None\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        inp = inp.view(inp.shape[0],-1)\n",
    "        size = inp.shape\n",
    "        self.s_regs_inp = torch.zeros(self.time_step,*size, device=device)\n",
    "        u_out = 0\n",
    "        \n",
    "        for t in range(self.time_step):\n",
    "            \n",
    "            spike_inp = PoissonGen(inp)\n",
    "            self.s_regs_inp[t] += spike_inp \n",
    "            \n",
    "            x = self.fc_1(spike_inp)\n",
    "            #x = quant(x,2**4)\n",
    "            x = self.lif1(x, t)\n",
    "            x = self.fc_2(x)\n",
    "            #x = quant(x,2**4)\n",
    "            x = self.lif2(x, t)\n",
    "            x = self.fc_out(x)\n",
    "            u_out = u_out + x\n",
    "        return u_out/self.time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8d72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,time_step,leak):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(28*28,512,bias=False)\n",
    "        self.fc_out = nn.Linear(512,10,bias=False)\n",
    "        self.lif1 = LIF(time_step,leak)\n",
    "        self.time_step = time_step\n",
    "        self.s_regs_inp = None\n",
    "        \n",
    "    def forward(self, inp):\n",
    "#         print(\"size is:\", (inp.view(inp.shape[0],1,28,28)).shape)\n",
    "        inp = inp.view(inp.shape[0],-1)\n",
    "        size = inp.shape\n",
    "        \n",
    "        self.s_regs_inp = torch.zeros(self.time_step,*size, device=device)\n",
    "        u_out = 0\n",
    "        \n",
    "        for t in range(self.time_step):\n",
    "            spike_inp = PoissonGen(inp)\n",
    "            self.s_regs_inp[t] += spike_inp \n",
    "            x = self.fc_1(spike_inp)\n",
    "            #x = quant(x,2**4)\n",
    "            x = self.lif1(x, t)\n",
    "            x = self.fc_out(x)\n",
    "            u_out = u_out + x\n",
    "        return u_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28765ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_5(nn.Module):\n",
    "    def __init__(self,time_step):\n",
    "        super(VGG_5, self).__init__()\n",
    "        \n",
    "        self.time_step = time_step\n",
    "        self.s_regs_inp = None\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.lif1 = LIF(time_step)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.lif2 = LIF(time_step)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.lif3 = LIF(time_step)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 7 * 7, 1024, bias=False)\n",
    "        self.lif4 = LIF(time_step)\n",
    "        self.fc_out = nn.Linear(1024, 10, bias=False)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \n",
    "#         inp = inp.view(inp.shape[0],-1)\n",
    "        size = inp.shape\n",
    "        self.s_regs_inp = torch.zeros(self.time_step,*size, device=device)\n",
    "        u_out = 0\n",
    "        for t in range(self.time_step):\n",
    "            spike_inp = PoissonGen(inp)\n",
    "            self.s_regs_inp[t] += spike_inp \n",
    "            x = self.conv1(spike_inp)\n",
    "            x = self.lif1(x,t)\n",
    "            x = self.pool1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.lif2(x,t)\n",
    "            x = self.conv3(x)\n",
    "            x = self.lif3(x,t)\n",
    "            x = self.pool2(x)\n",
    "            x = x.view(x.shape[0],-1)\n",
    "            print(x.shape)\n",
    "            x = self.fc1(x)\n",
    "            x = self.lif4(x,t)\n",
    "            x = self.fc_out(x)\n",
    "            u_out = u_out + x\n",
    "        return u_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adf32d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_1(nn.Module):\n",
    "    def __init__(self,time_step):\n",
    "        super(VGG_5, self).__init__()\n",
    "        \n",
    "        self.time_step = time_step\n",
    "        self.s_regs_inp = None\n",
    "        self.s_regs_conv = None\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.lif_conv1 = LIF(time_step)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2,return_indices=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 14 * 14, 1024, bias=False)\n",
    "        self.lif_fc1 = LIF(time_step)\n",
    "        self.fc_out = nn.Linear(1024, 10, bias=False)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "\n",
    "        size = inp.shape\n",
    "        self.s_regs_inp = torch.zeros(self.time_step,*size, device=device)\n",
    "        \n",
    "        u_out = 0\n",
    "        for t in range(self.time_step):\n",
    "            spike_inp = PoissonGen(inp)\n",
    "            self.s_regs_inp[t] += spike_inp \n",
    "            x = self.conv1(spike_inp)\n",
    "            x = self.lif_conv1(x,t)\n",
    "            x = self.pool1(x)\n",
    "            x = x.view(x.shape[0],-1)\n",
    "            if t == 0:\n",
    "                self.s_regs_conv = torch.zeros(self.time_step,*x.shape, device=device)\n",
    "            self.s_regs_conv[t] += x\n",
    "            x = self.fc1(x)\n",
    "            x = self.lif_fc1(x,t)\n",
    "            x = self.fc_out(x)\n",
    "            u_out = u_out + x\n",
    "        return u_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31ffe4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_VGG1(vgg,leak,time_step,du_out,s_regs_conv,l_r,th):\n",
    "    \n",
    "    ## First fc\n",
    "    du_fc1 = torch.matmul(du_out,vgg.fc_out.weight)*de_func(vgg.lif_fc1.u_regs[-1],th)\n",
    "    vgg.lif_fc1.du_regs[-1] += du_fc1\n",
    "    \n",
    "    ## Update weight\n",
    "    w_conv_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_conv[-1])\n",
    "    vgg.fc_1.weight.data -= l_r*w_conv_1\n",
    "    \n",
    "    w_1_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif_fc1.s_regs[-1])\n",
    "    vgg.fc_out.weight.data -= l_r*w_1_out\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d590741d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  0.,  0.,  0.],\n",
       "          [ 0.,  6.,  0.,  8.],\n",
       "          [ 0.,  0.,  0.,  0.],\n",
       "          [ 0., 14.,  0., 16.]]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "unpool = nn.MaxUnpool2d(2, stride=2)\n",
    "input = torch.tensor([[[[ 1.,  2,  3,  4],\n",
    "                            [ 5,  6,  7,  8],\n",
    "                            [ 9, 10, 11, 12],\n",
    "                            [13, 14, 15, 16]]]])\n",
    "output, indices = pool(input)\n",
    "\n",
    "unpool(output,indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a73518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e6f78f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIF(nn.Module):\n",
    "    def __init__(self, time_step,leak):\n",
    "        super(LIF, self).__init__()\n",
    "        \n",
    "        self.u_regs = None\n",
    "        self.du_regs = None\n",
    "        self.s_regs = None\n",
    "        self.leak = leak\n",
    "        self.time_step = time_step\n",
    "        self.thresh = 0.5\n",
    "        \n",
    "    def forward(self,inp,t):\n",
    "        if t == 0:\n",
    "            size = inp.shape\n",
    "            self.u_regs = torch.zeros(self.time_step,*size, device=device)\n",
    "            self.du_regs = torch.zeros(self.time_step,*size, device=device)\n",
    "            #inp = inp + torch.norm(inp)\n",
    "            err = torch.normal(0, 0.1,(1,1)).cuda()\n",
    "            inp = inp + err\n",
    "            self.u_regs[0] = quant(inp,2**4)\n",
    "            self.s_regs = torch.zeros(self.time_step,*size, device=device)\n",
    "\n",
    "            vol = inp - self.thresh\n",
    "\n",
    "            spike = spike_function(vol, k=1)\n",
    "\n",
    "            self.s_regs[0] = spike\n",
    "        else:\n",
    "            err = torch.normal(0, 0.1,(1,1)).cuda()\n",
    "            inp = inp + err\n",
    "            self.u_regs[t] = quant(self.leak * self.u_regs[t-1] * (1 - self.s_regs[t-1]) + (1-self.leak)*inp, 2**4)     \n",
    "            vol = self.u_regs[t] - self.thresh\n",
    "\n",
    "            spike = spike_function(vol, k=1)\n",
    "\n",
    "            self.s_regs[t] = spike\n",
    "        return spike\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8fc2dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Back propagation for MLP\n",
    "def bp_MLP(toy,leak,time_step,du_out,s_regs_inp,l_r,th):\n",
    "    \n",
    "    ## First fc\n",
    "    du_fc1 = torch.matmul(du_out,toy.fc_out.weight)*de_func(toy.lif1.u_regs[-1],th)\n",
    "    toy.lif1.du_regs[-1] += du_fc1\n",
    "\n",
    "    ## Update weight\n",
    "    w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[-1])\n",
    "    toy.fc_1.weight.data -= l_r*quant(w_inp_1,2**4)\n",
    "#     toy.fc_1.weight.data -= l_r*w_inp_1\n",
    "    \n",
    "    w_1_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif1.s_regs[-1])\n",
    "    toy.fc_out.weight.data -= l_r*quant(w_1_out,2**4)\n",
    "#     toy.fc_out.weight.data -= l_r*w_1_out\n",
    "\n",
    "    for t in range(time_step-2,-1,-1):\n",
    "        \n",
    "        ## First fc\n",
    "        ds_fc1 = torch.matmul(du_out,toy.fc_out.weight)+toy.lif1.du_regs[t+1]*(-leak*toy.lif1.u_regs[t])\n",
    "        du_fc1 = (ds_fc1)*de_func(toy.lif1.u_regs[t],th) + toy.lif1.du_regs[t+1]*leak*(1-toy.lif1.s_regs[t])\n",
    "        toy.lif1.du_regs[t] += du_fc1\n",
    "\n",
    "        ## Update weight\n",
    "        w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[t])\n",
    "        toy.fc_1.weight.data -= l_r*quant(w_inp_1,2**4)\n",
    "        #toy.fc_1.weight.data -= l_r*w_inp_1\n",
    "\n",
    "        w_1_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif1.s_regs[t])\n",
    "        toy.fc_out.weight.data -= l_r*quant(w_1_out,2**4)\n",
    "#         toy.fc_out.weight.data -= l_r*w_1_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b08084b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Back propagation\n",
    "def bp(toy,leak,time_step,du_out,s_regs_inp,l_r,th):\n",
    "    \n",
    "    ## Second fc    \n",
    "    du_fc2 = torch.matmul(du_out,toy.fc_out.weight)*de_func(toy.lif2.u_regs[-1],th)    \n",
    "    toy.lif2.du_regs[-1] = toy.lif2.du_regs[-1] + du_fc2\n",
    "    \n",
    "    ## First fc\n",
    "    du_fc1 = torch.matmul(du_fc2,toy.fc_2.weight)*de_func(toy.lif1.u_regs[-1],th)\n",
    "    toy.lif1.du_regs[-1] += du_fc1\n",
    "\n",
    "    \n",
    "    ## Update weight\n",
    "    w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[-1])\n",
    "    toy.fc_1.weight.data -= l_r*quant(w_inp_1,2**4)\n",
    "    #toy.fc_1.weight.data -= l_r*w_inp_1\n",
    "\n",
    "    w_1_2 = torch.matmul(torch.transpose(du_fc2,0,1),toy.lif1.s_regs[-1])\n",
    "    toy.fc_2.weight.data -= l_r*quant(w_1_2,2**4)\n",
    "    #toy.fc_2.weight.data -= l_r*w_1_2\n",
    "\n",
    "    w_2_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif2.s_regs[-1])\n",
    "    toy.fc_out.weight.data -= l_r*quant(w_2_out,2**4)\n",
    "    #toy.fc_out.weight.data -= l_r*w_2_out\n",
    "\n",
    "    for t in range(time_step-2,-1,-1):\n",
    "\n",
    "        ds_fc2 = torch.matmul(du_out,toy.fc_out.weight)+toy.lif2.du_regs[t+1]*(-leak*toy.lif2.u_regs[t])\n",
    "        du_fc2 = (ds_fc2)*de_func(toy.lif2.u_regs[t],th) + toy.lif2.du_regs[t+1]*leak*(1-toy.lif2.s_regs[t])\n",
    "        toy.lif2.du_regs[t] += du_fc2\n",
    "        \n",
    "        ## First fc\n",
    "        ds_fc1 = torch.matmul(du_fc2,toy.fc_2.weight)+toy.lif1.du_regs[t+1]*(-leak*toy.lif1.u_regs[t])\n",
    "        du_fc1 = (ds_fc1)*de_func(toy.lif1.u_regs[t],th) + toy.lif1.du_regs[t+1]*leak*(1-toy.lif1.s_regs[t])\n",
    "        toy.lif1.du_regs[t] += du_fc1\n",
    "\n",
    "        ## Update weight\n",
    "        w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[t])\n",
    "        toy.fc_1.weight.data -= l_r*quant(w_inp_1,2**4)\n",
    "        #toy.fc_1.weight.data -= l_r*w_inp_1\n",
    "\n",
    "        w_1_2 = torch.matmul(torch.transpose(du_fc2,0,1),toy.lif1.s_regs[t])\n",
    "        toy.fc_2.weight.data -= l_r*quant(w_1_2,2**4)\n",
    "        #toy.fc_2.weight.data -= l_r*w_1_2\n",
    "\n",
    "        w_2_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif2.s_regs[t])\n",
    "        toy.fc_out.weight.data -= l_r*quant(w_2_out,2**4)\n",
    "        #toy.fc_out.weight.data -= l_r*w_2_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c3de0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "batch_size_train = 216\n",
    "batch_size_test = 1000\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b70c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(5 + 1)]\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2ce9ca7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ruokai\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 3.1644, Accuracy: 1041/10000 (10%)\n",
      "\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 3.142369\n",
      "Train Epoch: 0 [2160/60000 (4%)]\tLoss: 0.685846\n",
      "Train Epoch: 0 [4320/60000 (7%)]\tLoss: 0.589213\n",
      "Train Epoch: 0 [6480/60000 (11%)]\tLoss: 0.530093\n",
      "Train Epoch: 0 [8640/60000 (14%)]\tLoss: 0.613660\n",
      "Train Epoch: 0 [10800/60000 (18%)]\tLoss: 0.383935\n",
      "Train Epoch: 0 [12960/60000 (22%)]\tLoss: 0.370530\n",
      "Train Epoch: 0 [15120/60000 (25%)]\tLoss: 0.466322\n",
      "Train Epoch: 0 [17280/60000 (29%)]\tLoss: 0.409320\n",
      "Train Epoch: 0 [19440/60000 (32%)]\tLoss: 0.345394\n",
      "Train Epoch: 0 [21600/60000 (36%)]\tLoss: 0.334662\n",
      "Train Epoch: 0 [23760/60000 (40%)]\tLoss: 0.267120\n",
      "Train Epoch: 0 [25920/60000 (43%)]\tLoss: 0.229792\n",
      "Train Epoch: 0 [28080/60000 (47%)]\tLoss: 0.194602\n",
      "Train Epoch: 0 [30240/60000 (50%)]\tLoss: 0.246486\n",
      "Train Epoch: 0 [32400/60000 (54%)]\tLoss: 0.136865\n",
      "Train Epoch: 0 [34560/60000 (58%)]\tLoss: 0.169419\n",
      "Train Epoch: 0 [36720/60000 (61%)]\tLoss: 0.213000\n",
      "Train Epoch: 0 [38880/60000 (65%)]\tLoss: 0.100269\n",
      "Train Epoch: 0 [41040/60000 (68%)]\tLoss: 0.265812\n",
      "Train Epoch: 0 [43200/60000 (72%)]\tLoss: 0.178213\n",
      "Train Epoch: 0 [45360/60000 (76%)]\tLoss: 0.185332\n",
      "Train Epoch: 0 [47520/60000 (79%)]\tLoss: 0.107007\n",
      "Train Epoch: 0 [49680/60000 (83%)]\tLoss: 0.074092\n",
      "Train Epoch: 0 [51840/60000 (86%)]\tLoss: 0.107110\n",
      "Train Epoch: 0 [54000/60000 (90%)]\tLoss: 0.270733\n",
      "Train Epoch: 0 [56160/60000 (94%)]\tLoss: 0.110492\n",
      "Train Epoch: 0 [58320/60000 (97%)]\tLoss: 0.185086\n",
      "\n",
      "Test set: Avg. loss: 0.1602, Accuracy: 9507/10000 (95%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.166193\n",
      "Train Epoch: 1 [2160/60000 (4%)]\tLoss: 0.073672\n",
      "Train Epoch: 1 [4320/60000 (7%)]\tLoss: 0.097359\n",
      "Train Epoch: 1 [6480/60000 (11%)]\tLoss: 0.290933\n",
      "Train Epoch: 1 [8640/60000 (14%)]\tLoss: 0.148304\n",
      "Train Epoch: 1 [10800/60000 (18%)]\tLoss: 0.097249\n",
      "Train Epoch: 1 [12960/60000 (22%)]\tLoss: 0.198546\n",
      "Train Epoch: 1 [15120/60000 (25%)]\tLoss: 0.109299\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.191504\n",
      "Train Epoch: 1 [19440/60000 (32%)]\tLoss: 0.204966\n",
      "Train Epoch: 1 [21600/60000 (36%)]\tLoss: 0.158082\n",
      "Train Epoch: 1 [23760/60000 (40%)]\tLoss: 0.155955\n",
      "Train Epoch: 1 [25920/60000 (43%)]\tLoss: 0.142514\n",
      "Train Epoch: 1 [28080/60000 (47%)]\tLoss: 0.141709\n",
      "Train Epoch: 1 [30240/60000 (50%)]\tLoss: 0.103217\n",
      "Train Epoch: 1 [32400/60000 (54%)]\tLoss: 0.097899\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.157337\n",
      "Train Epoch: 1 [36720/60000 (61%)]\tLoss: 0.226978\n",
      "Train Epoch: 1 [38880/60000 (65%)]\tLoss: 0.142924\n",
      "Train Epoch: 1 [41040/60000 (68%)]\tLoss: 0.142903\n",
      "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 0.130890\n",
      "Train Epoch: 1 [45360/60000 (76%)]\tLoss: 0.130975\n",
      "Train Epoch: 1 [47520/60000 (79%)]\tLoss: 0.143228\n",
      "Train Epoch: 1 [49680/60000 (83%)]\tLoss: 0.160467\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.054534\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.146370\n",
      "Train Epoch: 1 [56160/60000 (94%)]\tLoss: 0.182028\n",
      "Train Epoch: 1 [58320/60000 (97%)]\tLoss: 0.093642\n",
      "\n",
      "Test set: Avg. loss: 0.1189, Accuracy: 9609/10000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.063829\n",
      "Train Epoch: 2 [2160/60000 (4%)]\tLoss: 0.087252\n",
      "Train Epoch: 2 [4320/60000 (7%)]\tLoss: 0.103615\n",
      "Train Epoch: 2 [6480/60000 (11%)]\tLoss: 0.078204\n",
      "Train Epoch: 2 [8640/60000 (14%)]\tLoss: 0.096838\n",
      "Train Epoch: 2 [10800/60000 (18%)]\tLoss: 0.077850\n",
      "Train Epoch: 2 [12960/60000 (22%)]\tLoss: 0.164199\n",
      "Train Epoch: 2 [15120/60000 (25%)]\tLoss: 0.082629\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.156814\n",
      "Train Epoch: 2 [19440/60000 (32%)]\tLoss: 0.156571\n",
      "Train Epoch: 2 [21600/60000 (36%)]\tLoss: 0.115250\n",
      "Train Epoch: 2 [23760/60000 (40%)]\tLoss: 0.184136\n",
      "Train Epoch: 2 [25920/60000 (43%)]\tLoss: 0.092383\n",
      "Train Epoch: 2 [28080/60000 (47%)]\tLoss: 0.147986\n",
      "Train Epoch: 2 [30240/60000 (50%)]\tLoss: 0.074383\n",
      "Train Epoch: 2 [32400/60000 (54%)]\tLoss: 0.102438\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.122446\n",
      "Train Epoch: 2 [36720/60000 (61%)]\tLoss: 0.107931\n",
      "Train Epoch: 2 [38880/60000 (65%)]\tLoss: 0.042668\n",
      "Train Epoch: 2 [41040/60000 (68%)]\tLoss: 0.110268\n",
      "Train Epoch: 2 [43200/60000 (72%)]\tLoss: 0.071903\n",
      "Train Epoch: 2 [45360/60000 (76%)]\tLoss: 0.118143\n",
      "Train Epoch: 2 [47520/60000 (79%)]\tLoss: 0.136804\n",
      "Train Epoch: 2 [49680/60000 (83%)]\tLoss: 0.068729\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.139072\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.122366\n",
      "Train Epoch: 2 [56160/60000 (94%)]\tLoss: 0.129145\n",
      "Train Epoch: 2 [58320/60000 (97%)]\tLoss: 0.087140\n",
      "\n",
      "Test set: Avg. loss: 0.1010, Accuracy: 9681/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.060527\n",
      "Train Epoch: 3 [2160/60000 (4%)]\tLoss: 0.054636\n",
      "Train Epoch: 3 [4320/60000 (7%)]\tLoss: 0.079861\n",
      "Train Epoch: 3 [6480/60000 (11%)]\tLoss: 0.046752\n",
      "Train Epoch: 3 [8640/60000 (14%)]\tLoss: 0.062813\n",
      "Train Epoch: 3 [10800/60000 (18%)]\tLoss: 0.076823\n",
      "Train Epoch: 3 [12960/60000 (22%)]\tLoss: 0.122535\n",
      "Train Epoch: 3 [15120/60000 (25%)]\tLoss: 0.110226\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.079446\n",
      "Train Epoch: 3 [19440/60000 (32%)]\tLoss: 0.106100\n",
      "Train Epoch: 3 [21600/60000 (36%)]\tLoss: 0.053924\n",
      "Train Epoch: 3 [23760/60000 (40%)]\tLoss: 0.119847\n",
      "Train Epoch: 3 [25920/60000 (43%)]\tLoss: 0.096131\n",
      "Train Epoch: 3 [28080/60000 (47%)]\tLoss: 0.123867\n",
      "Train Epoch: 3 [30240/60000 (50%)]\tLoss: 0.091441\n",
      "Train Epoch: 3 [32400/60000 (54%)]\tLoss: 0.101403\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.038688\n",
      "Train Epoch: 3 [36720/60000 (61%)]\tLoss: 0.132028\n",
      "Train Epoch: 3 [38880/60000 (65%)]\tLoss: 0.111534\n",
      "Train Epoch: 3 [41040/60000 (68%)]\tLoss: 0.047147\n",
      "Train Epoch: 3 [43200/60000 (72%)]\tLoss: 0.087455\n",
      "Train Epoch: 3 [45360/60000 (76%)]\tLoss: 0.108362\n",
      "Train Epoch: 3 [47520/60000 (79%)]\tLoss: 0.058012\n",
      "Train Epoch: 3 [49680/60000 (83%)]\tLoss: 0.061859\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.100776\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 0.082855\n",
      "Train Epoch: 3 [56160/60000 (94%)]\tLoss: 0.045799\n",
      "Train Epoch: 3 [58320/60000 (97%)]\tLoss: 0.137732\n",
      "\n",
      "Test set: Avg. loss: 0.0960, Accuracy: 9683/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.058617\n",
      "Train Epoch: 4 [2160/60000 (4%)]\tLoss: 0.086713\n",
      "Train Epoch: 4 [4320/60000 (7%)]\tLoss: 0.048727\n",
      "Train Epoch: 4 [6480/60000 (11%)]\tLoss: 0.066565\n",
      "Train Epoch: 4 [8640/60000 (14%)]\tLoss: 0.090908\n",
      "Train Epoch: 4 [10800/60000 (18%)]\tLoss: 0.036654\n",
      "Train Epoch: 4 [12960/60000 (22%)]\tLoss: 0.045344\n",
      "Train Epoch: 4 [15120/60000 (25%)]\tLoss: 0.038853\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.064560\n",
      "Train Epoch: 4 [19440/60000 (32%)]\tLoss: 0.029597\n",
      "Train Epoch: 4 [21600/60000 (36%)]\tLoss: 0.115460\n",
      "Train Epoch: 4 [23760/60000 (40%)]\tLoss: 0.072317\n",
      "Train Epoch: 4 [25920/60000 (43%)]\tLoss: 0.084377\n",
      "Train Epoch: 4 [28080/60000 (47%)]\tLoss: 0.081942\n",
      "Train Epoch: 4 [30240/60000 (50%)]\tLoss: 0.068970\n",
      "Train Epoch: 4 [32400/60000 (54%)]\tLoss: 0.076325\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.105384\n",
      "Train Epoch: 4 [36720/60000 (61%)]\tLoss: 0.036915\n",
      "Train Epoch: 4 [38880/60000 (65%)]\tLoss: 0.029920\n",
      "Train Epoch: 4 [41040/60000 (68%)]\tLoss: 0.084838\n",
      "Train Epoch: 4 [43200/60000 (72%)]\tLoss: 0.077564\n",
      "Train Epoch: 4 [45360/60000 (76%)]\tLoss: 0.043154\n",
      "Train Epoch: 4 [47520/60000 (79%)]\tLoss: 0.032374\n",
      "Train Epoch: 4 [49680/60000 (83%)]\tLoss: 0.045581\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.073400\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 0.074571\n",
      "Train Epoch: 4 [56160/60000 (94%)]\tLoss: 0.195997\n",
      "Train Epoch: 4 [58320/60000 (97%)]\tLoss: 0.067513\n",
      "\n",
      "Test set: Avg. loss: 0.0983, Accuracy: 9688/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.112878\n",
      "Train Epoch: 5 [2160/60000 (4%)]\tLoss: 0.065618\n",
      "Train Epoch: 5 [4320/60000 (7%)]\tLoss: 0.070264\n",
      "Train Epoch: 5 [6480/60000 (11%)]\tLoss: 0.069597\n",
      "Train Epoch: 5 [8640/60000 (14%)]\tLoss: 0.060452\n",
      "Train Epoch: 5 [10800/60000 (18%)]\tLoss: 0.073709\n",
      "Train Epoch: 5 [12960/60000 (22%)]\tLoss: 0.057965\n",
      "Train Epoch: 5 [15120/60000 (25%)]\tLoss: 0.049108\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.086466\n",
      "Train Epoch: 5 [19440/60000 (32%)]\tLoss: 0.060868\n",
      "Train Epoch: 5 [21600/60000 (36%)]\tLoss: 0.143135\n",
      "Train Epoch: 5 [23760/60000 (40%)]\tLoss: 0.040180\n",
      "Train Epoch: 5 [25920/60000 (43%)]\tLoss: 0.026771\n",
      "Train Epoch: 5 [28080/60000 (47%)]\tLoss: 0.045935\n",
      "Train Epoch: 5 [30240/60000 (50%)]\tLoss: 0.064042\n",
      "Train Epoch: 5 [32400/60000 (54%)]\tLoss: 0.069650\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.048010\n",
      "Train Epoch: 5 [36720/60000 (61%)]\tLoss: 0.072364\n",
      "Train Epoch: 5 [38880/60000 (65%)]\tLoss: 0.038513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [41040/60000 (68%)]\tLoss: 0.060155\n",
      "Train Epoch: 5 [43200/60000 (72%)]\tLoss: 0.050306\n",
      "Train Epoch: 5 [45360/60000 (76%)]\tLoss: 0.069296\n",
      "Train Epoch: 5 [47520/60000 (79%)]\tLoss: 0.096938\n",
      "Train Epoch: 5 [49680/60000 (83%)]\tLoss: 0.083694\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.073268\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 0.044273\n",
      "Train Epoch: 5 [56160/60000 (94%)]\tLoss: 0.053045\n",
      "Train Epoch: 5 [58320/60000 (97%)]\tLoss: 0.108259\n",
      "\n",
      "Test set: Avg. loss: 0.0903, Accuracy: 9705/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.044617\n",
      "Train Epoch: 6 [2160/60000 (4%)]\tLoss: 0.046374\n",
      "Train Epoch: 6 [4320/60000 (7%)]\tLoss: 0.055196\n",
      "Train Epoch: 6 [6480/60000 (11%)]\tLoss: 0.017958\n",
      "Train Epoch: 6 [8640/60000 (14%)]\tLoss: 0.033721\n",
      "Train Epoch: 6 [10800/60000 (18%)]\tLoss: 0.037377\n",
      "Train Epoch: 6 [12960/60000 (22%)]\tLoss: 0.111183\n",
      "Train Epoch: 6 [15120/60000 (25%)]\tLoss: 0.041628\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.036906\n",
      "Train Epoch: 6 [19440/60000 (32%)]\tLoss: 0.077576\n",
      "Train Epoch: 6 [21600/60000 (36%)]\tLoss: 0.051665\n",
      "Train Epoch: 6 [23760/60000 (40%)]\tLoss: 0.067246\n",
      "Train Epoch: 6 [25920/60000 (43%)]\tLoss: 0.064762\n",
      "Train Epoch: 6 [28080/60000 (47%)]\tLoss: 0.048618\n",
      "Train Epoch: 6 [30240/60000 (50%)]\tLoss: 0.048663\n",
      "Train Epoch: 6 [32400/60000 (54%)]\tLoss: 0.050983\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.083234\n",
      "Train Epoch: 6 [36720/60000 (61%)]\tLoss: 0.040655\n",
      "Train Epoch: 6 [38880/60000 (65%)]\tLoss: 0.053206\n",
      "Train Epoch: 6 [41040/60000 (68%)]\tLoss: 0.044252\n",
      "Train Epoch: 6 [43200/60000 (72%)]\tLoss: 0.032297\n",
      "Train Epoch: 6 [45360/60000 (76%)]\tLoss: 0.035087\n",
      "Train Epoch: 6 [47520/60000 (79%)]\tLoss: 0.066332\n",
      "Train Epoch: 6 [49680/60000 (83%)]\tLoss: 0.048723\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.048634\n",
      "Train Epoch: 6 [54000/60000 (90%)]\tLoss: 0.048917\n",
      "Train Epoch: 6 [56160/60000 (94%)]\tLoss: 0.061904\n",
      "Train Epoch: 6 [58320/60000 (97%)]\tLoss: 0.056303\n",
      "\n",
      "Test set: Avg. loss: 0.0794, Accuracy: 9746/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.063894\n",
      "Train Epoch: 7 [2160/60000 (4%)]\tLoss: 0.051307\n",
      "Train Epoch: 7 [4320/60000 (7%)]\tLoss: 0.035096\n",
      "Train Epoch: 7 [6480/60000 (11%)]\tLoss: 0.059123\n",
      "Train Epoch: 7 [8640/60000 (14%)]\tLoss: 0.041039\n",
      "Train Epoch: 7 [10800/60000 (18%)]\tLoss: 0.058028\n",
      "Train Epoch: 7 [12960/60000 (22%)]\tLoss: 0.036147\n",
      "Train Epoch: 7 [15120/60000 (25%)]\tLoss: 0.032013\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.039364\n",
      "Train Epoch: 7 [19440/60000 (32%)]\tLoss: 0.036667\n",
      "Train Epoch: 7 [21600/60000 (36%)]\tLoss: 0.020454\n",
      "Train Epoch: 7 [23760/60000 (40%)]\tLoss: 0.028786\n",
      "Train Epoch: 7 [25920/60000 (43%)]\tLoss: 0.055860\n",
      "Train Epoch: 7 [28080/60000 (47%)]\tLoss: 0.070416\n",
      "Train Epoch: 7 [30240/60000 (50%)]\tLoss: 0.045827\n",
      "Train Epoch: 7 [32400/60000 (54%)]\tLoss: 0.062204\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.103464\n",
      "Train Epoch: 7 [36720/60000 (61%)]\tLoss: 0.085356\n",
      "Train Epoch: 7 [38880/60000 (65%)]\tLoss: 0.044682\n",
      "Train Epoch: 7 [41040/60000 (68%)]\tLoss: 0.034948\n",
      "Train Epoch: 7 [43200/60000 (72%)]\tLoss: 0.033216\n",
      "Train Epoch: 7 [45360/60000 (76%)]\tLoss: 0.035979\n",
      "Train Epoch: 7 [47520/60000 (79%)]\tLoss: 0.059888\n",
      "Train Epoch: 7 [49680/60000 (83%)]\tLoss: 0.033422\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.022448\n",
      "Train Epoch: 7 [54000/60000 (90%)]\tLoss: 0.032560\n",
      "Train Epoch: 7 [56160/60000 (94%)]\tLoss: 0.075277\n",
      "Train Epoch: 7 [58320/60000 (97%)]\tLoss: 0.041562\n",
      "\n",
      "Test set: Avg. loss: 0.0858, Accuracy: 9717/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.103755\n",
      "Train Epoch: 8 [2160/60000 (4%)]\tLoss: 0.058282\n",
      "Train Epoch: 8 [4320/60000 (7%)]\tLoss: 0.082835\n",
      "Train Epoch: 8 [6480/60000 (11%)]\tLoss: 0.024900\n",
      "Train Epoch: 8 [8640/60000 (14%)]\tLoss: 0.048872\n",
      "Train Epoch: 8 [10800/60000 (18%)]\tLoss: 0.028984\n",
      "Train Epoch: 8 [12960/60000 (22%)]\tLoss: 0.042327\n",
      "Train Epoch: 8 [15120/60000 (25%)]\tLoss: 0.062379\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.026539\n",
      "Train Epoch: 8 [19440/60000 (32%)]\tLoss: 0.046260\n",
      "Train Epoch: 8 [21600/60000 (36%)]\tLoss: 0.053305\n",
      "Train Epoch: 8 [23760/60000 (40%)]\tLoss: 0.036486\n",
      "Train Epoch: 8 [25920/60000 (43%)]\tLoss: 0.051200\n",
      "Train Epoch: 8 [28080/60000 (47%)]\tLoss: 0.065261\n",
      "Train Epoch: 8 [30240/60000 (50%)]\tLoss: 0.062519\n",
      "Train Epoch: 8 [32400/60000 (54%)]\tLoss: 0.053207\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.028663\n",
      "Train Epoch: 8 [36720/60000 (61%)]\tLoss: 0.036870\n",
      "Train Epoch: 8 [38880/60000 (65%)]\tLoss: 0.015851\n",
      "Train Epoch: 8 [41040/60000 (68%)]\tLoss: 0.038327\n",
      "Train Epoch: 8 [43200/60000 (72%)]\tLoss: 0.050027\n",
      "Train Epoch: 8 [45360/60000 (76%)]\tLoss: 0.020489\n",
      "Train Epoch: 8 [47520/60000 (79%)]\tLoss: 0.031130\n",
      "Train Epoch: 8 [49680/60000 (83%)]\tLoss: 0.048581\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.083838\n",
      "Train Epoch: 8 [54000/60000 (90%)]\tLoss: 0.028347\n",
      "Train Epoch: 8 [56160/60000 (94%)]\tLoss: 0.019725\n",
      "Train Epoch: 8 [58320/60000 (97%)]\tLoss: 0.070567\n",
      "\n",
      "Test set: Avg. loss: 0.0768, Accuracy: 9770/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.032473\n",
      "Train Epoch: 9 [2160/60000 (4%)]\tLoss: 0.028296\n",
      "Train Epoch: 9 [4320/60000 (7%)]\tLoss: 0.027075\n",
      "Train Epoch: 9 [6480/60000 (11%)]\tLoss: 0.015214\n",
      "Train Epoch: 9 [8640/60000 (14%)]\tLoss: 0.110830\n",
      "Train Epoch: 9 [10800/60000 (18%)]\tLoss: 0.042516\n",
      "Train Epoch: 9 [12960/60000 (22%)]\tLoss: 0.045509\n",
      "Train Epoch: 9 [15120/60000 (25%)]\tLoss: 0.049942\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.023479\n",
      "Train Epoch: 9 [19440/60000 (32%)]\tLoss: 0.024045\n",
      "Train Epoch: 9 [21600/60000 (36%)]\tLoss: 0.048038\n",
      "Train Epoch: 9 [23760/60000 (40%)]\tLoss: 0.031656\n",
      "Train Epoch: 9 [25920/60000 (43%)]\tLoss: 0.072369\n",
      "Train Epoch: 9 [28080/60000 (47%)]\tLoss: 0.050333\n",
      "Train Epoch: 9 [30240/60000 (50%)]\tLoss: 0.073615\n",
      "Train Epoch: 9 [32400/60000 (54%)]\tLoss: 0.025631\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.030378\n",
      "Train Epoch: 9 [36720/60000 (61%)]\tLoss: 0.024537\n",
      "Train Epoch: 9 [38880/60000 (65%)]\tLoss: 0.020164\n",
      "Train Epoch: 9 [41040/60000 (68%)]\tLoss: 0.023698\n",
      "Train Epoch: 9 [43200/60000 (72%)]\tLoss: 0.026725\n",
      "Train Epoch: 9 [45360/60000 (76%)]\tLoss: 0.023777\n",
      "Train Epoch: 9 [47520/60000 (79%)]\tLoss: 0.053202\n",
      "Train Epoch: 9 [49680/60000 (83%)]\tLoss: 0.026210\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.041660\n",
      "Train Epoch: 9 [54000/60000 (90%)]\tLoss: 0.026069\n",
      "Train Epoch: 9 [56160/60000 (94%)]\tLoss: 0.017125\n",
      "Train Epoch: 9 [58320/60000 (97%)]\tLoss: 0.035881\n",
      "\n",
      "Test set: Avg. loss: 0.0817, Accuracy: 9744/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.034103\n",
      "Train Epoch: 10 [2160/60000 (4%)]\tLoss: 0.035264\n",
      "Train Epoch: 10 [4320/60000 (7%)]\tLoss: 0.020502\n",
      "Train Epoch: 10 [6480/60000 (11%)]\tLoss: 0.042086\n",
      "Train Epoch: 10 [8640/60000 (14%)]\tLoss: 0.026005\n",
      "Train Epoch: 10 [10800/60000 (18%)]\tLoss: 0.016410\n",
      "Train Epoch: 10 [12960/60000 (22%)]\tLoss: 0.047360\n",
      "Train Epoch: 10 [15120/60000 (25%)]\tLoss: 0.053086\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.010403\n",
      "Train Epoch: 10 [19440/60000 (32%)]\tLoss: 0.022868\n",
      "Train Epoch: 10 [21600/60000 (36%)]\tLoss: 0.028962\n",
      "Train Epoch: 10 [23760/60000 (40%)]\tLoss: 0.033574\n",
      "Train Epoch: 10 [25920/60000 (43%)]\tLoss: 0.026417\n",
      "Train Epoch: 10 [28080/60000 (47%)]\tLoss: 0.053560\n",
      "Train Epoch: 10 [30240/60000 (50%)]\tLoss: 0.042565\n",
      "Train Epoch: 10 [32400/60000 (54%)]\tLoss: 0.033654\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.033303\n",
      "Train Epoch: 10 [36720/60000 (61%)]\tLoss: 0.031014\n",
      "Train Epoch: 10 [38880/60000 (65%)]\tLoss: 0.064975\n",
      "Train Epoch: 10 [41040/60000 (68%)]\tLoss: 0.018886\n",
      "Train Epoch: 10 [43200/60000 (72%)]\tLoss: 0.085168\n",
      "Train Epoch: 10 [45360/60000 (76%)]\tLoss: 0.012549\n",
      "Train Epoch: 10 [47520/60000 (79%)]\tLoss: 0.051250\n",
      "Train Epoch: 10 [49680/60000 (83%)]\tLoss: 0.039725\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.028538\n",
      "Train Epoch: 10 [54000/60000 (90%)]\tLoss: 0.056459\n",
      "Train Epoch: 10 [56160/60000 (94%)]\tLoss: 0.032072\n",
      "Train Epoch: 10 [58320/60000 (97%)]\tLoss: 0.023335\n",
      "\n",
      "Test set: Avg. loss: 0.0796, Accuracy: 9765/10000 (98%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.043517\n",
      "Train Epoch: 11 [2160/60000 (4%)]\tLoss: 0.055596\n",
      "Train Epoch: 11 [4320/60000 (7%)]\tLoss: 0.012807\n",
      "Train Epoch: 11 [6480/60000 (11%)]\tLoss: 0.007816\n",
      "Train Epoch: 11 [8640/60000 (14%)]\tLoss: 0.028575\n",
      "Train Epoch: 11 [10800/60000 (18%)]\tLoss: 0.018620\n",
      "Train Epoch: 11 [12960/60000 (22%)]\tLoss: 0.024192\n",
      "Train Epoch: 11 [15120/60000 (25%)]\tLoss: 0.028179\n",
      "Train Epoch: 11 [17280/60000 (29%)]\tLoss: 0.036157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [19440/60000 (32%)]\tLoss: 0.016478\n",
      "Train Epoch: 11 [21600/60000 (36%)]\tLoss: 0.037674\n",
      "Train Epoch: 11 [23760/60000 (40%)]\tLoss: 0.020461\n",
      "Train Epoch: 11 [25920/60000 (43%)]\tLoss: 0.036761\n",
      "Train Epoch: 11 [28080/60000 (47%)]\tLoss: 0.021611\n",
      "Train Epoch: 11 [30240/60000 (50%)]\tLoss: 0.023732\n",
      "Train Epoch: 11 [32400/60000 (54%)]\tLoss: 0.028493\n",
      "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 0.012023\n",
      "Train Epoch: 11 [36720/60000 (61%)]\tLoss: 0.033439\n",
      "Train Epoch: 11 [38880/60000 (65%)]\tLoss: 0.028843\n",
      "Train Epoch: 11 [41040/60000 (68%)]\tLoss: 0.031100\n",
      "Train Epoch: 11 [43200/60000 (72%)]\tLoss: 0.028807\n",
      "Train Epoch: 11 [45360/60000 (76%)]\tLoss: 0.029291\n",
      "Train Epoch: 11 [47520/60000 (79%)]\tLoss: 0.064729\n",
      "Train Epoch: 11 [49680/60000 (83%)]\tLoss: 0.017114\n",
      "Train Epoch: 11 [51840/60000 (86%)]\tLoss: 0.028767\n",
      "Train Epoch: 11 [54000/60000 (90%)]\tLoss: 0.021709\n",
      "Train Epoch: 11 [56160/60000 (94%)]\tLoss: 0.030470\n",
      "Train Epoch: 11 [58320/60000 (97%)]\tLoss: 0.037441\n",
      "\n",
      "Test set: Avg. loss: 0.0806, Accuracy: 9763/10000 (98%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.030699\n",
      "Train Epoch: 12 [2160/60000 (4%)]\tLoss: 0.005814\n",
      "Train Epoch: 12 [4320/60000 (7%)]\tLoss: 0.037053\n",
      "Train Epoch: 12 [6480/60000 (11%)]\tLoss: 0.016402\n",
      "Train Epoch: 12 [8640/60000 (14%)]\tLoss: 0.022551\n",
      "Train Epoch: 12 [10800/60000 (18%)]\tLoss: 0.031571\n",
      "Train Epoch: 12 [12960/60000 (22%)]\tLoss: 0.039575\n",
      "Train Epoch: 12 [15120/60000 (25%)]\tLoss: 0.038754\n",
      "Train Epoch: 12 [17280/60000 (29%)]\tLoss: 0.045496\n",
      "Train Epoch: 12 [19440/60000 (32%)]\tLoss: 0.036307\n",
      "Train Epoch: 12 [21600/60000 (36%)]\tLoss: 0.046376\n",
      "Train Epoch: 12 [23760/60000 (40%)]\tLoss: 0.022915\n",
      "Train Epoch: 12 [25920/60000 (43%)]\tLoss: 0.020231\n",
      "Train Epoch: 12 [28080/60000 (47%)]\tLoss: 0.038730\n",
      "Train Epoch: 12 [30240/60000 (50%)]\tLoss: 0.024048\n",
      "Train Epoch: 12 [32400/60000 (54%)]\tLoss: 0.041947\n",
      "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 0.022319\n",
      "Train Epoch: 12 [36720/60000 (61%)]\tLoss: 0.007929\n",
      "Train Epoch: 12 [38880/60000 (65%)]\tLoss: 0.028117\n",
      "Train Epoch: 12 [41040/60000 (68%)]\tLoss: 0.082610\n",
      "Train Epoch: 12 [43200/60000 (72%)]\tLoss: 0.028930\n",
      "Train Epoch: 12 [45360/60000 (76%)]\tLoss: 0.010903\n",
      "Train Epoch: 12 [47520/60000 (79%)]\tLoss: 0.012173\n",
      "Train Epoch: 12 [49680/60000 (83%)]\tLoss: 0.018537\n",
      "Train Epoch: 12 [51840/60000 (86%)]\tLoss: 0.056331\n",
      "Train Epoch: 12 [54000/60000 (90%)]\tLoss: 0.038626\n",
      "Train Epoch: 12 [56160/60000 (94%)]\tLoss: 0.020411\n",
      "Train Epoch: 12 [58320/60000 (97%)]\tLoss: 0.014004\n",
      "\n",
      "Test set: Avg. loss: 0.0784, Accuracy: 9753/10000 (98%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.039533\n",
      "Train Epoch: 13 [2160/60000 (4%)]\tLoss: 0.026253\n",
      "Train Epoch: 13 [4320/60000 (7%)]\tLoss: 0.015235\n",
      "Train Epoch: 13 [6480/60000 (11%)]\tLoss: 0.031512\n",
      "Train Epoch: 13 [8640/60000 (14%)]\tLoss: 0.019453\n",
      "Train Epoch: 13 [10800/60000 (18%)]\tLoss: 0.025034\n",
      "Train Epoch: 13 [12960/60000 (22%)]\tLoss: 0.020810\n",
      "Train Epoch: 13 [15120/60000 (25%)]\tLoss: 0.027380\n",
      "Train Epoch: 13 [17280/60000 (29%)]\tLoss: 0.012195\n",
      "Train Epoch: 13 [19440/60000 (32%)]\tLoss: 0.030729\n",
      "Train Epoch: 13 [21600/60000 (36%)]\tLoss: 0.020695\n",
      "Train Epoch: 13 [23760/60000 (40%)]\tLoss: 0.016949\n",
      "Train Epoch: 13 [25920/60000 (43%)]\tLoss: 0.032208\n",
      "Train Epoch: 13 [28080/60000 (47%)]\tLoss: 0.034264\n",
      "Train Epoch: 13 [30240/60000 (50%)]\tLoss: 0.045505\n",
      "Train Epoch: 13 [32400/60000 (54%)]\tLoss: 0.021885\n",
      "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 0.019134\n",
      "Train Epoch: 13 [36720/60000 (61%)]\tLoss: 0.022917\n",
      "Train Epoch: 13 [38880/60000 (65%)]\tLoss: 0.030113\n",
      "Train Epoch: 13 [41040/60000 (68%)]\tLoss: 0.008106\n",
      "Train Epoch: 13 [43200/60000 (72%)]\tLoss: 0.043127\n",
      "Train Epoch: 13 [45360/60000 (76%)]\tLoss: 0.022015\n",
      "Train Epoch: 13 [47520/60000 (79%)]\tLoss: 0.015069\n",
      "Train Epoch: 13 [49680/60000 (83%)]\tLoss: 0.011963\n",
      "Train Epoch: 13 [51840/60000 (86%)]\tLoss: 0.036412\n",
      "Train Epoch: 13 [54000/60000 (90%)]\tLoss: 0.014385\n",
      "Train Epoch: 13 [56160/60000 (94%)]\tLoss: 0.019344\n",
      "Train Epoch: 13 [58320/60000 (97%)]\tLoss: 0.029831\n",
      "\n",
      "Test set: Avg. loss: 0.0734, Accuracy: 9781/10000 (98%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.037598\n",
      "Train Epoch: 14 [2160/60000 (4%)]\tLoss: 0.010938\n",
      "Train Epoch: 14 [4320/60000 (7%)]\tLoss: 0.010524\n",
      "Train Epoch: 14 [6480/60000 (11%)]\tLoss: 0.021168\n",
      "Train Epoch: 14 [8640/60000 (14%)]\tLoss: 0.026748\n",
      "Train Epoch: 14 [10800/60000 (18%)]\tLoss: 0.013799\n",
      "Train Epoch: 14 [12960/60000 (22%)]\tLoss: 0.014907\n",
      "Train Epoch: 14 [15120/60000 (25%)]\tLoss: 0.043036\n",
      "Train Epoch: 14 [17280/60000 (29%)]\tLoss: 0.025151\n",
      "Train Epoch: 14 [19440/60000 (32%)]\tLoss: 0.042601\n",
      "Train Epoch: 14 [21600/60000 (36%)]\tLoss: 0.008806\n",
      "Train Epoch: 14 [23760/60000 (40%)]\tLoss: 0.013397\n",
      "Train Epoch: 14 [25920/60000 (43%)]\tLoss: 0.029975\n",
      "Train Epoch: 14 [28080/60000 (47%)]\tLoss: 0.012574\n",
      "Train Epoch: 14 [30240/60000 (50%)]\tLoss: 0.011657\n",
      "Train Epoch: 14 [32400/60000 (54%)]\tLoss: 0.026844\n",
      "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 0.023295\n",
      "Train Epoch: 14 [36720/60000 (61%)]\tLoss: 0.033778\n",
      "Train Epoch: 14 [38880/60000 (65%)]\tLoss: 0.019337\n",
      "Train Epoch: 14 [41040/60000 (68%)]\tLoss: 0.013359\n",
      "Train Epoch: 14 [43200/60000 (72%)]\tLoss: 0.044825\n",
      "Train Epoch: 14 [45360/60000 (76%)]\tLoss: 0.022778\n",
      "Train Epoch: 14 [47520/60000 (79%)]\tLoss: 0.019207\n",
      "Train Epoch: 14 [49680/60000 (83%)]\tLoss: 0.012965\n",
      "Train Epoch: 14 [51840/60000 (86%)]\tLoss: 0.015866\n",
      "Train Epoch: 14 [54000/60000 (90%)]\tLoss: 0.036368\n",
      "Train Epoch: 14 [56160/60000 (94%)]\tLoss: 0.023561\n",
      "Train Epoch: 14 [58320/60000 (97%)]\tLoss: 0.008801\n",
      "\n",
      "Test set: Avg. loss: 0.0753, Accuracy: 9780/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_step = 16\n",
    "leak = 0.98\n",
    "toy = MLP(time_step,leak)\n",
    "toy= toy.cuda()\n",
    "# vgg = VGG_5(time_step)\n",
    "# vgg =vgg.cuda()\n",
    "# print(\"weight\",toy.fc_1.weight)\n",
    "torch.nn.init.normal_(toy.fc_1.weight, mean=0.0, std=0.1)\n",
    "toy.fc_1.weight.data = quant(toy.fc_1.weight,2**4)\n",
    "# torch.nn.init.normal_(toy.fc_2.weight, mean=0.0, std=0.1)\n",
    "# toy.fc_2.weight.data = quant(toy.fc_2.weight,2**4)\n",
    "torch.nn.init.normal_(toy.fc_out.weight, mean=0.0, std=0.1)\n",
    "toy.fc_out.weight.data = quant(toy.fc_out.weight,2**4)\n",
    "# print(\"quantized weight\",toy.fc_1.weight)\n",
    "lr = 0.001\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "test(toy)\n",
    "for epoch in range(15):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "        out = toy(data)\n",
    "         \n",
    "        err = loss(out,target)\n",
    "        exp = torch.exp(out)\n",
    "        exp_sum = torch.sum(torch.exp(out),1, keepdim=True)   \n",
    "        target = F.one_hot(target, num_classes=10)\n",
    "        #L = -1*torch.sum((target*torch.log((exp/exp_sum))),1, keepdim=True)\n",
    "        du_out = exp/exp_sum\n",
    "        du_out = du_out - target\n",
    "        \n",
    "        \n",
    "#         vgg_out = vgg(data)\n",
    "#         exp_vgg = torch.exp(vgg_out)\n",
    "#         exp_sum_vgg = torch.sum(torch.exp(vgg_out),1, keepdim=True)\n",
    "#         du_out_vgg = exp_vgg/exp_sum_vgg\n",
    "#         du_out_vgg = du_out_vgg - target\n",
    "#         print(du_out_vgg)\n",
    "        \n",
    "        \n",
    "        \n",
    "        bp_MLP(toy,leak,time_step,du_out,toy.s_regs_inp,lr,toy.lif1.thresh)\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), err.item()))\n",
    "            train_losses.append(err.item())\n",
    "            train_counter.append((batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "\n",
    "    test(toy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cebce317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# fig = plt.figure()\n",
    "# plt.plot(train_counter, train_losses, color='blue')\n",
    "# plt.scatter(test_counter, test_losses, color='red')\n",
    "# plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "# plt.xlabel('number of training examples seen')\n",
    "# plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88d35d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor([[1,2],[3,4]])\n",
    "f = W.shape[-1]\n",
    "dH = torch.tensor([[1,2],[3,4]])\n",
    "n_W = dH.shape[-1]\n",
    "n_H = dH.shape[-2]\n",
    "X = torch.ones(3,3)\n",
    "dX = torch.zeros(X.shape)\n",
    "dW = torch.zeros(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a207aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in range(n_H):\n",
    "    for w in range(n_W):\n",
    "        dX[h:h+f, w:w+f] += W * dH[h][w]\n",
    "        dW += X[h:h+f, w:w+f] * dH[h][w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e9b6bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10.],\n",
       "        [10., 10.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "29bc4fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  4.],\n",
       "        [ 6., 20.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dX[0:0+2, 0:0+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e14ac2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W*dH[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245bd1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
