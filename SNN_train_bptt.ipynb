{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251e3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import gc\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b07e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33e21ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PoissonGen(inp, rescale_fac=2.0):\n",
    "    rand_inp = torch.rand_like(inp)\n",
    "    return torch.mul(torch.le(rand_inp * rescale_fac, torch.abs(inp)).float(), torch.sign(inp))\n",
    "\n",
    "# def spike_function(x):\n",
    "#     x[x>0] = 1\n",
    "#     x[x<=0] = 0\n",
    "#     return x\n",
    "\n",
    "def de_func(U,th):\n",
    "    alpha = 0.3\n",
    "    U = alpha*(1.0 - abs((U-th)/th))\n",
    "    U[U<0]=0\n",
    "    return U\n",
    "\n",
    "def test(toy,data,test_loader):\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    toy = toy.cuda()\n",
    "    for data, target in test_loader:\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "        output = toy(data)\n",
    "        test_loss +=F.cross_entropy(output, target,reduction='mean').item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    \n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "#     with torch.no_grad():\n",
    "#         for data in test_loader:\n",
    "#             toy = toy.cuda()\n",
    "            \n",
    "#             images, labels = data\n",
    "#             images = images.cuda()\n",
    "#             labels = labels.cuda()\n",
    "#             # calculate outputs by running images through the network\n",
    "#             outputs = toy(images)\n",
    "#             # the class with the highest energy is what we choose as prediction\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "#         100 * correct / total))\n",
    "\n",
    "def quant(input, k):\n",
    "    size = input.size()\n",
    "    #mean = torch.mean(input.abs(), 1, keepdim=True)\n",
    "    x = input\n",
    "    #print(x)\n",
    "    xmax = x.abs().max()\n",
    "    num_bits=k\n",
    "    v0 = 1\n",
    "    v1 = 2\n",
    "    v2 = -0.5\n",
    "    y = k #2.**num_bits - 1.\n",
    "    #print(y)\n",
    "    x = x.add(v0).div(v1)\n",
    "    #print(x)\n",
    "    x = x.mul(y).round_()\n",
    "    #print(x)\n",
    "    x = x.div(y)\n",
    "    #print(x)\n",
    "    x = x.add(v2)\n",
    "    #print(x)\n",
    "    x = x.mul(v1)\n",
    "    #print(x)\n",
    "    input = x\n",
    "    return input\n",
    "\n",
    "def conv_weight_update(dH,X,pad):\n",
    "    shap = dH.shape[1]\n",
    "    shap_X = X.shape[1]\n",
    "    dH = torch.sum(dH,0)\n",
    "    dH = torch.unsqueeze(dH,1)\n",
    "    dH = torch.repeat_interleave(dH,X.shape[1],0)\n",
    "    X = X.repeat(1,shap,1,1)\n",
    "    dw_conv = F.conv2d(X,dH,padding=pad,groups=X.shape[1])\n",
    "    batch = dw_conv.shape[0]\n",
    "    dw_conv = torch.sum(dw_conv,0)\n",
    "    dw_conv = dw_conv.view(shap,shap_X,dw_conv.shape[-1],dw_conv.shape[-1])\n",
    "    return dw_conv/batch\n",
    "\n",
    "def conv_dx_update(dH,W,pad):\n",
    "\n",
    "    W = torch.transpose(W,0,1)\n",
    "    W = torch.flip(W,[-1,-2]) # W = C*3*3\n",
    "    dx_conv = F.conv2d(dH,W,padding=1)\n",
    "    \n",
    "    return dx_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d766c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, time_step,leak):\n",
    "        super(model, self).__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(28*28,256,bias=False)\n",
    "        self.fc_2 = nn.Linear(256,256,bias=False)\n",
    "        self.fc_out = nn.Linear(256,10,bias=False)\n",
    "        \n",
    "        self.lif1 = LIF(time_step,leak)\n",
    "        self.lif2 = LIF(time_step,leak)\n",
    "        self.time_step = time_step\n",
    "        self.s_regs_inp = None\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        inp = inp.view(inp.shape[0],-1)\n",
    "        size = inp.shape\n",
    "        self.s_regs_inp = torch.zeros(self.time_step,*size, device=device)\n",
    "        u_out = 0\n",
    "        \n",
    "        for t in range(self.time_step):\n",
    "            \n",
    "            spike_inp = PoissonGen(inp)\n",
    "            self.s_regs_inp[t] += spike_inp \n",
    "            \n",
    "            x = self.fc_1(spike_inp)\n",
    "            #x = quant(x,2**4)\n",
    "            x = self.lif1(x, t)\n",
    "            x = self.fc_2(x)\n",
    "            #x = quant(x,2**4)\n",
    "            x = self.lif2(x, t)\n",
    "            x = self.fc_out(x)\n",
    "            u_out = u_out + x\n",
    "        return u_out/self.time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8d72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,time_step,leak):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(28*28,512,bias=False)\n",
    "        self.fc_out = nn.Linear(512,10,bias=False)\n",
    "        self.lif1 = LIF(time_step,leak)\n",
    "        self.time_step = time_step\n",
    "        self.s_regs_inp = None\n",
    "        \n",
    "    def forward(self, inp):\n",
    "#         print(\"size is:\", (inp.view(inp.shape[0],1,28,28)).shape)\n",
    "        inp = inp.view(inp.shape[0],-1)\n",
    "        size = inp.shape\n",
    "        \n",
    "        self.s_regs_inp = torch.zeros(self.time_step,*size, device=device)\n",
    "        u_out = 0\n",
    "        \n",
    "        for t in range(self.time_step):\n",
    "            spike_inp = PoissonGen(inp)\n",
    "            self.s_regs_inp[t] += spike_inp \n",
    "            x = self.fc_1(spike_inp)\n",
    "            #x = quant(x,2**4)\n",
    "            x = self.lif1(x, t)\n",
    "            x = self.fc_out(x)\n",
    "            u_out = u_out + x\n",
    "        return u_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28765ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_5(nn.Module):\n",
    "    def __init__(self,time_step, leak,data):\n",
    "        super(VGG_5, self).__init__()\n",
    "        \n",
    "        if data == \"cifar10\":\n",
    "            input_dim = 3\n",
    "            pre_linear_dim = 8\n",
    "        elif data == \"mnist\":\n",
    "            input_dim = 1\n",
    "            pre_linear_dim = 7\n",
    "        \n",
    "        self.time_step = time_step\n",
    "        self.s_regs_inp = None\n",
    "        self.s_regs_conv = None\n",
    "        self.conv1 = nn.Conv2d(input_dim, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv_lif1 = LIF(time_step, leak)\n",
    "        # self.conv1a = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        # self.conv_lif1a = LIF(time_step, leak)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2,return_indices=True)\n",
    "        self.pool1_ind = []\n",
    "        self.unpool1 = nn.MaxUnpool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv_lif2 = LIF(time_step, leak)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv_lif3 = LIF(time_step, leak)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2,return_indices=True)\n",
    "        self.pool2_ind = []\n",
    "        self.unpool2 = nn.MaxUnpool2d(kernel_size=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * pre_linear_dim * pre_linear_dim, 1024, bias=False)\n",
    "        self.fc_lif1 = LIF(time_step,leak)\n",
    "        self.fc_out = nn.Linear(1024, 10, bias=False)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "\n",
    "        size = inp.shape\n",
    "        self.s_regs_inp = torch.zeros(self.time_step,*size, device=device)\n",
    "        self.pool1_ind = []\n",
    "        self.pool2_ind = []\n",
    "        u_out = 0\n",
    "        \n",
    "        for t in range(self.time_step):\n",
    "            spike_inp = PoissonGen(inp)\n",
    "            self.s_regs_inp[t] += spike_inp \n",
    "            x = self.conv1(spike_inp)\n",
    "            x = self.conv_lif1(x,t)\n",
    "            # x = self.conv1a(x)\n",
    "            # x = self.conv_lif1a(x,t)\n",
    "            x,indices = self.pool1(x)\n",
    "            self.pool1_ind.append(indices)\n",
    "            x = self.conv2(x)\n",
    "            x = self.conv_lif2(x,t)\n",
    "            x = self.conv3(x)\n",
    "            x = self.conv_lif3(x,t)\n",
    "            x,indices = self.pool2(x)\n",
    "            x = x.view(x.shape[0],-1)\n",
    "            \n",
    "            if t == 0:\n",
    "                self.s_regs_conv = torch.zeros(self.time_step,*x.shape, device=device)\n",
    "            self.pool2_ind.append(indices)\n",
    "            self.s_regs_conv[t] += x\n",
    "            \n",
    "            x = self.fc1(x)\n",
    "            x = self.fc_lif1(x,t)\n",
    "            x = self.fc_out(x)\n",
    "            u_out = u_out + x\n",
    "        return u_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37c149d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_VGG5(vgg,leak,time_step,du_out,l_r,mom,th,pre_linear_dim,input_dim):\n",
    "    \n",
    "#     batch = du_out.shape[0]\n",
    "    \n",
    "    ## Update weight in FCs, time T\n",
    "    du_fc1 = torch.matmul(du_out,vgg.fc_out.weight)*de_func(vgg.fc_lif1.u_regs[-1],th)\n",
    "    vgg.fc_lif1.du_regs[-1] += du_fc1\n",
    "    w_conv_1 = torch.matmul(torch.transpose(du_fc1,0,1),vgg.s_regs_conv[-1])\n",
    "    vgg.fc1.weight.data -= l_r*w_conv_1   \n",
    "    w_1_out = torch.matmul(torch.transpose(du_out,0,1),vgg.fc_lif1.s_regs[-1])\n",
    "    vgg.fc_out.weight.data -= l_r*w_1_out\n",
    "    \n",
    "    ## Update du in pool2, time T\n",
    "    dx_pool2 = torch.matmul(du_fc1,vgg.fc1.weight)\n",
    "    dx_pool2 = dx_pool2.view(dx_pool2.shape[0],128,pre_linear_dim,pre_linear_dim)\n",
    "    du_pool2 = vgg.unpool2(dx_pool2,vgg.pool2_ind[-1])\n",
    "    \n",
    "    ## Update du and dw in conv3, time T\n",
    "    du_conv3 = du_pool2*de_func(toy.conv_lif3.u_regs[-1],th)\n",
    "    vgg.conv_lif3.du_regs[-1] += du_conv3 \n",
    "    dW_conv3 = conv_weight_update(du_pool2.type(torch.float),vgg.conv_lif2.s_regs[-1].type(torch.float),1)\n",
    "#     dW_conv3 = torch.sum(dW_conv3,0)\n",
    "#     dW_conv3 = dW_conv3.view(128,64,dW_conv3.shape[-1],dW_conv3.shape[-1])\n",
    "    vgg.conv3.weight.data -=l_r*dW_conv3\n",
    "    \n",
    "    ## Update du and dw in conv2, time T\n",
    "    \n",
    "    du_conv2 = conv_dx_update(du_conv3,vgg.conv3.weight,'same')*de_func(toy.conv_lif2.u_regs[-1],th)\n",
    "    vgg.conv_lif2.du_regs[-1] += du_conv2\n",
    "    dW_conv2 = conv_weight_update(du_conv2.type(torch.float),F.max_pool2d(vgg.conv_lif1.s_regs[-1].type(torch.float),kernel_size=2),1)\n",
    "#     dW_conv2 = torch.sum(dW_conv2,0)\n",
    "#     dW_conv2 = dW_conv2.view(64,64,dW_conv2.shape[-1],dW_conv2.shape[-1])\n",
    "    vgg.conv2.weight.data -=l_r*dW_conv2\n",
    "    \n",
    "    ## Update du in pool2, time t\n",
    "    du_pool1 = vgg.unpool1(conv_dx_update(du_conv2,vgg.conv2.weight,'same'),vgg.pool1_ind[-1])\n",
    "    \n",
    "    # du_conv1a = du_pool1*de_func(toy.conv_lif1a.u_regs[-1],th)\n",
    "    # vgg.conv_lif1a.du_regs[-1] += du_conv1a\n",
    "    # dW_conv1a = conv_weight_update(du_pool1.type(torch.float),vgg.conv_lif1.s_regs[-1].type(torch.float),1)\n",
    "    # dW_conv1a = torch.sum(dW_conv1a,0)\n",
    "    # dW_conv1a = dW_conv1a.view(64,32,dW_conv1a.shape[-1],dW_conv1a.shape[-1])\n",
    "    # vgg.conv1a.weight.data -=l_r*dW_conv1a\n",
    "    \n",
    "    du_conv1 = du_pool1*de_func(toy.conv_lif1.u_regs[-1],th)\n",
    "    vgg.conv_lif1.du_regs[-1] += du_conv1\n",
    "    dW_conv1 = conv_weight_update(du_conv1.type(torch.float),vgg.s_regs_inp[-1].type(torch.float),1)\n",
    "#     dW_conv1 = torch.sum(dW_conv1,0)\n",
    "#     dW_conv1 = dW_conv1.view(64,input_dim,dW_conv1.shape[-1],dW_conv1.shape[-1])\n",
    "    vgg.conv1.weight.data -=l_r*dW_conv1\n",
    "    \n",
    "    prev_fc1 = w_conv_1\n",
    "    prev_fc_out = w_1_out\n",
    "    prev_conv3 = dW_conv3\n",
    "    prev_conv2 = dW_conv2\n",
    "    prev_conv1 = dW_conv1\n",
    "\n",
    "\n",
    "    for t in range(time_step-2,-1,-1):\n",
    "        \n",
    "        ds_fc1 = torch.matmul(du_out,vgg.fc_out.weight)+vgg.fc_lif1.du_regs[t+1]*(-leak*vgg.fc_lif1.du_regs[t])\n",
    "        du_fc1 = (ds_fc1)*de_func(vgg.fc_lif1.du_regs[t],th) + vgg.fc_lif1.du_regs[t+1]*leak*(1-vgg.fc_lif1.s_regs[t])\n",
    "        vgg.fc_lif1.du_regs[t] += du_fc1\n",
    "        w_conv_1 = torch.matmul(torch.transpose(du_fc1,0,1),vgg.s_regs_conv[t]) + mom*prev_fc1\n",
    "        prev_fc1 = w_conv_1\n",
    "        vgg.fc1.weight.data -= l_r*w_conv_1 \n",
    "        w_1_out = torch.matmul(torch.transpose(du_out,0,1),vgg.fc_lif1.s_regs[t]) + mom*prev_fc_out\n",
    "        prev_fc_out = w_1_out\n",
    "        vgg.fc_out.weight.data -= l_r*w_1_out\n",
    "        \n",
    "        \n",
    "        dx_pool2 = torch.matmul(du_fc1,vgg.fc1.weight)\n",
    "        dx_pool2 = dx_pool2.view(dx_pool2.shape[0],128,pre_linear_dim,pre_linear_dim)\n",
    "        du_pool2 = vgg.unpool2(dx_pool2,vgg.pool2_ind[t])\n",
    "        ds_conv3 = du_pool2+vgg.conv_lif3.du_regs[t+1]*(-leak*vgg.conv_lif3.du_regs[t])\n",
    "        du_conv3 = ds_conv3*de_func(toy.conv_lif3.u_regs[t],th) + vgg.conv_lif3.du_regs[t+1]*leak*(1-vgg.conv_lif3.s_regs[t])\n",
    "        vgg.conv_lif3.du_regs[t] += du_conv3 \n",
    "        dW_conv3 = conv_weight_update(du_pool2.type(torch.float),vgg.conv_lif2.s_regs[t].type(torch.float),1) + mom*prev_conv3\n",
    "        prev_conv3 = dW_conv3\n",
    "#         dW_conv3 = torch.sum(dW_conv3,0)\n",
    "#         dW_conv3 = dW_conv3.view(128,64,dW_conv3.shape[-1],dW_conv3.shape[-1])\n",
    "        vgg.conv3.weight.data -=l_r*dW_conv3\n",
    "        \n",
    "        ds_conv2 = conv_dx_update(du_conv3,vgg.conv3.weight,'same')+vgg.conv_lif2.du_regs[t+1]*(-leak*vgg.conv_lif2.du_regs[t])\n",
    "        du_conv2 = ds_conv2*de_func(toy.conv_lif2.u_regs[t],th) + vgg.conv_lif2.du_regs[t+1]*leak*(1-vgg.conv_lif2.s_regs[t])\n",
    "        vgg.conv_lif2.du_regs[t] += du_conv2 \n",
    "        dW_conv2 = conv_weight_update(du_conv2.type(torch.float),F.max_pool2d(vgg.conv_lif1.s_regs[t].type(torch.float),kernel_size=2),1) + mom*prev_conv2\n",
    "        prev_conv2 = dW_conv2\n",
    "#         dW_conv2 = torch.sum(dW_conv2,0)\n",
    "#         dW_conv2 = dW_conv2.view(64,64,dW_conv2.shape[-1],dW_conv2.shape[-1])\n",
    "        vgg.conv2.weight.data -=l_r*dW_conv2\n",
    "        du_pool1 = vgg.unpool1(conv_dx_update(du_conv2,vgg.conv2.weight,'same'),vgg.pool1_ind[t])\n",
    "        \n",
    "        \n",
    "#         ds_conv1a = du_pool1+vgg.conv_lif1a.du_regs[t+1]*(-leak*vgg.conv_lif1a.du_regs[t])\n",
    "#         du_conv1a = ds_conv1a*de_func(toy.conv_lif1a.u_regs[t],th) + vgg.conv_lif1a.du_regs[t+1]*leak*(1-vgg.conv_lif1a.s_regs[t])\n",
    "#         vgg.conv_lif1a.du_regs[t] += du_conv1a\n",
    "#         dW_conv1a = conv_weight_update(du_pool1.type(torch.float),vgg.conv_lif1.s_regs[t].type(torch.float),1)\n",
    "#         dW_conv1a = torch.sum(dW_conv1a,0)\n",
    "#         dW_conv1a = dW_conv1a.view(64,32,dW_conv1a.shape[-1],dW_conv1a.shape[-1])\n",
    "#         vgg.conv1a.weight.data -=l_r*dW_conv1a\n",
    "        ds_conv1 = du_pool1 + vgg.conv_lif1.du_regs[t+1]*(-leak*vgg.conv_lif1.du_regs[t])\n",
    "        du_conv1 = ds_conv1*de_func(toy.conv_lif1.u_regs[t],th) + vgg.conv_lif1.du_regs[t+1]*leak*(1-vgg.conv_lif1.s_regs[t])\n",
    "        vgg.conv_lif1.du_regs[t] += du_conv1\n",
    "        dW_conv1 = conv_weight_update(du_conv1.type(torch.float),vgg.s_regs_inp[t].type(torch.float),1) + mom*prev_conv1\n",
    "        prev_conv1 = dW_conv1\n",
    "#         dW_conv1 = torch.sum(dW_conv1,0)\n",
    "#         dW_conv1 = dW_conv1.view(64,input_dim,dW_conv1.shape[-1],dW_conv1.shape[-1])\n",
    "        vgg.conv1.weight.data -=l_r*dW_conv1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5802d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     du_pool2 = torch.sum(du_pool2,0)\n",
    "#     du_pool2 = torch.unsqueeze(du_pool2,1)\n",
    "    \n",
    "    ## Update du in conv3, time T\n",
    "#     d_conv3 = nn.Conv2d(128, 128, stride=1, kernel_size=f, padding=f-1, bias=False)\n",
    "    \n",
    "    ## Update weight in Conv3, time T\n",
    "#     f = du_pool2.shape[-1]\n",
    "#     d_conv3 = nn.Conv2d(128, 128, stride=1, padding=1, kernel_size=f, bias=False)\n",
    "#     d_conv3.weight.data = du_pool2.type(torch.float)\n",
    "#     dW_conv3 = d_conv3(vgg.conv_lif2.s_regs[-1].type(torch.float))\n",
    "#     dW_conv3 = torch.sum(dW_conv3,0)\n",
    "#     dW_conv3 = torch.unsqueeze(dW_conv3,1)\n",
    "#     vgg.conv3.weight.data -= l_r*dW_conv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7adf32d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_1(nn.Module):\n",
    "    def __init__(self,time_step,leak):\n",
    "        super(VGG_1, self).__init__()\n",
    "        \n",
    "        self.time_step = time_step\n",
    "        self.s_regs_inp = None\n",
    "        self.s_regs_conv = None\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1, bias=False)\n",
    "        \n",
    "#         self.deconv1 = nn.Conv2d()\n",
    "        self.lif_conv1 = LIF(time_step,leak)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2,return_indices=True)\n",
    "        self.pool1_ind = []\n",
    "        self.unpool1 = nn.MaxUnpool2d(kernel_size=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(16 * 14 * 14, 512, bias=False)\n",
    "        self.lif_fc1 = LIF(time_step,leak)\n",
    "        self.fc_out = nn.Linear(512, 10, bias=False)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "\n",
    "        size = inp.shape\n",
    "        self.s_regs_inp = torch.zeros(self.time_step,*size, device=device)\n",
    "        self.pool1_ind = []\n",
    "\n",
    "        u_out = 0\n",
    "        for t in range(self.time_step):\n",
    "            spike_inp = PoissonGen(inp)\n",
    "            self.s_regs_inp[t] += spike_inp\n",
    "            x = self.conv1(spike_inp)\n",
    "            x = self.lif_conv1(x,t)\n",
    "            x, indices = self.pool1(x)\n",
    "            x= x.view(x.shape[0],-1)\n",
    "            \n",
    "            if t == 0:\n",
    "                self.s_regs_conv = torch.zeros(self.time_step,*x.shape, device=device)\n",
    "            self.pool1_ind.append(indices)\n",
    "            self.s_regs_conv[t] += x\n",
    "            \n",
    "            x = self.fc1(x)\n",
    "            x = self.lif_fc1(x,t)\n",
    "            \n",
    "            x = self.fc_out(x)\n",
    "            u_out = u_out + x\n",
    "\n",
    "        return u_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31ffe4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_VGG1(vgg,leak,time_step,du_out,l_r,th):\n",
    "   \n",
    "    ## First fc\n",
    "    du_fc1 = torch.matmul(du_out,vgg.fc_out.weight)*de_func(vgg.lif_fc1.u_regs[-1],th)\n",
    "    vgg.lif_fc1.du_regs[-1] += du_fc1\n",
    "       \n",
    "    ## Update weight\n",
    "    w_conv_1 = torch.matmul(torch.transpose(du_fc1,0,1),vgg.s_regs_conv[-1])\n",
    "    vgg.fc1.weight.data -= l_r*w_conv_1\n",
    "     \n",
    "    w_1_out = torch.matmul(torch.transpose(du_out,0,1),vgg.lif_fc1.s_regs[-1])\n",
    "    vgg.fc_out.weight.data -= l_r*w_1_out\n",
    "    \n",
    "    dx_pool1 = torch.matmul(du_fc1,vgg.fc1.weight)\n",
    "    dx_pool1 = dx_pool1.view(dx_pool1.shape[0],16,14,14)\n",
    "    du_pool1 = vgg.unpool1(dx_pool1,vgg.pool1_ind[-1])\n",
    "    du_pool1 = torch.sum(du_pool1,0)\n",
    "    du_pool1 = torch.unsqueeze(du_pool1,1)\n",
    "    f = du_pool1.shape[-1]\n",
    "    d_conv1_w = nn.Conv2d(1, 16, stride=1, padding=1,kernel_size=f, bias=False)\n",
    "    d_conv1_w.weight.data = du_pool1.type(torch.float)\n",
    "    dW = d_conv1_w(vgg.s_regs_inp[-1].type(torch.float))\n",
    "    dW = torch.sum(dW,0)\n",
    "    dW = torch.unsqueeze(dW,1)\n",
    "\n",
    "    vgg.conv1.weight.data -= l_r*dW\n",
    "    \n",
    "    for t in range(time_step-2,-1,-1):\n",
    "        \n",
    "        ds_fc1 = torch.matmul(du_out,vgg.fc_out.weight)+vgg.lif_fc1.du_regs[t+1]*(-leak*vgg.lif_fc1.du_regs[t])\n",
    "        du_fc1 = (ds_fc1)*de_func(vgg.lif_fc1.du_regs[t],th) + vgg.lif_fc1.du_regs[t+1]*leak*(1-vgg.lif_fc1.s_regs[t])\n",
    "        vgg.lif_fc1.du_regs[t] += du_fc1\n",
    "        \n",
    "        w_conv_1 = torch.matmul(torch.transpose(du_fc1,0,1),vgg.s_regs_conv[t])\n",
    "        vgg.fc1.weight.data -= l_r*w_conv_1\n",
    "        \n",
    "        \n",
    "        dx_pool1 = torch.matmul(du_fc1,vgg.fc1.weight)\n",
    "        dx_pool1 = dx_pool1.view(dx_pool1.shape[0],16,14,14)\n",
    "        du_pool1 = vgg.unpool1(dx_pool1,vgg.pool1_ind[t])\n",
    "        du_pool1 = torch.sum(du_pool1,0)\n",
    "        du_pool1 = torch.unsqueeze(du_pool1,1)\n",
    "        f = du_pool1.shape[-1]\n",
    "        d_conv1_w = nn.Conv2d(1, 16, stride=1, padding=1,kernel_size=f, bias=False)\n",
    "        d_conv1_w.weight.data = du_pool1.type(torch.float)\n",
    "        dW = d_conv1_w(vgg.s_regs_inp[t].type(torch.float))\n",
    "        dW = torch.sum(dW,0)\n",
    "        dW = torch.unsqueeze(dW,1)\n",
    "\n",
    "        vgg.conv1.weight.data -= l_r*dW\n",
    "    \n",
    "    \n",
    "    return 0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e6f78f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIF(nn.Module):\n",
    "    def __init__(self, time_step,leak):\n",
    "        super(LIF, self).__init__()\n",
    "        \n",
    "        self.u_regs = None\n",
    "        self.du_regs = None\n",
    "        self.s_regs = None\n",
    "        self.leak = leak\n",
    "        self.time_step = time_step\n",
    "        self.thresh = 0.5\n",
    "        \n",
    "    def forward(self,inp,t):\n",
    "        \n",
    "#         print(\"memory before clear\",torch.cuda.memory_allocated())\n",
    "        if t == 0:\n",
    "            size = inp.shape\n",
    "            self.u_regs = torch.zeros(self.time_step,*size, device=device)\n",
    "            self.du_regs = torch.zeros(self.time_step,*size, device=device)\n",
    "#             err = torch.normal(0, 0.1,(1,1)).cuda()\n",
    "#             inp = inp + err\n",
    "#             self.u_regs[0] = quant(inp,2**4)\n",
    "            self.u_regs[0] = inp\n",
    "            self.s_regs = torch.zeros(self.time_step,*size, device=device)\n",
    "\n",
    "            spike = inp.gt(self.thresh).float()\n",
    "\n",
    "            self.s_regs[0] = spike\n",
    "            \n",
    "        else:\n",
    "#             err = torch.normal(0, 0.1,(1,1))\n",
    "#             inp = inp + err\n",
    "#             self.u_regs[t] = quant(self.leak * self.u_regs[t-1] * (1 - self.s_regs[t-1]) + (1-self.leak)*inp, 2**4)\n",
    "            self.u_regs[t] = self.leak*self.u_regs[t-1]*(1-self.s_regs[t-1]) + inp\n",
    "\n",
    "            spike = self.u_regs[t].gt(self.thresh).float()\n",
    "\n",
    "            self.s_regs[t] = spike\n",
    "            \n",
    "#         print(\"memory after clear\",torch.cuda.memory_allocated())\n",
    "#         torch.cuda.empty_cache()\n",
    "#         gc.collect()\n",
    "        return spike\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8fc2dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Back propagation for MLP\n",
    "def bp_MLP(toy,leak,time_step,du_out,s_regs_inp,l_r,th):\n",
    "    \n",
    "    ## First fc\n",
    "    du_fc1 = torch.matmul(du_out,toy.fc_out.weight)*de_func(toy.lif1.u_regs[-1],th)\n",
    "    toy.lif1.du_regs[-1] += du_fc1\n",
    "\n",
    "    ## Update weight\n",
    "    w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[-1])\n",
    "#     toy.fc_1.weight.data -= l_r*quant(w_inp_1,2**4)\n",
    "    toy.fc_1.weight.data -= l_r*w_inp_1\n",
    "    \n",
    "    w_1_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif1.s_regs[-1])\n",
    "#     toy.fc_out.weight.data -= l_r*quant(w_1_out,2**4)\n",
    "    toy.fc_out.weight.data -= l_r*w_1_out\n",
    "\n",
    "    for t in range(time_step-2,-1,-1):\n",
    "        \n",
    "        ## First fc\n",
    "        ds_fc1 = torch.matmul(du_out,toy.fc_out.weight)+toy.lif1.du_regs[t+1]*(-leak*toy.lif1.u_regs[t])\n",
    "        du_fc1 = (ds_fc1)*de_func(toy.lif1.u_regs[t],th) + toy.lif1.du_regs[t+1]*leak*(1-toy.lif1.s_regs[t])\n",
    "        toy.lif1.du_regs[t] += du_fc1\n",
    "\n",
    "        ## Update weight\n",
    "        w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[t])\n",
    "#         toy.fc_1.weight.data -= l_r*quant(w_inp_1,2**4)\n",
    "#         print(\"du_size\",du_fc1.shape)\n",
    "#         print(\"s_size\",s_regs_inp[t].shape)\n",
    "#         print(\"dweight shape\",w_inp_1.shape)\n",
    "        toy.fc_1.weight.data -= l_r*w_inp_1\n",
    "\n",
    "        w_1_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif1.s_regs[t])\n",
    "#         toy.fc_out.weight.data -= l_r*quant(w_1_out,2**4)\n",
    "        toy.fc_out.weight.data -= l_r*w_1_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b08084b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Back propagation\n",
    "def bp(toy,leak,time_step,du_out,s_regs_inp,l_r,th):\n",
    "    \n",
    "    ## Second fc    \n",
    "    du_fc2 = torch.matmul(du_out,toy.fc_out.weight)*de_func(toy.lif2.u_regs[-1],th)    \n",
    "    toy.lif2.du_regs[-1] = toy.lif2.du_regs[-1] + du_fc2\n",
    "    \n",
    "    ## First fc\n",
    "    du_fc1 = torch.matmul(du_fc2,toy.fc_2.weight)*de_func(toy.lif1.u_regs[-1],th)\n",
    "    toy.lif1.du_regs[-1] += du_fc1\n",
    "\n",
    "    \n",
    "    ## Update weight\n",
    "    w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[-1])\n",
    "    toy.fc_1.weight.data -= l_r*quant(w_inp_1,2**4)\n",
    "    #toy.fc_1.weight.data -= l_r*w_inp_1\n",
    "\n",
    "    w_1_2 = torch.matmul(torch.transpose(du_fc2,0,1),toy.lif1.s_regs[-1])\n",
    "    toy.fc_2.weight.data -= l_r*quant(w_1_2,2**4)\n",
    "    #toy.fc_2.weight.data -= l_r*w_1_2\n",
    "\n",
    "    w_2_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif2.s_regs[-1])\n",
    "    toy.fc_out.weight.data -= l_r*quant(w_2_out,2**4)\n",
    "    #toy.fc_out.weight.data -= l_r*w_2_out\n",
    "\n",
    "    for t in range(time_step-2,-1,-1):\n",
    "\n",
    "        ds_fc2 = torch.matmul(du_out,toy.fc_out.weight)+toy.lif2.du_regs[t+1]*(-leak*toy.lif2.u_regs[t])\n",
    "        du_fc2 = (ds_fc2)*de_func(toy.lif2.u_regs[t],th) + toy.lif2.du_regs[t+1]*leak*(1-toy.lif2.s_regs[t])\n",
    "        toy.lif2.du_regs[t] += du_fc2\n",
    "        \n",
    "        ## First fc\n",
    "        ds_fc1 = torch.matmul(du_fc2,toy.fc_2.weight)+toy.lif1.du_regs[t+1]*(-leak*toy.lif1.u_regs[t])\n",
    "        du_fc1 = (ds_fc1)*de_func(toy.lif1.u_regs[t],th) + toy.lif1.du_regs[t+1]*leak*(1-toy.lif1.s_regs[t])\n",
    "        toy.lif1.du_regs[t] += du_fc1\n",
    "\n",
    "        ## Update weight\n",
    "        w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[t])\n",
    "        toy.fc_1.weight.data -= l_r*quant(w_inp_1,2**4)\n",
    "        \n",
    "        #toy.fc_1.weight.data -= l_r*w_inp_1\n",
    "\n",
    "        w_1_2 = torch.matmul(torch.transpose(du_fc2,0,1),toy.lif1.s_regs[t])\n",
    "        toy.fc_2.weight.data -= l_r*quant(w_1_2,2**4)\n",
    "        #toy.fc_2.weight.data -= l_r*w_1_2\n",
    "\n",
    "        w_2_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif2.s_regs[t])\n",
    "        toy.fc_out.weight.data -= l_r*quant(w_2_out,2**4)\n",
    "        #toy.fc_out.weight.data -= l_r*w_2_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c3de0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ruokai\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "batch_size_train = 128\n",
    "batch_size_test = 1000\n",
    "\n",
    "train_loader_mnist = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./mnist', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_mnist = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./mnist', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edee34dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# import ssl\n",
    "\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# response = urllib.request.urlopen('https://www.python.org')\n",
    "# print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6839e4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./cifar10', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader_cifar10 = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=4,pin_memory=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./cifar10', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "test_loader_cifar10 = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=4,pin_memory=True)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b70c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3bfbdde-e623-4836-8e97-3bf5c7487a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sparsity(toy):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2ce9ca7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ruokai\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:693: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.487159\n",
      "Train Epoch: 0 [320/50000 (1%)]\tLoss: 2.286694\n",
      "Train Epoch: 0 [640/50000 (1%)]\tLoss: 2.098488\n",
      "Train Epoch: 0 [960/50000 (2%)]\tLoss: 2.113365\n",
      "Train Epoch: 0 [1280/50000 (3%)]\tLoss: 2.286698\n",
      "Train Epoch: 0 [1600/50000 (3%)]\tLoss: 2.341202\n",
      "Train Epoch: 0 [1920/50000 (4%)]\tLoss: 1.991303\n",
      "Train Epoch: 0 [2240/50000 (4%)]\tLoss: 2.186518\n",
      "Train Epoch: 0 [2560/50000 (5%)]\tLoss: 1.988434\n",
      "Train Epoch: 0 [2880/50000 (6%)]\tLoss: 1.975398\n",
      "Train Epoch: 0 [3200/50000 (6%)]\tLoss: 2.005755\n",
      "Train Epoch: 0 [3520/50000 (7%)]\tLoss: 2.020694\n",
      "Train Epoch: 0 [3840/50000 (8%)]\tLoss: 1.872497\n",
      "Train Epoch: 0 [4160/50000 (8%)]\tLoss: 1.893964\n",
      "Train Epoch: 0 [4480/50000 (9%)]\tLoss: 1.868692\n",
      "Train Epoch: 0 [4800/50000 (10%)]\tLoss: 1.882379\n",
      "Train Epoch: 0 [5120/50000 (10%)]\tLoss: 2.240709\n",
      "Train Epoch: 0 [5440/50000 (11%)]\tLoss: 1.762554\n",
      "Train Epoch: 0 [5760/50000 (12%)]\tLoss: 1.796419\n",
      "Train Epoch: 0 [6080/50000 (12%)]\tLoss: 1.998616\n",
      "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 2.023433\n",
      "Train Epoch: 0 [6720/50000 (13%)]\tLoss: 2.087141\n",
      "Train Epoch: 0 [7040/50000 (14%)]\tLoss: 1.942255\n",
      "Train Epoch: 0 [7360/50000 (15%)]\tLoss: 1.924031\n",
      "Train Epoch: 0 [7680/50000 (15%)]\tLoss: 2.044326\n",
      "Train Epoch: 0 [8000/50000 (16%)]\tLoss: 1.728651\n",
      "Train Epoch: 0 [8320/50000 (17%)]\tLoss: 1.786294\n",
      "Train Epoch: 0 [8640/50000 (17%)]\tLoss: 2.273810\n",
      "Train Epoch: 0 [8960/50000 (18%)]\tLoss: 1.680871\n",
      "Train Epoch: 0 [9280/50000 (19%)]\tLoss: 1.976528\n",
      "Train Epoch: 0 [9600/50000 (19%)]\tLoss: 1.979069\n",
      "Train Epoch: 0 [9920/50000 (20%)]\tLoss: 1.928976\n",
      "Train Epoch: 0 [10240/50000 (20%)]\tLoss: 1.990442\n",
      "Train Epoch: 0 [10560/50000 (21%)]\tLoss: 1.918945\n",
      "Train Epoch: 0 [10880/50000 (22%)]\tLoss: 2.240317\n",
      "Train Epoch: 0 [11200/50000 (22%)]\tLoss: 1.937906\n",
      "Train Epoch: 0 [11520/50000 (23%)]\tLoss: 1.759497\n",
      "Train Epoch: 0 [11840/50000 (24%)]\tLoss: 1.741476\n",
      "Train Epoch: 0 [12160/50000 (24%)]\tLoss: 1.934347\n",
      "Train Epoch: 0 [12480/50000 (25%)]\tLoss: 1.870982\n",
      "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 1.727413\n",
      "Train Epoch: 0 [13120/50000 (26%)]\tLoss: 1.921659\n",
      "Train Epoch: 0 [13440/50000 (27%)]\tLoss: 2.184884\n",
      "Train Epoch: 0 [13760/50000 (28%)]\tLoss: 1.742676\n",
      "Train Epoch: 0 [14080/50000 (28%)]\tLoss: 1.988654\n",
      "Train Epoch: 0 [14400/50000 (29%)]\tLoss: 1.774506\n",
      "Train Epoch: 0 [14720/50000 (29%)]\tLoss: 1.650725\n",
      "Train Epoch: 0 [15040/50000 (30%)]\tLoss: 1.663781\n",
      "Train Epoch: 0 [15360/50000 (31%)]\tLoss: 2.030781\n",
      "Train Epoch: 0 [15680/50000 (31%)]\tLoss: 1.746711\n",
      "Train Epoch: 0 [16000/50000 (32%)]\tLoss: 1.722059\n",
      "Train Epoch: 0 [16320/50000 (33%)]\tLoss: 1.617437\n",
      "Train Epoch: 0 [16640/50000 (33%)]\tLoss: 1.962313\n",
      "Train Epoch: 0 [16960/50000 (34%)]\tLoss: 1.784391\n",
      "Train Epoch: 0 [17280/50000 (35%)]\tLoss: 1.549974\n",
      "Train Epoch: 0 [17600/50000 (35%)]\tLoss: 1.984205\n",
      "Train Epoch: 0 [17920/50000 (36%)]\tLoss: 1.786804\n",
      "Train Epoch: 0 [18240/50000 (36%)]\tLoss: 1.757310\n",
      "Train Epoch: 0 [18560/50000 (37%)]\tLoss: 1.778141\n",
      "Train Epoch: 0 [18880/50000 (38%)]\tLoss: 1.863322\n",
      "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 1.556722\n",
      "Train Epoch: 0 [19520/50000 (39%)]\tLoss: 1.438887\n",
      "Train Epoch: 0 [19840/50000 (40%)]\tLoss: 1.986107\n",
      "Train Epoch: 0 [20160/50000 (40%)]\tLoss: 1.640759\n",
      "Train Epoch: 0 [20480/50000 (41%)]\tLoss: 2.062592\n",
      "Train Epoch: 0 [20800/50000 (42%)]\tLoss: 1.829470\n",
      "Train Epoch: 0 [21120/50000 (42%)]\tLoss: 1.697027\n",
      "Train Epoch: 0 [21440/50000 (43%)]\tLoss: 1.737085\n",
      "Train Epoch: 0 [21760/50000 (44%)]\tLoss: 1.655896\n",
      "Train Epoch: 0 [22080/50000 (44%)]\tLoss: 1.538769\n",
      "Train Epoch: 0 [22400/50000 (45%)]\tLoss: 1.428459\n",
      "Train Epoch: 0 [22720/50000 (45%)]\tLoss: 1.591277\n",
      "Train Epoch: 0 [23040/50000 (46%)]\tLoss: 1.881983\n",
      "Train Epoch: 0 [23360/50000 (47%)]\tLoss: 2.065404\n",
      "Train Epoch: 0 [23680/50000 (47%)]\tLoss: 1.567543\n",
      "Train Epoch: 0 [24000/50000 (48%)]\tLoss: 1.671671\n",
      "Train Epoch: 0 [24320/50000 (49%)]\tLoss: 1.604882\n",
      "Train Epoch: 0 [24640/50000 (49%)]\tLoss: 1.555923\n",
      "Train Epoch: 0 [24960/50000 (50%)]\tLoss: 1.456307\n",
      "Train Epoch: 0 [25280/50000 (51%)]\tLoss: 1.720624\n",
      "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 1.640969\n",
      "Train Epoch: 0 [25920/50000 (52%)]\tLoss: 1.387269\n",
      "Train Epoch: 0 [26240/50000 (52%)]\tLoss: 1.350899\n",
      "Train Epoch: 0 [26560/50000 (53%)]\tLoss: 1.513003\n",
      "Train Epoch: 0 [26880/50000 (54%)]\tLoss: 1.624963\n",
      "Train Epoch: 0 [27200/50000 (54%)]\tLoss: 1.644220\n",
      "Train Epoch: 0 [27520/50000 (55%)]\tLoss: 1.810035\n",
      "Train Epoch: 0 [27840/50000 (56%)]\tLoss: 1.696685\n",
      "Train Epoch: 0 [28160/50000 (56%)]\tLoss: 1.894729\n",
      "Train Epoch: 0 [28480/50000 (57%)]\tLoss: 1.381800\n",
      "Train Epoch: 0 [28800/50000 (58%)]\tLoss: 1.397048\n",
      "Train Epoch: 0 [29120/50000 (58%)]\tLoss: 1.837084\n",
      "Train Epoch: 0 [29440/50000 (59%)]\tLoss: 1.929354\n",
      "Train Epoch: 0 [29760/50000 (60%)]\tLoss: 1.912359\n",
      "Train Epoch: 0 [30080/50000 (60%)]\tLoss: 1.611710\n",
      "Train Epoch: 0 [30400/50000 (61%)]\tLoss: 1.487419\n",
      "Train Epoch: 0 [30720/50000 (61%)]\tLoss: 1.835859\n",
      "Train Epoch: 0 [31040/50000 (62%)]\tLoss: 1.715239\n",
      "Train Epoch: 0 [31360/50000 (63%)]\tLoss: 1.731677\n",
      "Train Epoch: 0 [31680/50000 (63%)]\tLoss: 1.407516\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 1.656971\n",
      "Train Epoch: 0 [32320/50000 (65%)]\tLoss: 1.412768\n",
      "Train Epoch: 0 [32640/50000 (65%)]\tLoss: 1.670529\n",
      "Train Epoch: 0 [32960/50000 (66%)]\tLoss: 1.594943\n",
      "Train Epoch: 0 [33280/50000 (67%)]\tLoss: 1.712399\n",
      "Train Epoch: 0 [33600/50000 (67%)]\tLoss: 1.650177\n",
      "Train Epoch: 0 [33920/50000 (68%)]\tLoss: 1.390147\n",
      "Train Epoch: 0 [34240/50000 (68%)]\tLoss: 1.316522\n",
      "Train Epoch: 0 [34560/50000 (69%)]\tLoss: 1.933728\n",
      "Train Epoch: 0 [34880/50000 (70%)]\tLoss: 1.610264\n",
      "Train Epoch: 0 [35200/50000 (70%)]\tLoss: 1.497294\n",
      "Train Epoch: 0 [35520/50000 (71%)]\tLoss: 1.605601\n",
      "Train Epoch: 0 [35840/50000 (72%)]\tLoss: 1.699009\n",
      "Train Epoch: 0 [36160/50000 (72%)]\tLoss: 1.632671\n",
      "Train Epoch: 0 [36480/50000 (73%)]\tLoss: 1.567779\n",
      "Train Epoch: 0 [36800/50000 (74%)]\tLoss: 1.714343\n",
      "Train Epoch: 0 [37120/50000 (74%)]\tLoss: 1.817288\n",
      "Train Epoch: 0 [37440/50000 (75%)]\tLoss: 1.816069\n",
      "Train Epoch: 0 [37760/50000 (75%)]\tLoss: 1.405993\n",
      "Train Epoch: 0 [38080/50000 (76%)]\tLoss: 1.410786\n",
      "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 1.588081\n",
      "Train Epoch: 0 [38720/50000 (77%)]\tLoss: 1.873908\n",
      "Train Epoch: 0 [39040/50000 (78%)]\tLoss: 1.520830\n",
      "Train Epoch: 0 [39360/50000 (79%)]\tLoss: 1.400671\n",
      "Train Epoch: 0 [39680/50000 (79%)]\tLoss: 1.847589\n",
      "Train Epoch: 0 [40000/50000 (80%)]\tLoss: 1.545725\n",
      "Train Epoch: 0 [40320/50000 (81%)]\tLoss: 1.400639\n",
      "Train Epoch: 0 [40640/50000 (81%)]\tLoss: 1.398764\n",
      "Train Epoch: 0 [40960/50000 (82%)]\tLoss: 1.501390\n",
      "Train Epoch: 0 [41280/50000 (83%)]\tLoss: 1.481844\n",
      "Train Epoch: 0 [41600/50000 (83%)]\tLoss: 1.319000\n",
      "Train Epoch: 0 [41920/50000 (84%)]\tLoss: 1.455041\n",
      "Train Epoch: 0 [42240/50000 (84%)]\tLoss: 1.533285\n",
      "Train Epoch: 0 [42560/50000 (85%)]\tLoss: 1.458190\n",
      "Train Epoch: 0 [42880/50000 (86%)]\tLoss: 1.620048\n",
      "Train Epoch: 0 [43200/50000 (86%)]\tLoss: 1.540030\n",
      "Train Epoch: 0 [43520/50000 (87%)]\tLoss: 1.524396\n",
      "Train Epoch: 0 [43840/50000 (88%)]\tLoss: 1.296104\n",
      "Train Epoch: 0 [44160/50000 (88%)]\tLoss: 1.340026\n",
      "Train Epoch: 0 [44480/50000 (89%)]\tLoss: 1.417824\n",
      "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 1.640707\n",
      "Train Epoch: 0 [45120/50000 (90%)]\tLoss: 1.641969\n",
      "Train Epoch: 0 [45440/50000 (91%)]\tLoss: 1.603797\n",
      "Train Epoch: 0 [45760/50000 (91%)]\tLoss: 1.566372\n",
      "Train Epoch: 0 [46080/50000 (92%)]\tLoss: 1.540904\n",
      "Train Epoch: 0 [46400/50000 (93%)]\tLoss: 1.710114\n",
      "Train Epoch: 0 [46720/50000 (93%)]\tLoss: 1.539717\n",
      "Train Epoch: 0 [47040/50000 (94%)]\tLoss: 1.521446\n",
      "Train Epoch: 0 [47360/50000 (95%)]\tLoss: 1.664220\n",
      "Train Epoch: 0 [47680/50000 (95%)]\tLoss: 1.550326\n",
      "Train Epoch: 0 [48000/50000 (96%)]\tLoss: 1.810947\n",
      "Train Epoch: 0 [48320/50000 (97%)]\tLoss: 1.428761\n",
      "Train Epoch: 0 [48640/50000 (97%)]\tLoss: 1.875106\n",
      "Train Epoch: 0 [48960/50000 (98%)]\tLoss: 2.010372\n",
      "Train Epoch: 0 [49280/50000 (99%)]\tLoss: 1.607690\n",
      "Train Epoch: 0 [49600/50000 (99%)]\tLoss: 1.899405\n",
      "Train Epoch: 0 [49920/50000 (100%)]\tLoss: 1.498840\n",
      "\n",
      "Test set: Avg. loss: 0.0492, Accuracy: 4419/10000 (44%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 1.627618\n",
      "Train Epoch: 1 [320/50000 (1%)]\tLoss: 1.515647\n",
      "Train Epoch: 1 [640/50000 (1%)]\tLoss: 1.216476\n",
      "Train Epoch: 1 [960/50000 (2%)]\tLoss: 1.659480\n",
      "Train Epoch: 1 [1280/50000 (3%)]\tLoss: 1.691379\n",
      "Train Epoch: 1 [1600/50000 (3%)]\tLoss: 1.686314\n",
      "Train Epoch: 1 [1920/50000 (4%)]\tLoss: 1.487450\n",
      "Train Epoch: 1 [2240/50000 (4%)]\tLoss: 1.528227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [2560/50000 (5%)]\tLoss: 1.515159\n",
      "Train Epoch: 1 [2880/50000 (6%)]\tLoss: 1.532378\n",
      "Train Epoch: 1 [3200/50000 (6%)]\tLoss: 1.337052\n",
      "Train Epoch: 1 [3520/50000 (7%)]\tLoss: 1.352556\n",
      "Train Epoch: 1 [3840/50000 (8%)]\tLoss: 1.546277\n",
      "Train Epoch: 1 [4160/50000 (8%)]\tLoss: 1.535403\n",
      "Train Epoch: 1 [4480/50000 (9%)]\tLoss: 1.422228\n",
      "Train Epoch: 1 [4800/50000 (10%)]\tLoss: 1.468355\n",
      "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 1.687755\n",
      "Train Epoch: 1 [5440/50000 (11%)]\tLoss: 1.477018\n",
      "Train Epoch: 1 [5760/50000 (12%)]\tLoss: 1.617760\n",
      "Train Epoch: 1 [6080/50000 (12%)]\tLoss: 1.497887\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 1.214386\n",
      "Train Epoch: 1 [6720/50000 (13%)]\tLoss: 1.512636\n",
      "Train Epoch: 1 [7040/50000 (14%)]\tLoss: 1.669103\n",
      "Train Epoch: 1 [7360/50000 (15%)]\tLoss: 1.434196\n",
      "Train Epoch: 1 [7680/50000 (15%)]\tLoss: 1.716050\n",
      "Train Epoch: 1 [8000/50000 (16%)]\tLoss: 1.519763\n",
      "Train Epoch: 1 [8320/50000 (17%)]\tLoss: 1.427240\n",
      "Train Epoch: 1 [8640/50000 (17%)]\tLoss: 1.380222\n",
      "Train Epoch: 1 [8960/50000 (18%)]\tLoss: 1.341120\n",
      "Train Epoch: 1 [9280/50000 (19%)]\tLoss: 1.411860\n",
      "Train Epoch: 1 [9600/50000 (19%)]\tLoss: 1.701330\n",
      "Train Epoch: 1 [9920/50000 (20%)]\tLoss: 1.697059\n",
      "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 1.484705\n",
      "Train Epoch: 1 [10560/50000 (21%)]\tLoss: 1.749273\n",
      "Train Epoch: 1 [10880/50000 (22%)]\tLoss: 1.792621\n",
      "Train Epoch: 1 [11200/50000 (22%)]\tLoss: 1.627275\n",
      "Train Epoch: 1 [11520/50000 (23%)]\tLoss: 1.514737\n",
      "Train Epoch: 1 [11840/50000 (24%)]\tLoss: 1.297240\n",
      "Train Epoch: 1 [12160/50000 (24%)]\tLoss: 1.455729\n",
      "Train Epoch: 1 [12480/50000 (25%)]\tLoss: 1.756583\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.562749\n",
      "Train Epoch: 1 [13120/50000 (26%)]\tLoss: 1.758141\n",
      "Train Epoch: 1 [13440/50000 (27%)]\tLoss: 1.898214\n",
      "Train Epoch: 1 [13760/50000 (28%)]\tLoss: 1.220459\n",
      "Train Epoch: 1 [14080/50000 (28%)]\tLoss: 1.662002\n",
      "Train Epoch: 1 [14400/50000 (29%)]\tLoss: 1.516612\n",
      "Train Epoch: 1 [14720/50000 (29%)]\tLoss: 1.385486\n",
      "Train Epoch: 1 [15040/50000 (30%)]\tLoss: 1.323046\n",
      "Train Epoch: 1 [15360/50000 (31%)]\tLoss: 1.734640\n",
      "Train Epoch: 1 [15680/50000 (31%)]\tLoss: 1.565797\n",
      "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 1.740602\n",
      "Train Epoch: 1 [16320/50000 (33%)]\tLoss: 1.638086\n",
      "Train Epoch: 1 [16640/50000 (33%)]\tLoss: 1.949984\n",
      "Train Epoch: 1 [16960/50000 (34%)]\tLoss: 1.609734\n",
      "Train Epoch: 1 [17280/50000 (35%)]\tLoss: 1.767606\n",
      "Train Epoch: 1 [17600/50000 (35%)]\tLoss: 1.551959\n",
      "Train Epoch: 1 [17920/50000 (36%)]\tLoss: 1.755454\n",
      "Train Epoch: 1 [18240/50000 (36%)]\tLoss: 1.660656\n",
      "Train Epoch: 1 [18560/50000 (37%)]\tLoss: 1.429466\n",
      "Train Epoch: 1 [18880/50000 (38%)]\tLoss: 1.506529\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 1.733397\n",
      "Train Epoch: 1 [19520/50000 (39%)]\tLoss: 1.457164\n",
      "Train Epoch: 1 [19840/50000 (40%)]\tLoss: 1.448587\n",
      "Train Epoch: 1 [20160/50000 (40%)]\tLoss: 1.532478\n",
      "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 1.425420\n",
      "Train Epoch: 1 [20800/50000 (42%)]\tLoss: 1.575861\n",
      "Train Epoch: 1 [21120/50000 (42%)]\tLoss: 1.703513\n",
      "Train Epoch: 1 [21440/50000 (43%)]\tLoss: 1.590016\n",
      "Train Epoch: 1 [21760/50000 (44%)]\tLoss: 1.622562\n",
      "Train Epoch: 1 [22080/50000 (44%)]\tLoss: 1.497414\n",
      "Train Epoch: 1 [22400/50000 (45%)]\tLoss: 1.679572\n",
      "Train Epoch: 1 [22720/50000 (45%)]\tLoss: 1.520560\n",
      "Train Epoch: 1 [23040/50000 (46%)]\tLoss: 1.640643\n",
      "Train Epoch: 1 [23360/50000 (47%)]\tLoss: 1.617270\n",
      "Train Epoch: 1 [23680/50000 (47%)]\tLoss: 1.535509\n",
      "Train Epoch: 1 [24000/50000 (48%)]\tLoss: 1.647541\n",
      "Train Epoch: 1 [24320/50000 (49%)]\tLoss: 1.541126\n",
      "Train Epoch: 1 [24640/50000 (49%)]\tLoss: 1.582964\n",
      "Train Epoch: 1 [24960/50000 (50%)]\tLoss: 1.266859\n",
      "Train Epoch: 1 [25280/50000 (51%)]\tLoss: 1.454195\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.706369\n",
      "Train Epoch: 1 [25920/50000 (52%)]\tLoss: 1.498194\n",
      "Train Epoch: 1 [26240/50000 (52%)]\tLoss: 1.433381\n",
      "Train Epoch: 1 [26560/50000 (53%)]\tLoss: 1.833773\n",
      "Train Epoch: 1 [26880/50000 (54%)]\tLoss: 1.356884\n",
      "Train Epoch: 1 [27200/50000 (54%)]\tLoss: 1.499913\n",
      "Train Epoch: 1 [27520/50000 (55%)]\tLoss: 1.077256\n",
      "Train Epoch: 1 [27840/50000 (56%)]\tLoss: 1.490895\n",
      "Train Epoch: 1 [28160/50000 (56%)]\tLoss: 1.509862\n",
      "Train Epoch: 1 [28480/50000 (57%)]\tLoss: 1.619120\n",
      "Train Epoch: 1 [28800/50000 (58%)]\tLoss: 1.663396\n",
      "Train Epoch: 1 [29120/50000 (58%)]\tLoss: 1.770057\n",
      "Train Epoch: 1 [29440/50000 (59%)]\tLoss: 1.293647\n",
      "Train Epoch: 1 [29760/50000 (60%)]\tLoss: 1.590969\n",
      "Train Epoch: 1 [30080/50000 (60%)]\tLoss: 1.591323\n",
      "Train Epoch: 1 [30400/50000 (61%)]\tLoss: 1.538887\n",
      "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 1.528242\n",
      "Train Epoch: 1 [31040/50000 (62%)]\tLoss: 1.427952\n",
      "Train Epoch: 1 [31360/50000 (63%)]\tLoss: 1.119263\n",
      "Train Epoch: 1 [31680/50000 (63%)]\tLoss: 1.373046\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.600990\n",
      "Train Epoch: 1 [32320/50000 (65%)]\tLoss: 1.557356\n",
      "Train Epoch: 1 [32640/50000 (65%)]\tLoss: 1.498473\n",
      "Train Epoch: 1 [32960/50000 (66%)]\tLoss: 1.219805\n",
      "Train Epoch: 1 [33280/50000 (67%)]\tLoss: 1.261997\n",
      "Train Epoch: 1 [33600/50000 (67%)]\tLoss: 1.458475\n",
      "Train Epoch: 1 [33920/50000 (68%)]\tLoss: 1.529950\n",
      "Train Epoch: 1 [34240/50000 (68%)]\tLoss: 1.828360\n",
      "Train Epoch: 1 [34560/50000 (69%)]\tLoss: 1.442758\n",
      "Train Epoch: 1 [34880/50000 (70%)]\tLoss: 1.508374\n",
      "Train Epoch: 1 [35200/50000 (70%)]\tLoss: 1.759599\n",
      "Train Epoch: 1 [35520/50000 (71%)]\tLoss: 1.823967\n",
      "Train Epoch: 1 [35840/50000 (72%)]\tLoss: 1.547884\n",
      "Train Epoch: 1 [36160/50000 (72%)]\tLoss: 1.404682\n",
      "Train Epoch: 1 [36480/50000 (73%)]\tLoss: 1.178597\n",
      "Train Epoch: 1 [36800/50000 (74%)]\tLoss: 1.571847\n",
      "Train Epoch: 1 [37120/50000 (74%)]\tLoss: 1.422884\n",
      "Train Epoch: 1 [37440/50000 (75%)]\tLoss: 1.667923\n",
      "Train Epoch: 1 [37760/50000 (75%)]\tLoss: 1.236885\n",
      "Train Epoch: 1 [38080/50000 (76%)]\tLoss: 1.699258\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.459778\n",
      "Train Epoch: 1 [38720/50000 (77%)]\tLoss: 1.369342\n",
      "Train Epoch: 1 [39040/50000 (78%)]\tLoss: 1.593781\n",
      "Train Epoch: 1 [39360/50000 (79%)]\tLoss: 1.676892\n",
      "Train Epoch: 1 [39680/50000 (79%)]\tLoss: 1.467880\n",
      "Train Epoch: 1 [40000/50000 (80%)]\tLoss: 1.454550\n",
      "Train Epoch: 1 [40320/50000 (81%)]\tLoss: 1.805950\n",
      "Train Epoch: 1 [40640/50000 (81%)]\tLoss: 1.460328\n",
      "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 1.297738\n",
      "Train Epoch: 1 [41280/50000 (83%)]\tLoss: 1.718187\n",
      "Train Epoch: 1 [41600/50000 (83%)]\tLoss: 1.624831\n",
      "Train Epoch: 1 [41920/50000 (84%)]\tLoss: 1.726480\n",
      "Train Epoch: 1 [42240/50000 (84%)]\tLoss: 1.431559\n",
      "Train Epoch: 1 [42560/50000 (85%)]\tLoss: 1.452523\n",
      "Train Epoch: 1 [42880/50000 (86%)]\tLoss: 1.646960\n",
      "Train Epoch: 1 [43200/50000 (86%)]\tLoss: 1.617305\n",
      "Train Epoch: 1 [43520/50000 (87%)]\tLoss: 1.632258\n",
      "Train Epoch: 1 [43840/50000 (88%)]\tLoss: 1.522405\n",
      "Train Epoch: 1 [44160/50000 (88%)]\tLoss: 1.522290\n",
      "Train Epoch: 1 [44480/50000 (89%)]\tLoss: 1.113548\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.551724\n",
      "Train Epoch: 1 [45120/50000 (90%)]\tLoss: 1.565967\n",
      "Train Epoch: 1 [45440/50000 (91%)]\tLoss: 1.579918\n",
      "Train Epoch: 1 [45760/50000 (91%)]\tLoss: 1.265224\n",
      "Train Epoch: 1 [46080/50000 (92%)]\tLoss: 1.443138\n",
      "Train Epoch: 1 [46400/50000 (93%)]\tLoss: 1.520599\n",
      "Train Epoch: 1 [46720/50000 (93%)]\tLoss: 1.507359\n",
      "Train Epoch: 1 [47040/50000 (94%)]\tLoss: 1.550708\n",
      "Train Epoch: 1 [47360/50000 (95%)]\tLoss: 1.668311\n",
      "Train Epoch: 1 [47680/50000 (95%)]\tLoss: 1.676796\n",
      "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 1.440378\n",
      "Train Epoch: 1 [48320/50000 (97%)]\tLoss: 1.747158\n",
      "Train Epoch: 1 [48640/50000 (97%)]\tLoss: 1.911581\n",
      "Train Epoch: 1 [48960/50000 (98%)]\tLoss: 1.598181\n",
      "Train Epoch: 1 [49280/50000 (99%)]\tLoss: 1.414326\n",
      "Train Epoch: 1 [49600/50000 (99%)]\tLoss: 1.590125\n",
      "Train Epoch: 1 [49920/50000 (100%)]\tLoss: 1.453162\n",
      "\n",
      "Test set: Avg. loss: 0.0523, Accuracy: 4139/10000 (41%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.487210\n",
      "Train Epoch: 2 [320/50000 (1%)]\tLoss: 1.518564\n",
      "Train Epoch: 2 [640/50000 (1%)]\tLoss: 1.447433\n",
      "Train Epoch: 2 [960/50000 (2%)]\tLoss: 1.576636\n",
      "Train Epoch: 2 [1280/50000 (3%)]\tLoss: 1.455894\n",
      "Train Epoch: 2 [1600/50000 (3%)]\tLoss: 1.248833\n",
      "Train Epoch: 2 [1920/50000 (4%)]\tLoss: 1.699690\n",
      "Train Epoch: 2 [2240/50000 (4%)]\tLoss: 1.455564\n",
      "Train Epoch: 2 [2560/50000 (5%)]\tLoss: 1.571458\n",
      "Train Epoch: 2 [2880/50000 (6%)]\tLoss: 1.427107\n",
      "Train Epoch: 2 [3200/50000 (6%)]\tLoss: 1.497116\n",
      "Train Epoch: 2 [3520/50000 (7%)]\tLoss: 1.614431\n",
      "Train Epoch: 2 [3840/50000 (8%)]\tLoss: 1.118620\n",
      "Train Epoch: 2 [4160/50000 (8%)]\tLoss: 1.290990\n",
      "Train Epoch: 2 [4480/50000 (9%)]\tLoss: 1.446411\n",
      "Train Epoch: 2 [4800/50000 (10%)]\tLoss: 1.599095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [5120/50000 (10%)]\tLoss: 1.217290\n",
      "Train Epoch: 2 [5440/50000 (11%)]\tLoss: 1.540077\n",
      "Train Epoch: 2 [5760/50000 (12%)]\tLoss: 1.310592\n",
      "Train Epoch: 2 [6080/50000 (12%)]\tLoss: 1.548982\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.560676\n",
      "Train Epoch: 2 [6720/50000 (13%)]\tLoss: 1.586344\n",
      "Train Epoch: 2 [7040/50000 (14%)]\tLoss: 1.503838\n",
      "Train Epoch: 2 [7360/50000 (15%)]\tLoss: 1.163265\n",
      "Train Epoch: 2 [7680/50000 (15%)]\tLoss: 1.378438\n",
      "Train Epoch: 2 [8000/50000 (16%)]\tLoss: 1.606966\n",
      "Train Epoch: 2 [8320/50000 (17%)]\tLoss: 1.480074\n",
      "Train Epoch: 2 [8640/50000 (17%)]\tLoss: 1.533418\n",
      "Train Epoch: 2 [8960/50000 (18%)]\tLoss: 1.892455\n",
      "Train Epoch: 2 [9280/50000 (19%)]\tLoss: 1.368538\n",
      "Train Epoch: 2 [9600/50000 (19%)]\tLoss: 1.596556\n",
      "Train Epoch: 2 [9920/50000 (20%)]\tLoss: 1.405903\n",
      "Train Epoch: 2 [10240/50000 (20%)]\tLoss: 1.653269\n",
      "Train Epoch: 2 [10560/50000 (21%)]\tLoss: 1.226272\n",
      "Train Epoch: 2 [10880/50000 (22%)]\tLoss: 1.515558\n",
      "Train Epoch: 2 [11200/50000 (22%)]\tLoss: 1.362530\n",
      "Train Epoch: 2 [11520/50000 (23%)]\tLoss: 1.597680\n",
      "Train Epoch: 2 [11840/50000 (24%)]\tLoss: 1.336126\n",
      "Train Epoch: 2 [12160/50000 (24%)]\tLoss: 1.165352\n",
      "Train Epoch: 2 [12480/50000 (25%)]\tLoss: 1.799247\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.372015\n",
      "Train Epoch: 2 [13120/50000 (26%)]\tLoss: 1.598350\n",
      "Train Epoch: 2 [13440/50000 (27%)]\tLoss: 1.562908\n",
      "Train Epoch: 2 [13760/50000 (28%)]\tLoss: 1.191963\n",
      "Train Epoch: 2 [14080/50000 (28%)]\tLoss: 1.433897\n",
      "Train Epoch: 2 [14400/50000 (29%)]\tLoss: 1.596539\n",
      "Train Epoch: 2 [14720/50000 (29%)]\tLoss: 1.324039\n",
      "Train Epoch: 2 [15040/50000 (30%)]\tLoss: 1.339923\n",
      "Train Epoch: 2 [15360/50000 (31%)]\tLoss: 1.787396\n",
      "Train Epoch: 2 [15680/50000 (31%)]\tLoss: 1.458754\n",
      "Train Epoch: 2 [16000/50000 (32%)]\tLoss: 1.459034\n",
      "Train Epoch: 2 [16320/50000 (33%)]\tLoss: 1.999238\n",
      "Train Epoch: 2 [16640/50000 (33%)]\tLoss: 1.179485\n",
      "Train Epoch: 2 [16960/50000 (34%)]\tLoss: 1.318545\n",
      "Train Epoch: 2 [17280/50000 (35%)]\tLoss: 1.558348\n",
      "Train Epoch: 2 [17600/50000 (35%)]\tLoss: 1.278318\n",
      "Train Epoch: 2 [17920/50000 (36%)]\tLoss: 1.656905\n",
      "Train Epoch: 2 [18240/50000 (36%)]\tLoss: 1.816173\n",
      "Train Epoch: 2 [18560/50000 (37%)]\tLoss: 1.566203\n",
      "Train Epoch: 2 [18880/50000 (38%)]\tLoss: 1.675644\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 2.200242\n",
      "Train Epoch: 2 [19520/50000 (39%)]\tLoss: 1.792709\n",
      "Train Epoch: 2 [19840/50000 (40%)]\tLoss: 1.370314\n",
      "Train Epoch: 2 [20160/50000 (40%)]\tLoss: 1.607828\n",
      "Train Epoch: 2 [20480/50000 (41%)]\tLoss: 1.332204\n",
      "Train Epoch: 2 [20800/50000 (42%)]\tLoss: 1.607164\n",
      "Train Epoch: 2 [21120/50000 (42%)]\tLoss: 1.164943\n",
      "Train Epoch: 2 [21440/50000 (43%)]\tLoss: 1.968545\n",
      "Train Epoch: 2 [21760/50000 (44%)]\tLoss: 1.432613\n",
      "Train Epoch: 2 [22080/50000 (44%)]\tLoss: 1.635423\n",
      "Train Epoch: 2 [22400/50000 (45%)]\tLoss: 1.796401\n",
      "Train Epoch: 2 [22720/50000 (45%)]\tLoss: 1.646775\n",
      "Train Epoch: 2 [23040/50000 (46%)]\tLoss: 1.619406\n",
      "Train Epoch: 2 [23360/50000 (47%)]\tLoss: 1.558128\n",
      "Train Epoch: 2 [23680/50000 (47%)]\tLoss: 1.711059\n",
      "Train Epoch: 2 [24000/50000 (48%)]\tLoss: 1.502167\n",
      "Train Epoch: 2 [24320/50000 (49%)]\tLoss: 1.167048\n",
      "Train Epoch: 2 [24640/50000 (49%)]\tLoss: 1.246352\n",
      "Train Epoch: 2 [24960/50000 (50%)]\tLoss: 1.696059\n",
      "Train Epoch: 2 [25280/50000 (51%)]\tLoss: 1.409403\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.395316\n",
      "Train Epoch: 2 [25920/50000 (52%)]\tLoss: 1.294248\n",
      "Train Epoch: 2 [26240/50000 (52%)]\tLoss: 1.530198\n",
      "Train Epoch: 2 [26560/50000 (53%)]\tLoss: 1.504049\n",
      "Train Epoch: 2 [26880/50000 (54%)]\tLoss: 1.063510\n",
      "Train Epoch: 2 [27200/50000 (54%)]\tLoss: 1.608934\n",
      "Train Epoch: 2 [27520/50000 (55%)]\tLoss: 1.565779\n",
      "Train Epoch: 2 [27840/50000 (56%)]\tLoss: 1.541715\n",
      "Train Epoch: 2 [28160/50000 (56%)]\tLoss: 1.223197\n",
      "Train Epoch: 2 [28480/50000 (57%)]\tLoss: 1.486134\n",
      "Train Epoch: 2 [28800/50000 (58%)]\tLoss: 1.686013\n",
      "Train Epoch: 2 [29120/50000 (58%)]\tLoss: 1.096690\n",
      "Train Epoch: 2 [29440/50000 (59%)]\tLoss: 1.813511\n",
      "Train Epoch: 2 [29760/50000 (60%)]\tLoss: 1.587995\n",
      "Train Epoch: 2 [30080/50000 (60%)]\tLoss: 1.550843\n",
      "Train Epoch: 2 [30400/50000 (61%)]\tLoss: 1.233182\n",
      "Train Epoch: 2 [30720/50000 (61%)]\tLoss: 1.527638\n",
      "Train Epoch: 2 [31040/50000 (62%)]\tLoss: 1.500942\n",
      "Train Epoch: 2 [31360/50000 (63%)]\tLoss: 1.367828\n",
      "Train Epoch: 2 [31680/50000 (63%)]\tLoss: 1.682537\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.612221\n",
      "Train Epoch: 2 [32320/50000 (65%)]\tLoss: 1.450812\n",
      "Train Epoch: 2 [32640/50000 (65%)]\tLoss: 2.018629\n",
      "Train Epoch: 2 [32960/50000 (66%)]\tLoss: 1.413976\n",
      "Train Epoch: 2 [33280/50000 (67%)]\tLoss: 1.581711\n",
      "Train Epoch: 2 [33600/50000 (67%)]\tLoss: 1.647541\n",
      "Train Epoch: 2 [33920/50000 (68%)]\tLoss: 1.332765\n",
      "Train Epoch: 2 [34240/50000 (68%)]\tLoss: 1.538886\n",
      "Train Epoch: 2 [34560/50000 (69%)]\tLoss: 1.733305\n",
      "Train Epoch: 2 [34880/50000 (70%)]\tLoss: 1.545488\n",
      "Train Epoch: 2 [35200/50000 (70%)]\tLoss: 1.772666\n",
      "Train Epoch: 2 [35520/50000 (71%)]\tLoss: 1.298911\n",
      "Train Epoch: 2 [35840/50000 (72%)]\tLoss: 1.326328\n",
      "Train Epoch: 2 [36160/50000 (72%)]\tLoss: 1.316236\n",
      "Train Epoch: 2 [36480/50000 (73%)]\tLoss: 1.757655\n",
      "Train Epoch: 2 [36800/50000 (74%)]\tLoss: 1.435799\n",
      "Train Epoch: 2 [37120/50000 (74%)]\tLoss: 1.420761\n",
      "Train Epoch: 2 [37440/50000 (75%)]\tLoss: 1.088349\n",
      "Train Epoch: 2 [37760/50000 (75%)]\tLoss: 1.298937\n",
      "Train Epoch: 2 [38080/50000 (76%)]\tLoss: 1.537808\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.475724\n",
      "Train Epoch: 2 [38720/50000 (77%)]\tLoss: 1.266079\n",
      "Train Epoch: 2 [39040/50000 (78%)]\tLoss: 1.438977\n",
      "Train Epoch: 2 [39360/50000 (79%)]\tLoss: 0.936754\n",
      "Train Epoch: 2 [39680/50000 (79%)]\tLoss: 1.421935\n",
      "Train Epoch: 2 [40000/50000 (80%)]\tLoss: 1.655030\n",
      "Train Epoch: 2 [40320/50000 (81%)]\tLoss: 1.327681\n",
      "Train Epoch: 2 [40640/50000 (81%)]\tLoss: 1.262347\n",
      "Train Epoch: 2 [40960/50000 (82%)]\tLoss: 1.329151\n",
      "Train Epoch: 2 [41280/50000 (83%)]\tLoss: 1.387835\n",
      "Train Epoch: 2 [41600/50000 (83%)]\tLoss: 1.551351\n",
      "Train Epoch: 2 [41920/50000 (84%)]\tLoss: 1.682244\n",
      "Train Epoch: 2 [42240/50000 (84%)]\tLoss: 1.647277\n",
      "Train Epoch: 2 [42560/50000 (85%)]\tLoss: 1.471616\n",
      "Train Epoch: 2 [42880/50000 (86%)]\tLoss: 1.357619\n",
      "Train Epoch: 2 [43200/50000 (86%)]\tLoss: 1.220371\n",
      "Train Epoch: 2 [43520/50000 (87%)]\tLoss: 1.274402\n",
      "Train Epoch: 2 [43840/50000 (88%)]\tLoss: 1.327229\n",
      "Train Epoch: 2 [44160/50000 (88%)]\tLoss: 1.327053\n",
      "Train Epoch: 2 [44480/50000 (89%)]\tLoss: 1.436357\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 1.039740\n",
      "Train Epoch: 2 [45120/50000 (90%)]\tLoss: 1.586961\n",
      "Train Epoch: 2 [45440/50000 (91%)]\tLoss: 1.289508\n",
      "Train Epoch: 2 [45760/50000 (91%)]\tLoss: 1.311605\n",
      "Train Epoch: 2 [46080/50000 (92%)]\tLoss: 1.827332\n",
      "Train Epoch: 2 [46400/50000 (93%)]\tLoss: 1.604179\n",
      "Train Epoch: 2 [46720/50000 (93%)]\tLoss: 1.226768\n",
      "Train Epoch: 2 [47040/50000 (94%)]\tLoss: 2.001592\n",
      "Train Epoch: 2 [47360/50000 (95%)]\tLoss: 1.932501\n",
      "Train Epoch: 2 [47680/50000 (95%)]\tLoss: 1.675541\n",
      "Train Epoch: 2 [48000/50000 (96%)]\tLoss: 1.416943\n",
      "Train Epoch: 2 [48320/50000 (97%)]\tLoss: 1.748302\n",
      "Train Epoch: 2 [48640/50000 (97%)]\tLoss: 1.645145\n",
      "Train Epoch: 2 [48960/50000 (98%)]\tLoss: 1.465752\n",
      "Train Epoch: 2 [49280/50000 (99%)]\tLoss: 1.445107\n",
      "Train Epoch: 2 [49600/50000 (99%)]\tLoss: 1.362153\n",
      "Train Epoch: 2 [49920/50000 (100%)]\tLoss: 1.383206\n",
      "\n",
      "Test set: Avg. loss: 0.0478, Accuracy: 4688/10000 (47%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.716216\n",
      "Train Epoch: 3 [320/50000 (1%)]\tLoss: 1.517882\n",
      "Train Epoch: 3 [640/50000 (1%)]\tLoss: 1.595409\n",
      "Train Epoch: 3 [960/50000 (2%)]\tLoss: 1.569484\n",
      "Train Epoch: 3 [1280/50000 (3%)]\tLoss: 1.225750\n",
      "Train Epoch: 3 [1600/50000 (3%)]\tLoss: 1.297568\n",
      "Train Epoch: 3 [1920/50000 (4%)]\tLoss: 1.602660\n",
      "Train Epoch: 3 [2240/50000 (4%)]\tLoss: 1.560364\n",
      "Train Epoch: 3 [2560/50000 (5%)]\tLoss: 1.443702\n",
      "Train Epoch: 3 [2880/50000 (6%)]\tLoss: 1.564966\n",
      "Train Epoch: 3 [3200/50000 (6%)]\tLoss: 1.609124\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-8f57601ca574>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mdu_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mbp_VGG5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleak\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdu_out\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmom\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpre_linear_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;31m#             print(\"memory after bp\",torch.cuda.memory_allocated()/10000000)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-7e838e648fa8>\u001b[0m in \u001b[0;36mbp_VGG5\u001b[1;34m(vgg, leak, time_step, du_out, l_r, mom, th, pre_linear_dim, input_dim)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mdu_pool2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpool2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdx_pool2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool2_ind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mds_conv3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdu_pool2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_lif3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdu_regs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mleak\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_lif3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdu_regs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mdu_conv3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_conv3\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mde_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_lif3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mu_regs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_lif3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdu_regs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mleak\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_lif3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms_regs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_lif3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdu_regs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdu_conv3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mdW_conv3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_weight_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdu_pool2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_lif2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms_regs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmom\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mprev_conv3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-6aa90361ba5f>\u001b[0m in \u001b[0;36mde_func\u001b[1;34m(U, th)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mU\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mU\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_step = 8\n",
    "leak = 0.99\n",
    "\n",
    "data = \"cifar10\"\n",
    "toy = VGG_5(time_step,leak,data).cuda()\n",
    "\n",
    "if data == \"cifar10\":\n",
    "    train_loader = train_loader_cifar10\n",
    "    input_dim = 3\n",
    "    pre_linear_dim = 8\n",
    "    test_loader = test_loader_cifar10\n",
    "elif data == \"mnist\":\n",
    "    train_loader = train_loader_mnist\n",
    "    input_dim = 1\n",
    "    pre_linear_dim = 7\n",
    "    test_loader = test_loader_mnist\n",
    "# vgg = VGG_5(time_step)\n",
    "# vgg =vgg.cuda()\n",
    "# print(\"weight\",toy.fc_1.weight)\n",
    "# torch.nn.init.normal_(toy.fc_1.weight, mean=0.0, std=0.1)\n",
    "# toy.fc_1.weight.data = quant(toy.fc_1.weight,2**4)\n",
    "# torch.nn.init.normal_(toy.fc_2.weight, mean=0.0, std=0.1)\n",
    "# toy.fc_2.weight.data = quant(toy.fc_2.weight,2**4)\n",
    "# torch.nn.init.normal_(toy.fc_out.weight, mean=0.0, std=0.1)\n",
    "# toy.fc_out.weight.data = quant(toy.fc_out.weight,2**4)\n",
    "# print(\"quantized weight\",toy.fc_1.weight)\n",
    "lr = 0.005\n",
    "mom = 0.9\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# test(toy,data,test_loader)\n",
    "running_loss = 0.0\n",
    "for epoch in range(15):\n",
    "#     if epoch > 2:\n",
    "#         lr = 0.004\n",
    "#     if epoch > 4:\n",
    "#         lr = 0.002\n",
    "#     if epoch>8:\n",
    "#         lr = 0.001\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # print(torch.mean(data))\n",
    "        with torch.no_grad():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "            out = toy(data)\n",
    "#             print(\"memory after fwd\",torch.cuda.memory_allocated()/10000000)\n",
    "        out = Variable(out,requires_grad=True)\n",
    "\n",
    "        # err = loss(out,target,reduction='sum')\n",
    "        err = F.cross_entropy(out, target,reduction='mean')\n",
    "        err.backward()\n",
    "\n",
    "        # exp = torch.exp(out)\n",
    "        # exp_sum = torch.sum(torch.exp(out),1, keepdim=True)   \n",
    "        # target = F.one_hot(target, num_classes=10)\n",
    "        # #L = -1*torch.sum((target*torch.log((exp/exp_sum))),1, keepdim=True)\n",
    "        # du_out = exp/exp_sum\n",
    "        # du_out = (du_out - target)/batch_size_train\n",
    "        du_out = out.grad\n",
    "\n",
    "        bp_VGG5(toy,leak,time_step,du_out,lr,mom,0.5,pre_linear_dim,input_dim)\n",
    "#             print(\"memory after bp\",torch.cuda.memory_allocated()/10000000)\n",
    "\n",
    "        # bp_MLP(toy,leak,time_step,du_out,toy.s_regs_inp,lr,0.5)\n",
    "\n",
    "\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), err.item()))\n",
    "            \n",
    "        # print statistics\n",
    "        # running_loss += err.item()\n",
    "        # if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "        #     print('[%d, %5d] loss: %.3f' %\n",
    "        #           (epoch + 1, i + 1, running_loss / 2000))\n",
    "        #     running_loss = 0.0\n",
    "\n",
    "#             del toy.lif_conv1.s_regs\n",
    "#             del toy.lif_conv1.u_regs\n",
    "#             del toy.lif_conv1.du_regs\n",
    "#             del toy.lif_fc1.du_regs\n",
    "#             del toy.lif_fc1.u_regs\n",
    "#             del toy.lif_fc1.s_regs\n",
    "#             del toy.s_regs_conv\n",
    "#             del toy.s_regs_inp\n",
    "#             del data\n",
    "#             del target\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#             gc.collect()\n",
    "#             print(\"memory after clear\",torch.cuda.memory_allocated()/10000000)\n",
    "\n",
    "    test(toy,data,test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ec3880-e283-4519-8552-3a502e0fae78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3842f727-73a7-4f8d-9f6d-44b90e602b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db95058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class test(nn.Module):\n",
    "#     def __init__(self,time_step, leak, data):\n",
    "#         super(test, self).__init__()\n",
    "        \n",
    "#         if data == \"cifar10\":\n",
    "#             input_dim = 3\n",
    "#             pre_linear_dim = 8\n",
    "#         elif data == \"mnist\":\n",
    "#             input_dim = 1\n",
    "#             pre_linear_dim = 7\n",
    "            \n",
    "#         self.conv1 = nn.Conv2d(input_dim, 64, kernel_size=3, padding=1, bias=False)\n",
    "#         self.conv_lif1 = LIF(time_step, leak)\n",
    "#         self.pool1 = nn.MaxPool2d(kernel_size=2,return_indices=True)\n",
    "#         self.pool1_ind = []\n",
    "#         self.unpool1 = nn.MaxUnpool2d(kernel_size=2)\n",
    "        \n",
    "#         self.fc1 = nn.Linear(128 * pre_linear_dim * pre_linear_dim, 1024, bias=False)\n",
    "#         self.fc_lif1 = LIF(time_step,leak)\n",
    "#         self.fc_out = nn.Linear(1024, 10, bias=False)\n",
    "        \n",
    "        \n",
    "#         def forward(self, inp):\n",
    "\n",
    "#         size = inp.shape\n",
    "#         self.s_regs_inp = torch.zeros(self.time_step,*size, device=device)\n",
    "#         self.pool1_ind = []\n",
    "#         u_out = 0\n",
    "        \n",
    "#         for t in range(self.time_step):\n",
    "#             spike_inp = PoissonGen(inp)\n",
    "#             self.s_regs_inp[t] += spike_inp \n",
    "#             x = self.conv1(spike_inp)\n",
    "#             x = self.conv_lif1(x,t)\n",
    "#             x,indices = self.pool1(x)\n",
    "#             self.pool1_ind.append(indices)\n",
    "#             x = x.view(x.shape[0],-1)\n",
    "            \n",
    "#             if t == 0:\n",
    "#                 self.s_regs_conv = torch.zeros(self.time_step,*x.shape, device=device)\n",
    "#             self.s_regs_conv[t] += x\n",
    "            \n",
    "#             x = self.fc1(x)\n",
    "#             x = self.fc_lif1(x,t)\n",
    "#             x = self.fc_out(x)\n",
    "#             u_out = u_out + x\n",
    "#         return u_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed613b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.rand(10,3,4,4,device=device,requires_grad=True)\n",
    "# w = torch.randn(4,3,3,3,device=device,requires_grad=True)\n",
    "# conv1 = F.conv2d(a,w,padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa40b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv1.backward(torch.ones_like(conv1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c934e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dw = conv_weight_update(torch.ones_like(conv1),a,1)\n",
    "# dx = conv_dx_update(torch.ones_like(conv1),w,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5391920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82474b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a17286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.sum(dw-w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a00016e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
