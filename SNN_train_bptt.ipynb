{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251e3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b07e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33e21ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PoissonGen(inp, rescale_fac=2.0):\n",
    "    rand_inp = torch.rand_like(inp)\n",
    "    return torch.mul(torch.le(rand_inp * rescale_fac, torch.abs(inp)).float(), torch.sign(inp))\n",
    "\n",
    "def spike_function(x, k):\n",
    "    x[x>0] = 1\n",
    "    x[x<=0] = 0\n",
    "    return x\n",
    "\n",
    "def de_func(U,th):\n",
    "    alpha = 0.3\n",
    "    U = alpha*(1.0 - abs((U-th)/th))\n",
    "    U[U<0]=0\n",
    "    return U\n",
    "\n",
    "def test():\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        output = toy(data)\n",
    "        test_loss +=F.cross_entropy(output, target, size_average=False).item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "def quant(input, k):\n",
    "    size = input.size()\n",
    "    #mean = torch.mean(input.abs(), 1, keepdim=True)\n",
    "    x = input\n",
    "    #print(x)\n",
    "    xmax = x.abs().max()\n",
    "    num_bits=k\n",
    "    v0 = 1\n",
    "    v1 = 2\n",
    "    v2 = -0.5\n",
    "    y = k #2.**num_bits - 1.\n",
    "    #print(y)\n",
    "    x = x.add(v0).div(v1)\n",
    "    #print(x)\n",
    "    x = x.mul(y).round_()\n",
    "    #print(x)\n",
    "    x = x.div(y)\n",
    "    #print(x)\n",
    "    x = x.add(v2)\n",
    "    #print(x)\n",
    "    x = x.mul(v1)\n",
    "    #print(x)\n",
    "    input = x\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d766c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(28*28,256,bias=False)\n",
    "        self.fc_2 = nn.Linear(256,256,bias=False)\n",
    "        self.fc_out = nn.Linear(256,10,bias=False)\n",
    "        self.lif1 = LIF()\n",
    "        self.lif2 = LIF()\n",
    "        self.time_step = 5\n",
    "        self.s_regs_inp = None\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        inp = inp.view(inp.shape[0],-1)\n",
    "        size = inp.shape\n",
    "        self.s_regs_inp = torch.zeros(self.time_step,*size)\n",
    "        u_out = 0\n",
    "        \n",
    "        for t in range(self.time_step):\n",
    "            \n",
    "            spike_inp = PoissonGen(inp)\n",
    "            self.s_regs_inp[t] += spike_inp \n",
    "            \n",
    "            x = self.fc_1(spike_inp)\n",
    "            x = self.lif1(x, t)\n",
    "            x = self.fc_2(x)\n",
    "            x = self.lif2(x, t)\n",
    "            x = self.fc_out(x)\n",
    "            u_out = u_out + x\n",
    "        return u_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8d72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(28*28,512,bias=False)\n",
    "        self.fc_out = nn.Linear(512,10,bias=False)\n",
    "        self.lif1 = LIF()\n",
    "        self.time_step = 5\n",
    "        self.s_regs_inp = None\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        inp = inp.view(inp.shape[0],-1)\n",
    "        size = inp.shape\n",
    "        self.s_regs_inp = torch.zeros(self.time_step,*size)\n",
    "        u_out = 0\n",
    "        \n",
    "        for t in range(self.time_step):\n",
    "            spike_inp = PoissonGen(inp)\n",
    "            self.s_regs_inp[t] += spike_inp \n",
    "            x = self.fc_1(spike_inp)\n",
    "            x = quant(x,4)\n",
    "            x = self.lif1(x, t)\n",
    "            x = self.fc_out(x)\n",
    "            u_out = u_out + x\n",
    "        return u_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e6f78f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LIF, self).__init__()\n",
    "        \n",
    "        self.u_regs = None\n",
    "        self.du_regs = None\n",
    "        self.s_regs = None\n",
    "        self.leak = 0.99\n",
    "        self.time_step = 5\n",
    "        self.thresh = 0.5\n",
    "        \n",
    "    def forward(self,inp,t):\n",
    "        if t == 0:\n",
    "            size = inp.shape\n",
    "            self.u_regs = torch.zeros(self.time_step,*size)\n",
    "            self.du_regs = torch.zeros(self.time_step,*size)\n",
    "            self.u_regs[0] = inp\n",
    "            self.s_regs = torch.zeros(self.time_step,*size)\n",
    "\n",
    "            vol = inp - self.thresh\n",
    "\n",
    "            spike = spike_function(vol, k=1)\n",
    "\n",
    "            self.s_regs[0] = spike\n",
    "        else:\n",
    "            self.u_regs[t] = self.leak * self.u_regs[t-1] * (1 - self.s_regs[t-1]) + inp\n",
    "\n",
    "            vol = self.u_regs[t] - self.thresh\n",
    "\n",
    "            spike = spike_function(vol, k=1)\n",
    "\n",
    "            self.s_regs[t] = spike\n",
    "        return spike\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8fc2dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Back propagation for MLP\n",
    "def bp_MLP(toy,leak,time_step,du_out,s_regs_inp,l_r,th):\n",
    "    \n",
    "    ## First fc\n",
    "    du_fc1 = torch.matmul(du_out,toy.fc_out.weight)*de_func(toy.lif1.u_regs[-1],th)\n",
    "    toy.lif1.du_regs[-1] += du_fc1\n",
    "\n",
    "    ## Update weight\n",
    "    w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[-1])\n",
    "    toy.fc_1.weight.data -= l_r*quant(w_inp_1,4)\n",
    "#     toy.fc_1.weight.data -= l_r*w_inp_1\n",
    "    \n",
    "    w_1_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif1.s_regs[-1])\n",
    "    toy.fc_out.weight.data -= l_r*quant(w_1_out,4)\n",
    "#     toy.fc_out.weight.data -= l_r*w_1_out\n",
    "\n",
    "    for t in range(time_step-2,-1,-1):\n",
    "        \n",
    "        ## First fc\n",
    "        ds_fc1 = torch.matmul(du_out,toy.fc_out.weight)+toy.lif1.du_regs[t+1]*(-leak*toy.lif1.u_regs[t])\n",
    "        du_fc1 = (ds_fc1)*de_func(toy.lif1.u_regs[t],th) + toy.lif1.du_regs[t+1]*leak*(1-toy.lif1.s_regs[t])\n",
    "        toy.lif1.du_regs[t] += du_fc1\n",
    "\n",
    "        ## Update weight\n",
    "        w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[t])\n",
    "        toy.fc_1.weight.data -= l_r*quant(w_inp_1,4)\n",
    "#         toy.fc_1.weight.data -= l_r*w_inp_1\n",
    "\n",
    "        w_1_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif1.s_regs[t])\n",
    "        toy.fc_out.weight.data -= l_r*quant(w_1_out,4)\n",
    "#         toy.fc_out.weight.data -= l_r*w_1_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b08084b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Back propagation\n",
    "def bp(toy,leak,time_step,du_out,s_regs_inp,l_r,th):\n",
    "    \n",
    "    ## Second fc    \n",
    "    du_fc2 = torch.matmul(du_out,toy.fc_out.weight)*de_func(toy.lif2.u_regs[-1],th)    \n",
    "    toy.lif2.du_regs[-1] = toy.lif2.du_regs[-1] + du_fc2\n",
    "    \n",
    "    ## First fc\n",
    "    du_fc1 = torch.matmul(du_fc2,toy.fc_2.weight)*de_func(toy.lif1.u_regs[-1],th)\n",
    "    toy.lif1.du_regs[-1] += du_fc1\n",
    "\n",
    "    \n",
    "    ## Update weight\n",
    "    w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[-1])\n",
    "    toy.fc_1.weight.data -= l_r*w_inp_1\n",
    "\n",
    "    w_1_2 = torch.matmul(torch.transpose(du_fc2,0,1),toy.lif1.s_regs[-1])\n",
    "    toy.fc_2.weight.data -= l_r*w_1_2\n",
    "\n",
    "    w_2_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif2.s_regs[-1])\n",
    "    toy.fc_out.weight.data -= l_r*w_2_out\n",
    "\n",
    "    for t in range(time_step-2,-1,-1):\n",
    "\n",
    "        ds_fc2 = torch.matmul(du_out,toy.fc_out.weight)+toy.lif2.du_regs[t+1]*(-leak*toy.lif2.u_regs[t])\n",
    "        du_fc2 = (ds_fc2)*de_func(toy.lif2.u_regs[t],th) + toy.lif2.du_regs[t+1]*leak*(1-toy.lif2.s_regs[t])\n",
    "        toy.lif2.du_regs[t] += du_fc2\n",
    "        \n",
    "        ## First fc\n",
    "        ds_fc1 = torch.matmul(du_fc2,toy.fc_2.weight)+toy.lif1.du_regs[t+1]*(-leak*toy.lif1.u_regs[t])\n",
    "        du_fc1 = (ds_fc1)*de_func(toy.lif1.u_regs[t],th) + toy.lif1.du_regs[t+1]*leak*(1-toy.lif1.s_regs[t])\n",
    "        toy.lif1.du_regs[t] += du_fc1\n",
    "\n",
    "        ## Update weight\n",
    "        w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[t])\n",
    "        toy.fc_1.weight.data -= l_r*w_inp_1\n",
    "\n",
    "        w_1_2 = torch.matmul(torch.transpose(du_fc2,0,1),toy.lif1.s_regs[t])\n",
    "        toy.fc_2.weight.data -= l_r*w_1_2\n",
    "\n",
    "        w_2_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif2.s_regs[t])\n",
    "        toy.fc_out.weight.data -= l_r*w_2_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c3de0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b70c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(5 + 1)]\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2ce9ca7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ruokai\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3788, Accuracy: 810/10000 (8%)\n",
      "\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.412682\n",
      "Train Epoch: 0 [640/60000 (1%)]\tLoss: 1.758564\n",
      "Train Epoch: 0 [1280/60000 (2%)]\tLoss: 1.344471\n",
      "Train Epoch: 0 [1920/60000 (3%)]\tLoss: 1.238862\n",
      "Train Epoch: 0 [2560/60000 (4%)]\tLoss: 1.150987\n",
      "Train Epoch: 0 [3200/60000 (5%)]\tLoss: 0.929962\n",
      "Train Epoch: 0 [3840/60000 (6%)]\tLoss: 0.848691\n",
      "Train Epoch: 0 [4480/60000 (7%)]\tLoss: 0.740039\n",
      "Train Epoch: 0 [5120/60000 (9%)]\tLoss: 0.951320\n",
      "Train Epoch: 0 [5760/60000 (10%)]\tLoss: 0.667834\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.643327\n",
      "Train Epoch: 0 [7040/60000 (12%)]\tLoss: 0.655383\n",
      "Train Epoch: 0 [7680/60000 (13%)]\tLoss: 0.689219\n",
      "Train Epoch: 0 [8320/60000 (14%)]\tLoss: 0.707154\n",
      "Train Epoch: 0 [8960/60000 (15%)]\tLoss: 0.531569\n",
      "Train Epoch: 0 [9600/60000 (16%)]\tLoss: 0.555789\n",
      "Train Epoch: 0 [10240/60000 (17%)]\tLoss: 0.640221\n",
      "Train Epoch: 0 [10880/60000 (18%)]\tLoss: 0.710306\n",
      "Train Epoch: 0 [11520/60000 (19%)]\tLoss: 0.707092\n",
      "Train Epoch: 0 [12160/60000 (20%)]\tLoss: 0.564331\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.590599\n",
      "Train Epoch: 0 [13440/60000 (22%)]\tLoss: 0.498172\n",
      "Train Epoch: 0 [14080/60000 (23%)]\tLoss: 0.549775\n",
      "Train Epoch: 0 [14720/60000 (25%)]\tLoss: 0.589149\n",
      "Train Epoch: 0 [15360/60000 (26%)]\tLoss: 0.577179\n",
      "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 0.580226\n",
      "Train Epoch: 0 [16640/60000 (28%)]\tLoss: 0.674140\n",
      "Train Epoch: 0 [17280/60000 (29%)]\tLoss: 0.542178\n",
      "Train Epoch: 0 [17920/60000 (30%)]\tLoss: 0.501208\n",
      "Train Epoch: 0 [18560/60000 (31%)]\tLoss: 0.561870\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.480175\n",
      "Train Epoch: 0 [19840/60000 (33%)]\tLoss: 0.515979\n",
      "Train Epoch: 0 [20480/60000 (34%)]\tLoss: 0.651611\n",
      "Train Epoch: 0 [21120/60000 (35%)]\tLoss: 0.492802\n",
      "Train Epoch: 0 [21760/60000 (36%)]\tLoss: 0.513511\n",
      "Train Epoch: 0 [22400/60000 (37%)]\tLoss: 0.564889\n",
      "Train Epoch: 0 [23040/60000 (38%)]\tLoss: 0.558792\n",
      "Train Epoch: 0 [23680/60000 (39%)]\tLoss: 0.460076\n",
      "Train Epoch: 0 [24320/60000 (41%)]\tLoss: 0.719973\n",
      "Train Epoch: 0 [24960/60000 (42%)]\tLoss: 0.462604\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.506205\n",
      "Train Epoch: 0 [26240/60000 (44%)]\tLoss: 0.502973\n",
      "Train Epoch: 0 [26880/60000 (45%)]\tLoss: 0.574429\n",
      "Train Epoch: 0 [27520/60000 (46%)]\tLoss: 0.446841\n",
      "Train Epoch: 0 [28160/60000 (47%)]\tLoss: 0.432318\n",
      "Train Epoch: 0 [28800/60000 (48%)]\tLoss: 0.557502\n",
      "Train Epoch: 0 [29440/60000 (49%)]\tLoss: 0.638791\n",
      "Train Epoch: 0 [30080/60000 (50%)]\tLoss: 0.381288\n",
      "Train Epoch: 0 [30720/60000 (51%)]\tLoss: 0.490728\n",
      "Train Epoch: 0 [31360/60000 (52%)]\tLoss: 0.329308\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.338478\n",
      "Train Epoch: 0 [32640/60000 (54%)]\tLoss: 0.390383\n",
      "Train Epoch: 0 [33280/60000 (55%)]\tLoss: 0.552425\n",
      "Train Epoch: 0 [33920/60000 (57%)]\tLoss: 0.384897\n",
      "Train Epoch: 0 [34560/60000 (58%)]\tLoss: 0.391830\n",
      "Train Epoch: 0 [35200/60000 (59%)]\tLoss: 0.251301\n",
      "Train Epoch: 0 [35840/60000 (60%)]\tLoss: 0.458357\n",
      "Train Epoch: 0 [36480/60000 (61%)]\tLoss: 0.500848\n",
      "Train Epoch: 0 [37120/60000 (62%)]\tLoss: 0.434496\n",
      "Train Epoch: 0 [37760/60000 (63%)]\tLoss: 0.277985\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.444265\n",
      "Train Epoch: 0 [39040/60000 (65%)]\tLoss: 0.428796\n",
      "Train Epoch: 0 [39680/60000 (66%)]\tLoss: 0.411817\n",
      "Train Epoch: 0 [40320/60000 (67%)]\tLoss: 0.621050\n",
      "Train Epoch: 0 [40960/60000 (68%)]\tLoss: 0.321410\n",
      "Train Epoch: 0 [41600/60000 (69%)]\tLoss: 0.456044\n",
      "Train Epoch: 0 [42240/60000 (70%)]\tLoss: 0.472984\n",
      "Train Epoch: 0 [42880/60000 (71%)]\tLoss: 0.622096\n",
      "Train Epoch: 0 [43520/60000 (72%)]\tLoss: 0.319293\n",
      "Train Epoch: 0 [44160/60000 (74%)]\tLoss: 0.475921\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.354229\n",
      "Train Epoch: 0 [45440/60000 (76%)]\tLoss: 0.576302\n",
      "Train Epoch: 0 [46080/60000 (77%)]\tLoss: 0.282377\n",
      "Train Epoch: 0 [46720/60000 (78%)]\tLoss: 0.531157\n",
      "Train Epoch: 0 [47360/60000 (79%)]\tLoss: 0.449862\n",
      "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 0.366992\n",
      "Train Epoch: 0 [48640/60000 (81%)]\tLoss: 0.459629\n",
      "Train Epoch: 0 [49280/60000 (82%)]\tLoss: 0.369549\n",
      "Train Epoch: 0 [49920/60000 (83%)]\tLoss: 0.566797\n",
      "Train Epoch: 0 [50560/60000 (84%)]\tLoss: 0.511678\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.329615\n",
      "Train Epoch: 0 [51840/60000 (86%)]\tLoss: 0.590522\n",
      "Train Epoch: 0 [52480/60000 (87%)]\tLoss: 0.233067\n",
      "Train Epoch: 0 [53120/60000 (88%)]\tLoss: 0.351518\n",
      "Train Epoch: 0 [53760/60000 (90%)]\tLoss: 0.524708\n",
      "Train Epoch: 0 [54400/60000 (91%)]\tLoss: 0.231643\n",
      "Train Epoch: 0 [55040/60000 (92%)]\tLoss: 0.353563\n",
      "Train Epoch: 0 [55680/60000 (93%)]\tLoss: 0.519973\n",
      "Train Epoch: 0 [56320/60000 (94%)]\tLoss: 0.648092\n",
      "Train Epoch: 0 [56960/60000 (95%)]\tLoss: 0.617976\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.291263\n",
      "Train Epoch: 0 [58240/60000 (97%)]\tLoss: 0.479315\n",
      "Train Epoch: 0 [58880/60000 (98%)]\tLoss: 0.434566\n",
      "Train Epoch: 0 [59520/60000 (99%)]\tLoss: 0.473378\n",
      "\n",
      "Test set: Avg. loss: 0.3739, Accuracy: 8923/10000 (89%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.398084\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.334481\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.341855\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.400641\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.356089\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.312922\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.314762\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.509848\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.447902\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.479750\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.299318\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.458201\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.362686\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.310954\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.313763\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.350504\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.502820\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.273780\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.357611\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.657180\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.533910\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.462895\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.277338\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.348829\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.514832\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.309572\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.418639\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.420318\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.616454\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.418310\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.385369\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.354965\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.309131\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.459171\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.434700\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.406570\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.412089\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.451022\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.466305\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.338361\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.508907\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.246972\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.441497\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.445949\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.248136\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.268508\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.267402\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.345199\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.536551\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.421151\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.635167\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.345415\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.281812\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.411172\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.408639\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.417584\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.659171\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.465019\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.319591\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.578318\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.678383\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.274552\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.403659\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.381763\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.433489\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.433427\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.510819\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.267686\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.400734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.468403\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.390202\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.315817\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.458415\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.335353\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.346387\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.438205\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.259317\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.460880\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.262963\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.535317\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.264793\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.238257\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.550445\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.417763\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.319437\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.424231\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.392273\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.259963\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.371085\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.379808\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.185813\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.306245\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.332195\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.364027\n",
      "\n",
      "Test set: Avg. loss: 0.3468, Accuracy: 8961/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.486877\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.229412\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.477698\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.220124\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.325136\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.324060\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.328263\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.460991\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.402802\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.269049\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.322647\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.298869\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.230807\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.225770\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.248610\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.494970\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.412226\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.497616\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.268991\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.290320\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.485724\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.195089\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.281499\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.340421\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.275044\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.291680\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.290053\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.490394\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.338473\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.187309\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.265146\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.426684\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.301104\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.316170\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.258027\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.263026\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.233654\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.438527\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.293649\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.238781\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.333267\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.274327\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.459707\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.236142\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.273452\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.228080\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.467635\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.361146\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.484737\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.267605\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.341288\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.389759\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.293928\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.247010\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.425137\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.528290\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.204531\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.294061\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.317664\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.403744\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.297657\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.273451\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.504454\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.300487\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.207028\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.401985\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.436161\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.241103\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.274747\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.228324\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.388181\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.168857\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.295018\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.525039\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.271955\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.312348\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.380174\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.403298\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.261531\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.367340\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.220549\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.328400\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.330452\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.384322\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.216635\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.252812\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.315297\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.241506\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.277360\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.287876\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.228043\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.411686\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.462915\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.278020\n",
      "\n",
      "Test set: Avg. loss: 0.3084, Accuracy: 9090/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.335059\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.418308\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.409944\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.218336\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.314977\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.361616\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.177177\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.379940\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.398114\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.242803\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.482107\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.326612\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.435226\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.281210\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.272364\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.191864\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.186193\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.286499\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.181918\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.352154\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.294383\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.439226\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.366278\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.284769\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.145522\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.435414\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.173784\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.164891\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.269406\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.456153\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.295724\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.373143\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.417553\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.336773\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.467931\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.373103\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.205033\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.339346\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.420811\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.397638\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.401273\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.185785\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.299079\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.303675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.241789\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.259808\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.318320\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.296121\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.469909\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.413953\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.183141\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.305228\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.277475\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.412391\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.144194\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.312298\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.279090\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.332774\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.274555\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.279662\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.328158\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.259188\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.208457\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.207037\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.180577\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.417753\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.290277\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.468307\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.206302\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.227595\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.329682\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.304276\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.301412\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.312472\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.205364\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.579429\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.297201\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.308023\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.458343\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.330910\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.185778\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.380018\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.413470\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.233929\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.297798\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.271181\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.204572\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.253084\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.261417\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.248590\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.483902\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.273516\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.267861\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.424063\n",
      "\n",
      "Test set: Avg. loss: 0.2990, Accuracy: 9096/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.282024\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.434859\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.309143\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.443811\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.173938\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.232809\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.482542\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.318688\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.360041\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.275077\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.249149\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.164085\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.171556\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.179770\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.259850\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.227892\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.251021\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.366885\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.352869\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.335519\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.241958\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.179017\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.339365\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.461336\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.377137\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.222223\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.443615\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.182415\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.180565\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.426360\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.257742\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.372521\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.396550\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.208817\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.331683\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.228978\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.305340\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.320445\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.387453\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.250438\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.263446\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.233901\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.235975\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.192795\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.447650\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.256987\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.437276\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.276015\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.278973\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.473504\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.329904\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.175987\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.175766\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.337022\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.214596\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.377251\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.179441\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.198866\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.247031\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.174176\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.376271\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.455471\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.229497\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.439499\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.354803\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.439080\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.199913\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.366459\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.330826\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.378567\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.221291\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.275374\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.474159\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.277479\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.267337\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.254735\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.228043\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.180972\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.404483\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.330098\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.248737\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.432659\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.267405\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.324954\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.514228\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.268747\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.283233\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.338002\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.237797\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.536026\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.423071\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.376375\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.273274\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.324125\n",
      "\n",
      "Test set: Avg. loss: 0.2842, Accuracy: 9124/10000 (91%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toy = MLP()\n",
    "leak = 0.99\n",
    "time_step = 5\n",
    "lr = 0.0005\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "test()\n",
    "for epoch in range(5):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        out = toy(data)\n",
    "            \n",
    "        err = loss(out,target)\n",
    "        exp = torch.exp(out)\n",
    "        exp_sum = torch.sum(torch.exp(out),1, keepdim=True)\n",
    "        target = F.one_hot(target, num_classes=10)\n",
    "        L = -1*torch.sum((target*torch.log((exp/exp_sum))),1, keepdim=True)\n",
    "        \n",
    "        \n",
    "        du_out = exp/exp_sum\n",
    "\n",
    "        du_out = du_out - target\n",
    "        \n",
    "        bp_MLP(toy,leak,time_step,du_out,toy.s_regs_inp,lr,toy.lif1.thresh)\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), err.item()))\n",
    "            train_losses.append(err.item())\n",
    "            train_counter.append((batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cebce317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# fig = plt.figure()\n",
    "# plt.plot(train_counter, train_losses, color='blue')\n",
    "# plt.scatter(test_counter, test_losses, color='red')\n",
    "# plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "# plt.xlabel('number of training examples seen')\n",
    "# plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b633d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
