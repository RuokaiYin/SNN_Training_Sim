{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251e3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import gc\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b07e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33e21ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PoissonGen(inp, rescale_fac=2.0):\n",
    "    rand_inp = torch.rand_like(inp)\n",
    "    return torch.mul(torch.le(rand_inp * rescale_fac, torch.abs(inp)).float(), torch.sign(inp))\n",
    "\n",
    "# def spike_function(x):\n",
    "#     x[x>0] = 1\n",
    "#     x[x<=0] = 0\n",
    "#     return x\n",
    "\n",
    "def de_func(U,th):\n",
    "    alpha = 0.3\n",
    "    U = alpha*(1.0 - abs((U-th)/th))\n",
    "    U[U<0]=0\n",
    "    return U\n",
    "\n",
    "def test(toy,data,test_loader):\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    toy = toy.cuda()\n",
    "    for data, target in test_loader:\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "        output = toy(data)\n",
    "        test_loss +=F.cross_entropy(output, target,reduction='sum').item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    \n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "#     with torch.no_grad():\n",
    "#         for data in test_loader:\n",
    "#             toy = toy.cuda()\n",
    "            \n",
    "#             images, labels = data\n",
    "#             images = images.cuda()\n",
    "#             labels = labels.cuda()\n",
    "#             # calculate outputs by running images through the network\n",
    "#             outputs = toy(images)\n",
    "#             # the class with the highest energy is what we choose as prediction\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "#         100 * correct / total))\n",
    "\n",
    "def quant(input, k):\n",
    "    size = input.size()\n",
    "    #mean = torch.mean(input.abs(), 1, keepdim=True)\n",
    "    x = input\n",
    "    #print(x)\n",
    "    xmax = x.abs().max()\n",
    "    num_bits=k\n",
    "    v0 = 1\n",
    "    v1 = 2\n",
    "    v2 = -0.5\n",
    "    y = k #2.**num_bits - 1.\n",
    "    #print(y)\n",
    "    x = x.add(v0).div(v1)\n",
    "    #print(x)\n",
    "    x = x.mul(y).round_()\n",
    "    #print(x)\n",
    "    x = x.div(y)\n",
    "    #print(x)\n",
    "    x = x.add(v2)\n",
    "    #print(x)\n",
    "    x = x.mul(v1)\n",
    "    #print(x)\n",
    "    input = x\n",
    "    return input\n",
    "\n",
    "def conv_weight_update(dH,X,pad):\n",
    "    shap = dH.shape[1]\n",
    "    dH = torch.sum(dH,0)\n",
    "    dH = torch.unsqueeze(dH,1)\n",
    "    dH = torch.repeat_interleave(dH,X.shape[1],0)\n",
    "    X = X.repeat(1,shap,1,1)\n",
    "    dw_conv = F.conv2d(X,dH,padding=pad,groups=X.shape[1])\n",
    "    return dw_conv\n",
    "\n",
    "def conv_dx_update(dH,W,pad):\n",
    "\n",
    "    W = torch.transpose(W,0,1)\n",
    "    W = torch.flip(W,[-1,-2]) # W = C*3*3\n",
    "    dx_conv = F.conv2d(dH,W,padding=1)\n",
    "    \n",
    "    return dx_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d766c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, time_step,leak):\n",
    "        super(model, self).__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(28*28,256,bias=False)\n",
    "        self.fc_2 = nn.Linear(256,256,bias=False)\n",
    "        self.fc_out = nn.Linear(256,10,bias=False)\n",
    "        \n",
    "        self.lif1 = LIF(time_step,leak)\n",
    "        self.lif2 = LIF(time_step,leak)\n",
    "        self.time_step = time_step\n",
    "        self.s_regs_inp = None\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        inp = inp.view(inp.shape[0],-1)\n",
    "        size = inp.shape\n",
    "        self.s_regs_inp = torch.zeros(self.time_step,*size, device=device)\n",
    "        u_out = 0\n",
    "        \n",
    "        for t in range(self.time_step):\n",
    "            \n",
    "            spike_inp = PoissonGen(inp)\n",
    "            self.s_regs_inp[t] += spike_inp \n",
    "            \n",
    "            x = self.fc_1(spike_inp)\n",
    "            #x = quant(x,2**4)\n",
    "            x = self.lif1(x, t)\n",
    "            x = self.fc_2(x)\n",
    "            #x = quant(x,2**4)\n",
    "            x = self.lif2(x, t)\n",
    "            x = self.fc_out(x)\n",
    "            u_out = u_out + x\n",
    "        return u_out/self.time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8d72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,time_step,leak):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(28*28,512,bias=False)\n",
    "        self.fc_out = nn.Linear(512,10,bias=False)\n",
    "        self.lif1 = LIF(time_step,leak)\n",
    "        self.time_step = time_step\n",
    "        self.s_regs_inp = None\n",
    "        \n",
    "    def forward(self, inp):\n",
    "#         print(\"size is:\", (inp.view(inp.shape[0],1,28,28)).shape)\n",
    "        inp = inp.view(inp.shape[0],-1)\n",
    "        size = inp.shape\n",
    "        \n",
    "        self.s_regs_inp = torch.zeros(self.time_step,*size, device=device)\n",
    "        u_out = 0\n",
    "        \n",
    "        for t in range(self.time_step):\n",
    "            spike_inp = PoissonGen(inp)\n",
    "            self.s_regs_inp[t] += spike_inp \n",
    "            x = self.fc_1(spike_inp)\n",
    "            #x = quant(x,2**4)\n",
    "            x = self.lif1(x, t)\n",
    "            x = self.fc_out(x)\n",
    "            u_out = u_out + x\n",
    "        return u_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28765ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_5(nn.Module):\n",
    "    def __init__(self,time_step, leak,data):\n",
    "        super(VGG_5, self).__init__()\n",
    "        \n",
    "        if data == \"cifar10\":\n",
    "            input_dim = 3\n",
    "            pre_linear_dim = 8\n",
    "        elif data == \"mnist\":\n",
    "            input_dim = 1\n",
    "            pre_linear_dim = 7\n",
    "        \n",
    "        self.time_step = time_step\n",
    "        self.s_regs_inp = None\n",
    "        self.s_regs_conv = None\n",
    "        self.conv1 = nn.Conv2d(input_dim, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv_lif1 = LIF(time_step, leak)\n",
    "        # self.conv1a = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        # self.conv_lif1a = LIF(time_step, leak)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2,return_indices=True)\n",
    "        self.pool1_ind = []\n",
    "        self.unpool1 = nn.MaxUnpool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv_lif2 = LIF(time_step, leak)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv_lif3 = LIF(time_step, leak)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2,return_indices=True)\n",
    "        self.pool2_ind = []\n",
    "        self.unpool2 = nn.MaxUnpool2d(kernel_size=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * pre_linear_dim * pre_linear_dim, 1024, bias=False)\n",
    "        self.fc_lif1 = LIF(time_step,leak)\n",
    "        self.fc_out = nn.Linear(1024, 10, bias=False)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "\n",
    "        size = inp.shape\n",
    "        self.s_regs_inp = torch.zeros(self.time_step,*size, device=device)\n",
    "        self.pool1_ind = []\n",
    "        self.pool2_ind = []\n",
    "        u_out = 0\n",
    "        \n",
    "        for t in range(self.time_step):\n",
    "            spike_inp = PoissonGen(inp)\n",
    "            self.s_regs_inp[t] += spike_inp \n",
    "            x = self.conv1(spike_inp)\n",
    "            x = self.conv_lif1(x,t)\n",
    "            # x = self.conv1a(x)\n",
    "            # x = self.conv_lif1a(x,t)\n",
    "            x,indices = self.pool1(x)\n",
    "            self.pool1_ind.append(indices)\n",
    "            x = self.conv2(x)\n",
    "            x = self.conv_lif2(x,t)\n",
    "            x = self.conv3(x)\n",
    "            x = self.conv_lif3(x,t)\n",
    "            x,indices = self.pool2(x)\n",
    "            x = x.view(x.shape[0],-1)\n",
    "            \n",
    "            if t == 0:\n",
    "                self.s_regs_conv = torch.zeros(self.time_step,*x.shape, device=device)\n",
    "            self.pool2_ind.append(indices)\n",
    "            self.s_regs_conv[t] += x\n",
    "            \n",
    "            x = self.fc1(x)\n",
    "            x = self.fc_lif1(x,t)\n",
    "            x = self.fc_out(x)\n",
    "            u_out = u_out + x\n",
    "        return u_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37c149d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_VGG5(vgg,leak,time_step,du_out,l_r,th,pre_linear_dim,input_dim):\n",
    "    \n",
    "   \n",
    "    ## Update weight in FCs, time T\n",
    "    du_fc1 = torch.matmul(du_out,vgg.fc_out.weight)*de_func(vgg.fc_lif1.u_regs[-1],th)\n",
    "    vgg.fc_lif1.du_regs[-1] += du_fc1\n",
    "    w_conv_1 = torch.matmul(torch.transpose(du_fc1,0,1),vgg.s_regs_conv[-1])\n",
    "    vgg.fc1.weight.data -= l_r*w_conv_1   \n",
    "    w_1_out = torch.matmul(torch.transpose(du_out,0,1),vgg.fc_lif1.s_regs[-1])\n",
    "    vgg.fc_out.weight.data -= l_r*w_1_out\n",
    "    \n",
    "    ## Update du in pool2, time T\n",
    "    dx_pool2 = torch.matmul(du_fc1,vgg.fc1.weight)\n",
    "    dx_pool2 = dx_pool2.view(dx_pool2.shape[0],128,pre_linear_dim,pre_linear_dim)\n",
    "    du_pool2 = vgg.unpool2(dx_pool2,vgg.pool2_ind[-1])\n",
    "    \n",
    "    ## Update du and dw in conv3, time T\n",
    "    du_conv3 = du_pool2*de_func(toy.conv_lif3.u_regs[-1],th)\n",
    "    vgg.conv_lif3.du_regs[-1] += du_conv3 \n",
    "    dW_conv3 = conv_weight_update(du_pool2.type(torch.float),vgg.conv_lif2.s_regs[-1].type(torch.float),1)\n",
    "    dW_conv3 = torch.sum(dW_conv3,0)\n",
    "    dW_conv3 = dW_conv3.view(128,64,dW_conv3.shape[-1],dW_conv3.shape[-1])\n",
    "    vgg.conv3.weight.data -=l_r*dW_conv3\n",
    "    \n",
    "    ## Update du and dw in conv2, time T\n",
    "    \n",
    "    du_conv2 = conv_dx_update(du_conv3,vgg.conv3.weight,'same')*de_func(toy.conv_lif2.u_regs[-1],th)\n",
    "    vgg.conv_lif2.du_regs[-1] += du_conv2\n",
    "    dW_conv2 = conv_weight_update(du_conv2.type(torch.float),F.max_pool2d(vgg.conv_lif1.s_regs[-1].type(torch.float),kernel_size=2),1)\n",
    "    dW_conv2 = torch.sum(dW_conv2,0)\n",
    "    dW_conv2 = dW_conv2.view(64,64,dW_conv2.shape[-1],dW_conv2.shape[-1])\n",
    "    vgg.conv2.weight.data -=l_r*dW_conv2\n",
    "    \n",
    "    ## Update du in pool2, time t\n",
    "    du_pool1 = vgg.unpool1(conv_dx_update(du_conv2,vgg.conv2.weight,'same'),vgg.pool1_ind[-1])\n",
    "    \n",
    "    # du_conv1a = du_pool1*de_func(toy.conv_lif1a.u_regs[-1],th)\n",
    "    # vgg.conv_lif1a.du_regs[-1] += du_conv1a\n",
    "    # dW_conv1a = conv_weight_update(du_pool1.type(torch.float),vgg.conv_lif1.s_regs[-1].type(torch.float),1)\n",
    "    # dW_conv1a = torch.sum(dW_conv1a,0)\n",
    "    # dW_conv1a = dW_conv1a.view(64,32,dW_conv1a.shape[-1],dW_conv1a.shape[-1])\n",
    "    # vgg.conv1a.weight.data -=l_r*dW_conv1a\n",
    "    \n",
    "    du_conv1 = du_pool1*de_func(toy.conv_lif1.u_regs[-1],th)\n",
    "    vgg.conv_lif1.du_regs[-1] += du_conv1\n",
    "    dW_conv1 = conv_weight_update(du_conv1.type(torch.float),vgg.s_regs_inp[-1].type(torch.float),1)\n",
    "    dW_conv1 = torch.sum(dW_conv1,0)\n",
    "    dW_conv1 = dW_conv1.view(64,input_dim,dW_conv1.shape[-1],dW_conv1.shape[-1])\n",
    "    vgg.conv1.weight.data -=l_r*dW_conv1\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    for t in range(time_step-2,-1,-1):\n",
    "        \n",
    "        ds_fc1 = torch.matmul(du_out,vgg.fc_out.weight)+vgg.fc_lif1.du_regs[t+1]*(-leak*vgg.fc_lif1.du_regs[t])\n",
    "        du_fc1 = (ds_fc1)*de_func(vgg.fc_lif1.du_regs[t],th) + vgg.fc_lif1.du_regs[t+1]*leak*(1-vgg.fc_lif1.s_regs[t])\n",
    "        vgg.fc_lif1.du_regs[t] += du_fc1\n",
    "        w_conv_1 = torch.matmul(torch.transpose(du_fc1,0,1),vgg.s_regs_conv[t])\n",
    "        vgg.fc1.weight.data -= l_r*w_conv_1   \n",
    "        w_1_out = torch.matmul(torch.transpose(du_out,0,1),vgg.fc_lif1.s_regs[t])\n",
    "        vgg.fc_out.weight.data -= l_r*w_1_out\n",
    "        \n",
    "        \n",
    "        dx_pool2 = torch.matmul(du_fc1,vgg.fc1.weight)\n",
    "        dx_pool2 = dx_pool2.view(dx_pool2.shape[0],128,pre_linear_dim,pre_linear_dim)\n",
    "        du_pool2 = vgg.unpool2(dx_pool2,vgg.pool2_ind[t])\n",
    "        ds_conv3 = du_pool2+vgg.conv_lif3.du_regs[t+1]*(-leak*vgg.conv_lif3.du_regs[t])\n",
    "        du_conv3 = ds_conv3*de_func(toy.conv_lif3.u_regs[t],th) + vgg.conv_lif3.du_regs[t+1]*leak*(1-vgg.conv_lif3.s_regs[t])\n",
    "        vgg.conv_lif3.du_regs[t] += du_conv3 \n",
    "        dW_conv3 = conv_weight_update(du_pool2.type(torch.float),vgg.conv_lif2.s_regs[t].type(torch.float),1)\n",
    "        dW_conv3 = torch.sum(dW_conv3,0)\n",
    "        dW_conv3 = dW_conv3.view(128,64,dW_conv3.shape[-1],dW_conv3.shape[-1])\n",
    "        vgg.conv3.weight.data -=l_r*dW_conv3\n",
    "        \n",
    "        ds_conv2 = conv_dx_update(du_conv3,vgg.conv3.weight,'same')+vgg.conv_lif2.du_regs[t+1]*(-leak*vgg.conv_lif2.du_regs[t])\n",
    "        du_conv2 = ds_conv2*de_func(toy.conv_lif2.u_regs[t],th) + vgg.conv_lif2.du_regs[t+1]*leak*(1-vgg.conv_lif2.s_regs[t])\n",
    "        vgg.conv_lif2.du_regs[t] += du_conv2 \n",
    "        dW_conv2 = conv_weight_update(du_conv2.type(torch.float),F.max_pool2d(vgg.conv_lif1.s_regs[t].type(torch.float),kernel_size=2),1)\n",
    "        dW_conv2 = torch.sum(dW_conv2,0)\n",
    "        dW_conv2 = dW_conv2.view(64,64,dW_conv2.shape[-1],dW_conv2.shape[-1])\n",
    "        vgg.conv2.weight.data -=l_r*dW_conv2\n",
    "        \n",
    "        \n",
    "        du_pool1 = vgg.unpool1(conv_dx_update(du_conv2,vgg.conv2.weight,'same'),vgg.pool1_ind[t])\n",
    "        \n",
    "        \n",
    "#         ds_conv1a = du_pool1+vgg.conv_lif1a.du_regs[t+1]*(-leak*vgg.conv_lif1a.du_regs[t])\n",
    "#         du_conv1a = ds_conv1a*de_func(toy.conv_lif1a.u_regs[t],th) + vgg.conv_lif1a.du_regs[t+1]*leak*(1-vgg.conv_lif1a.s_regs[t])\n",
    "#         vgg.conv_lif1a.du_regs[t] += du_conv1a\n",
    "#         dW_conv1a = conv_weight_update(du_pool1.type(torch.float),vgg.conv_lif1.s_regs[t].type(torch.float),1)\n",
    "#         dW_conv1a = torch.sum(dW_conv1a,0)\n",
    "#         dW_conv1a = dW_conv1a.view(64,32,dW_conv1a.shape[-1],dW_conv1a.shape[-1])\n",
    "#         vgg.conv1a.weight.data -=l_r*dW_conv1a\n",
    "    \n",
    "        \n",
    "        \n",
    "        ds_conv1 = du_pool1 + vgg.conv_lif1.du_regs[t+1]*(-leak*vgg.conv_lif1.du_regs[t])\n",
    "        du_conv1 = ds_conv1*de_func(toy.conv_lif1.u_regs[t],th) + vgg.conv_lif1.du_regs[t+1]*leak*(1-vgg.conv_lif1.s_regs[t])\n",
    "        vgg.conv_lif1.du_regs[t] += du_conv1\n",
    "        dW_conv1 = conv_weight_update(du_conv1.type(torch.float),vgg.s_regs_inp[t].type(torch.float),1)\n",
    "        dW_conv1 = torch.sum(dW_conv1,0)\n",
    "        dW_conv1 = dW_conv1.view(64,input_dim,dW_conv1.shape[-1],dW_conv1.shape[-1])\n",
    "        vgg.conv1.weight.data -=l_r*dW_conv1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5802d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     du_pool2 = torch.sum(du_pool2,0)\n",
    "#     du_pool2 = torch.unsqueeze(du_pool2,1)\n",
    "    \n",
    "    ## Update du in conv3, time T\n",
    "#     d_conv3 = nn.Conv2d(128, 128, stride=1, kernel_size=f, padding=f-1, bias=False)\n",
    "    \n",
    "    ## Update weight in Conv3, time T\n",
    "#     f = du_pool2.shape[-1]\n",
    "#     d_conv3 = nn.Conv2d(128, 128, stride=1, padding=1, kernel_size=f, bias=False)\n",
    "#     d_conv3.weight.data = du_pool2.type(torch.float)\n",
    "#     dW_conv3 = d_conv3(vgg.conv_lif2.s_regs[-1].type(torch.float))\n",
    "#     dW_conv3 = torch.sum(dW_conv3,0)\n",
    "#     dW_conv3 = torch.unsqueeze(dW_conv3,1)\n",
    "#     vgg.conv3.weight.data -= l_r*dW_conv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7adf32d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_1(nn.Module):\n",
    "    def __init__(self,time_step,leak):\n",
    "        super(VGG_1, self).__init__()\n",
    "        \n",
    "        self.time_step = time_step\n",
    "        self.s_regs_inp = None\n",
    "        self.s_regs_conv = None\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1, bias=False)\n",
    "        \n",
    "#         self.deconv1 = nn.Conv2d()\n",
    "        self.lif_conv1 = LIF(time_step,leak)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2,return_indices=True)\n",
    "        self.pool1_ind = []\n",
    "        self.unpool1 = nn.MaxUnpool2d(kernel_size=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(16 * 14 * 14, 512, bias=False)\n",
    "        self.lif_fc1 = LIF(time_step,leak)\n",
    "        self.fc_out = nn.Linear(512, 10, bias=False)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "\n",
    "        size = inp.shape\n",
    "        self.s_regs_inp = torch.zeros(self.time_step,*size, device=device)\n",
    "        self.pool1_ind = []\n",
    "\n",
    "        u_out = 0\n",
    "        for t in range(self.time_step):\n",
    "            spike_inp = PoissonGen(inp)\n",
    "            self.s_regs_inp[t] += spike_inp\n",
    "            x = self.conv1(spike_inp)\n",
    "            x = self.lif_conv1(x,t)\n",
    "            x, indices = self.pool1(x)\n",
    "            x= x.view(x.shape[0],-1)\n",
    "            \n",
    "            if t == 0:\n",
    "                self.s_regs_conv = torch.zeros(self.time_step,*x.shape, device=device)\n",
    "            self.pool1_ind.append(indices)\n",
    "            self.s_regs_conv[t] += x\n",
    "            \n",
    "            x = self.fc1(x)\n",
    "            x = self.lif_fc1(x,t)\n",
    "            \n",
    "            x = self.fc_out(x)\n",
    "            u_out = u_out + x\n",
    "\n",
    "        return u_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31ffe4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_VGG1(vgg,leak,time_step,du_out,l_r,th):\n",
    "   \n",
    "    ## First fc\n",
    "    du_fc1 = torch.matmul(du_out,vgg.fc_out.weight)*de_func(vgg.lif_fc1.u_regs[-1],th)\n",
    "    vgg.lif_fc1.du_regs[-1] += du_fc1\n",
    "       \n",
    "    ## Update weight\n",
    "    w_conv_1 = torch.matmul(torch.transpose(du_fc1,0,1),vgg.s_regs_conv[-1])\n",
    "    vgg.fc1.weight.data -= l_r*w_conv_1\n",
    "     \n",
    "    w_1_out = torch.matmul(torch.transpose(du_out,0,1),vgg.lif_fc1.s_regs[-1])\n",
    "    vgg.fc_out.weight.data -= l_r*w_1_out\n",
    "    \n",
    "    dx_pool1 = torch.matmul(du_fc1,vgg.fc1.weight)\n",
    "    dx_pool1 = dx_pool1.view(dx_pool1.shape[0],16,14,14)\n",
    "    du_pool1 = vgg.unpool1(dx_pool1,vgg.pool1_ind[-1])\n",
    "    du_pool1 = torch.sum(du_pool1,0)\n",
    "    du_pool1 = torch.unsqueeze(du_pool1,1)\n",
    "    f = du_pool1.shape[-1]\n",
    "    d_conv1_w = nn.Conv2d(1, 16, stride=1, padding=1,kernel_size=f, bias=False)\n",
    "    d_conv1_w.weight.data = du_pool1.type(torch.float)\n",
    "    dW = d_conv1_w(vgg.s_regs_inp[-1].type(torch.float))\n",
    "    dW = torch.sum(dW,0)\n",
    "    dW = torch.unsqueeze(dW,1)\n",
    "\n",
    "    vgg.conv1.weight.data -= l_r*dW\n",
    "    \n",
    "    for t in range(time_step-2,-1,-1):\n",
    "        \n",
    "        ds_fc1 = torch.matmul(du_out,vgg.fc_out.weight)+vgg.lif_fc1.du_regs[t+1]*(-leak*vgg.lif_fc1.du_regs[t])\n",
    "        du_fc1 = (ds_fc1)*de_func(vgg.lif_fc1.du_regs[t],th) + vgg.lif_fc1.du_regs[t+1]*leak*(1-vgg.lif_fc1.s_regs[t])\n",
    "        vgg.lif_fc1.du_regs[t] += du_fc1\n",
    "        \n",
    "        w_conv_1 = torch.matmul(torch.transpose(du_fc1,0,1),vgg.s_regs_conv[t])\n",
    "        vgg.fc1.weight.data -= l_r*w_conv_1\n",
    "        \n",
    "        \n",
    "        dx_pool1 = torch.matmul(du_fc1,vgg.fc1.weight)\n",
    "        dx_pool1 = dx_pool1.view(dx_pool1.shape[0],16,14,14)\n",
    "        du_pool1 = vgg.unpool1(dx_pool1,vgg.pool1_ind[t])\n",
    "        du_pool1 = torch.sum(du_pool1,0)\n",
    "        du_pool1 = torch.unsqueeze(du_pool1,1)\n",
    "        f = du_pool1.shape[-1]\n",
    "        d_conv1_w = nn.Conv2d(1, 16, stride=1, padding=1,kernel_size=f, bias=False)\n",
    "        d_conv1_w.weight.data = du_pool1.type(torch.float)\n",
    "        dW = d_conv1_w(vgg.s_regs_inp[t].type(torch.float))\n",
    "        dW = torch.sum(dW,0)\n",
    "        dW = torch.unsqueeze(dW,1)\n",
    "\n",
    "        vgg.conv1.weight.data -= l_r*dW\n",
    "    \n",
    "    \n",
    "    return 0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e6f78f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIF(nn.Module):\n",
    "    def __init__(self, time_step,leak):\n",
    "        super(LIF, self).__init__()\n",
    "        \n",
    "        self.u_regs = None\n",
    "        self.du_regs = None\n",
    "        self.s_regs = None\n",
    "        self.leak = leak\n",
    "        self.time_step = time_step\n",
    "        self.thresh = 0.5\n",
    "        \n",
    "    def forward(self,inp,t):\n",
    "        \n",
    "#         print(\"memory before clear\",torch.cuda.memory_allocated())\n",
    "        if t == 0:\n",
    "            size = inp.shape\n",
    "            self.u_regs = torch.zeros(self.time_step,*size, device=device)\n",
    "            self.du_regs = torch.zeros(self.time_step,*size, device=device)\n",
    "#             err = torch.normal(0, 0.1,(1,1)).cuda()\n",
    "#             inp = inp + err\n",
    "#             self.u_regs[0] = quant(inp,2**4)\n",
    "            self.u_regs[0] = inp\n",
    "            self.s_regs = torch.zeros(self.time_step,*size, device=device)\n",
    "\n",
    "            spike = inp.gt(self.thresh).float()\n",
    "\n",
    "            self.s_regs[0] = spike\n",
    "            \n",
    "        else:\n",
    "#             err = torch.normal(0, 0.1,(1,1))\n",
    "#             inp = inp + err\n",
    "#             self.u_regs[t] = quant(self.leak * self.u_regs[t-1] * (1 - self.s_regs[t-1]) + (1-self.leak)*inp, 2**4)\n",
    "            self.u_regs[t] = self.leak*self.u_regs[t-1]*(1-self.s_regs[t-1]) + inp\n",
    "\n",
    "            spike = self.u_regs[t].gt(self.thresh).float()\n",
    "\n",
    "            self.s_regs[t] = spike\n",
    "            \n",
    "#         print(\"memory after clear\",torch.cuda.memory_allocated())\n",
    "#         torch.cuda.empty_cache()\n",
    "#         gc.collect()\n",
    "        return spike\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8fc2dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Back propagation for MLP\n",
    "def bp_MLP(toy,leak,time_step,du_out,s_regs_inp,l_r,th):\n",
    "    \n",
    "    ## First fc\n",
    "    du_fc1 = torch.matmul(du_out,toy.fc_out.weight)*de_func(toy.lif1.u_regs[-1],th)\n",
    "    toy.lif1.du_regs[-1] += du_fc1\n",
    "\n",
    "    ## Update weight\n",
    "    w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[-1])\n",
    "#     toy.fc_1.weight.data -= l_r*quant(w_inp_1,2**4)\n",
    "    toy.fc_1.weight.data -= l_r*w_inp_1\n",
    "    \n",
    "    w_1_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif1.s_regs[-1])\n",
    "#     toy.fc_out.weight.data -= l_r*quant(w_1_out,2**4)\n",
    "    toy.fc_out.weight.data -= l_r*w_1_out\n",
    "\n",
    "    for t in range(time_step-2,-1,-1):\n",
    "        \n",
    "        ## First fc\n",
    "        ds_fc1 = torch.matmul(du_out,toy.fc_out.weight)+toy.lif1.du_regs[t+1]*(-leak*toy.lif1.u_regs[t])\n",
    "        du_fc1 = (ds_fc1)*de_func(toy.lif1.u_regs[t],th) + toy.lif1.du_regs[t+1]*leak*(1-toy.lif1.s_regs[t])\n",
    "        toy.lif1.du_regs[t] += du_fc1\n",
    "\n",
    "        ## Update weight\n",
    "        w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[t])\n",
    "#         toy.fc_1.weight.data -= l_r*quant(w_inp_1,2**4)\n",
    "#         print(\"du_size\",du_fc1.shape)\n",
    "#         print(\"s_size\",s_regs_inp[t].shape)\n",
    "#         print(\"dweight shape\",w_inp_1.shape)\n",
    "        toy.fc_1.weight.data -= l_r*w_inp_1\n",
    "\n",
    "        w_1_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif1.s_regs[t])\n",
    "#         toy.fc_out.weight.data -= l_r*quant(w_1_out,2**4)\n",
    "        toy.fc_out.weight.data -= l_r*w_1_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b08084b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Back propagation\n",
    "def bp(toy,leak,time_step,du_out,s_regs_inp,l_r,th):\n",
    "    \n",
    "    ## Second fc    \n",
    "    du_fc2 = torch.matmul(du_out,toy.fc_out.weight)*de_func(toy.lif2.u_regs[-1],th)    \n",
    "    toy.lif2.du_regs[-1] = toy.lif2.du_regs[-1] + du_fc2\n",
    "    \n",
    "    ## First fc\n",
    "    du_fc1 = torch.matmul(du_fc2,toy.fc_2.weight)*de_func(toy.lif1.u_regs[-1],th)\n",
    "    toy.lif1.du_regs[-1] += du_fc1\n",
    "\n",
    "    \n",
    "    ## Update weight\n",
    "    w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[-1])\n",
    "    toy.fc_1.weight.data -= l_r*quant(w_inp_1,2**4)\n",
    "    #toy.fc_1.weight.data -= l_r*w_inp_1\n",
    "\n",
    "    w_1_2 = torch.matmul(torch.transpose(du_fc2,0,1),toy.lif1.s_regs[-1])\n",
    "    toy.fc_2.weight.data -= l_r*quant(w_1_2,2**4)\n",
    "    #toy.fc_2.weight.data -= l_r*w_1_2\n",
    "\n",
    "    w_2_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif2.s_regs[-1])\n",
    "    toy.fc_out.weight.data -= l_r*quant(w_2_out,2**4)\n",
    "    #toy.fc_out.weight.data -= l_r*w_2_out\n",
    "\n",
    "    for t in range(time_step-2,-1,-1):\n",
    "\n",
    "        ds_fc2 = torch.matmul(du_out,toy.fc_out.weight)+toy.lif2.du_regs[t+1]*(-leak*toy.lif2.u_regs[t])\n",
    "        du_fc2 = (ds_fc2)*de_func(toy.lif2.u_regs[t],th) + toy.lif2.du_regs[t+1]*leak*(1-toy.lif2.s_regs[t])\n",
    "        toy.lif2.du_regs[t] += du_fc2\n",
    "        \n",
    "        ## First fc\n",
    "        ds_fc1 = torch.matmul(du_fc2,toy.fc_2.weight)+toy.lif1.du_regs[t+1]*(-leak*toy.lif1.u_regs[t])\n",
    "        du_fc1 = (ds_fc1)*de_func(toy.lif1.u_regs[t],th) + toy.lif1.du_regs[t+1]*leak*(1-toy.lif1.s_regs[t])\n",
    "        toy.lif1.du_regs[t] += du_fc1\n",
    "\n",
    "        ## Update weight\n",
    "        w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[t])\n",
    "        toy.fc_1.weight.data -= l_r*quant(w_inp_1,2**4)\n",
    "        \n",
    "        #toy.fc_1.weight.data -= l_r*w_inp_1\n",
    "\n",
    "        w_1_2 = torch.matmul(torch.transpose(du_fc2,0,1),toy.lif1.s_regs[t])\n",
    "        toy.fc_2.weight.data -= l_r*quant(w_1_2,2**4)\n",
    "        #toy.fc_2.weight.data -= l_r*w_1_2\n",
    "\n",
    "        w_2_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif2.s_regs[t])\n",
    "        toy.fc_out.weight.data -= l_r*quant(w_2_out,2**4)\n",
    "        #toy.fc_out.weight.data -= l_r*w_2_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c3de0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "batch_size_train = 128\n",
    "batch_size_test = 1000\n",
    "\n",
    "train_loader_mnist = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./mnist', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_mnist = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./mnist', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edee34dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# import ssl\n",
    "\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# response = urllib.request.urlopen('https://www.python.org')\n",
    "# print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6839e4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/loomis/project/panda/ry263/conda_envs/torch_cuda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./cifar10', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader_cifar10 = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=4,pin_memory=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./cifar10', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "test_loader_cifar10 = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=4,pin_memory=True)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b70c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3bfbdde-e623-4836-8e97-3bf5c7487a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sparsity(toy):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2ce9ca7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.527207\n",
      "Train Epoch: 0 [1280/50000 (3%)]\tLoss: 2.933245\n",
      "Train Epoch: 0 [2560/50000 (5%)]\tLoss: 2.449613\n",
      "Train Epoch: 0 [3840/50000 (8%)]\tLoss: 2.402166\n",
      "Train Epoch: 0 [5120/50000 (10%)]\tLoss: 2.277757\n",
      "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 2.302380\n",
      "Train Epoch: 0 [7680/50000 (15%)]\tLoss: 2.124977\n",
      "Train Epoch: 0 [8960/50000 (18%)]\tLoss: 2.076686\n",
      "Train Epoch: 0 [10240/50000 (20%)]\tLoss: 2.457146\n",
      "Train Epoch: 0 [11520/50000 (23%)]\tLoss: 2.180952\n",
      "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 2.033736\n",
      "Train Epoch: 0 [14080/50000 (28%)]\tLoss: 2.385054\n",
      "Train Epoch: 0 [15360/50000 (31%)]\tLoss: 2.256353\n",
      "Train Epoch: 0 [16640/50000 (33%)]\tLoss: 2.070146\n",
      "Train Epoch: 0 [17920/50000 (36%)]\tLoss: 2.052105\n",
      "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 2.195777\n",
      "Train Epoch: 0 [20480/50000 (41%)]\tLoss: 2.069132\n",
      "Train Epoch: 0 [21760/50000 (43%)]\tLoss: 2.389847\n",
      "Train Epoch: 0 [23040/50000 (46%)]\tLoss: 2.328788\n",
      "Train Epoch: 0 [24320/50000 (49%)]\tLoss: 2.035145\n",
      "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 2.219576\n",
      "Train Epoch: 0 [26880/50000 (54%)]\tLoss: 2.006036\n",
      "Train Epoch: 0 [28160/50000 (56%)]\tLoss: 2.217445\n",
      "Train Epoch: 0 [29440/50000 (59%)]\tLoss: 2.067543\n",
      "Train Epoch: 0 [30720/50000 (61%)]\tLoss: 2.194173\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 2.382097\n",
      "Train Epoch: 0 [33280/50000 (66%)]\tLoss: 1.947454\n",
      "Train Epoch: 0 [34560/50000 (69%)]\tLoss: 2.184271\n",
      "Train Epoch: 0 [35840/50000 (72%)]\tLoss: 2.119794\n",
      "Train Epoch: 0 [37120/50000 (74%)]\tLoss: 2.168786\n",
      "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 2.028112\n",
      "Train Epoch: 0 [39680/50000 (79%)]\tLoss: 2.195015\n",
      "Train Epoch: 0 [40960/50000 (82%)]\tLoss: 2.038604\n",
      "Train Epoch: 0 [42240/50000 (84%)]\tLoss: 2.019813\n",
      "Train Epoch: 0 [43520/50000 (87%)]\tLoss: 2.313366\n",
      "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 1.854343\n",
      "Train Epoch: 0 [46080/50000 (92%)]\tLoss: 2.077433\n",
      "Train Epoch: 0 [47360/50000 (95%)]\tLoss: 2.160011\n",
      "Train Epoch: 0 [48640/50000 (97%)]\tLoss: 2.081628\n",
      "Train Epoch: 0 [31200/50000 (100%)]\tLoss: 2.029565\n",
      "\n",
      "Test set: Avg. loss: 2.1284, Accuracy: 2175/10000 (22%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.099425\n",
      "Train Epoch: 1 [1280/50000 (3%)]\tLoss: 1.921133\n",
      "Train Epoch: 1 [2560/50000 (5%)]\tLoss: 2.165237\n",
      "Train Epoch: 1 [3840/50000 (8%)]\tLoss: 2.194519\n",
      "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 1.974638\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.168518\n",
      "Train Epoch: 1 [7680/50000 (15%)]\tLoss: 2.235335\n",
      "Train Epoch: 1 [8960/50000 (18%)]\tLoss: 2.120577\n",
      "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 2.130499\n",
      "Train Epoch: 1 [11520/50000 (23%)]\tLoss: 1.886667\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 2.171588\n",
      "Train Epoch: 1 [14080/50000 (28%)]\tLoss: 2.085612\n",
      "Train Epoch: 1 [15360/50000 (31%)]\tLoss: 1.994839\n",
      "Train Epoch: 1 [16640/50000 (33%)]\tLoss: 2.160587\n",
      "Train Epoch: 1 [17920/50000 (36%)]\tLoss: 2.045866\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 1.950976\n",
      "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 1.775753\n",
      "Train Epoch: 1 [21760/50000 (43%)]\tLoss: 1.928567\n",
      "Train Epoch: 1 [23040/50000 (46%)]\tLoss: 1.955946\n",
      "Train Epoch: 1 [24320/50000 (49%)]\tLoss: 1.988601\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.991000\n",
      "Train Epoch: 1 [26880/50000 (54%)]\tLoss: 2.057856\n",
      "Train Epoch: 1 [28160/50000 (56%)]\tLoss: 1.812589\n",
      "Train Epoch: 1 [29440/50000 (59%)]\tLoss: 2.056464\n",
      "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 1.893016\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.892137\n",
      "Train Epoch: 1 [33280/50000 (66%)]\tLoss: 2.048019\n",
      "Train Epoch: 1 [34560/50000 (69%)]\tLoss: 1.927738\n",
      "Train Epoch: 1 [35840/50000 (72%)]\tLoss: 1.969504\n",
      "Train Epoch: 1 [37120/50000 (74%)]\tLoss: 2.170085\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.885718\n",
      "Train Epoch: 1 [39680/50000 (79%)]\tLoss: 2.003218\n",
      "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 2.022715\n",
      "Train Epoch: 1 [42240/50000 (84%)]\tLoss: 2.077643\n",
      "Train Epoch: 1 [43520/50000 (87%)]\tLoss: 1.982348\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.933409\n",
      "Train Epoch: 1 [46080/50000 (92%)]\tLoss: 2.003406\n",
      "Train Epoch: 1 [47360/50000 (95%)]\tLoss: 2.012547\n",
      "Train Epoch: 1 [48640/50000 (97%)]\tLoss: 1.952880\n",
      "Train Epoch: 1 [31200/50000 (100%)]\tLoss: 2.069713\n",
      "\n",
      "Test set: Avg. loss: 2.0967, Accuracy: 2315/10000 (23%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 2.164567\n",
      "Train Epoch: 2 [1280/50000 (3%)]\tLoss: 1.662871\n",
      "Train Epoch: 2 [2560/50000 (5%)]\tLoss: 1.971914\n",
      "Train Epoch: 2 [3840/50000 (8%)]\tLoss: 2.180591\n",
      "Train Epoch: 2 [5120/50000 (10%)]\tLoss: 1.908525\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.937665\n",
      "Train Epoch: 2 [7680/50000 (15%)]\tLoss: 1.916815\n",
      "Train Epoch: 2 [8960/50000 (18%)]\tLoss: 1.960987\n",
      "Train Epoch: 2 [10240/50000 (20%)]\tLoss: 2.058267\n",
      "Train Epoch: 2 [11520/50000 (23%)]\tLoss: 1.800386\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 2.088719\n",
      "Train Epoch: 2 [14080/50000 (28%)]\tLoss: 1.921593\n",
      "Train Epoch: 2 [15360/50000 (31%)]\tLoss: 2.043571\n",
      "Train Epoch: 2 [16640/50000 (33%)]\tLoss: 1.993358\n",
      "Train Epoch: 2 [17920/50000 (36%)]\tLoss: 1.854591\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 1.874516\n",
      "Train Epoch: 2 [20480/50000 (41%)]\tLoss: 1.826817\n",
      "Train Epoch: 2 [21760/50000 (43%)]\tLoss: 1.949229\n",
      "Train Epoch: 2 [23040/50000 (46%)]\tLoss: 2.109922\n",
      "Train Epoch: 2 [24320/50000 (49%)]\tLoss: 1.893121\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 2.088271\n",
      "Train Epoch: 2 [26880/50000 (54%)]\tLoss: 1.828642\n",
      "Train Epoch: 2 [28160/50000 (56%)]\tLoss: 1.952114\n",
      "Train Epoch: 2 [29440/50000 (59%)]\tLoss: 1.872267\n",
      "Train Epoch: 2 [30720/50000 (61%)]\tLoss: 1.935380\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 2.305811\n",
      "Train Epoch: 2 [33280/50000 (66%)]\tLoss: 1.985208\n",
      "Train Epoch: 2 [34560/50000 (69%)]\tLoss: 1.893956\n",
      "Train Epoch: 2 [35840/50000 (72%)]\tLoss: 2.013466\n",
      "Train Epoch: 2 [37120/50000 (74%)]\tLoss: 1.970907\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.885535\n",
      "Train Epoch: 2 [39680/50000 (79%)]\tLoss: 1.935821\n",
      "Train Epoch: 2 [40960/50000 (82%)]\tLoss: 1.952032\n",
      "Train Epoch: 2 [42240/50000 (84%)]\tLoss: 1.970550\n",
      "Train Epoch: 2 [43520/50000 (87%)]\tLoss: 1.967119\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 1.955717\n",
      "Train Epoch: 2 [46080/50000 (92%)]\tLoss: 2.036042\n",
      "Train Epoch: 2 [47360/50000 (95%)]\tLoss: 1.990656\n",
      "Train Epoch: 2 [48640/50000 (97%)]\tLoss: 2.004188\n",
      "Train Epoch: 2 [31200/50000 (100%)]\tLoss: 2.050345\n",
      "\n",
      "Test set: Avg. loss: 1.9781, Accuracy: 3119/10000 (31%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.880062\n",
      "Train Epoch: 3 [1280/50000 (3%)]\tLoss: 1.965693\n",
      "Train Epoch: 3 [2560/50000 (5%)]\tLoss: 1.839488\n",
      "Train Epoch: 3 [3840/50000 (8%)]\tLoss: 1.870483\n",
      "Train Epoch: 3 [5120/50000 (10%)]\tLoss: 1.829778\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 1.647075\n",
      "Train Epoch: 3 [7680/50000 (15%)]\tLoss: 1.781943\n",
      "Train Epoch: 3 [8960/50000 (18%)]\tLoss: 1.914810\n",
      "Train Epoch: 3 [10240/50000 (20%)]\tLoss: 1.737356\n",
      "Train Epoch: 3 [11520/50000 (23%)]\tLoss: 1.893015\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 1.821685\n",
      "Train Epoch: 3 [14080/50000 (28%)]\tLoss: 1.912093\n",
      "Train Epoch: 3 [15360/50000 (31%)]\tLoss: 1.883455\n",
      "Train Epoch: 3 [16640/50000 (33%)]\tLoss: 1.715725\n",
      "Train Epoch: 3 [17920/50000 (36%)]\tLoss: 1.846616\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 1.820924\n",
      "Train Epoch: 3 [20480/50000 (41%)]\tLoss: 1.826017\n",
      "Train Epoch: 3 [21760/50000 (43%)]\tLoss: 1.856700\n",
      "Train Epoch: 3 [23040/50000 (46%)]\tLoss: 1.936260\n",
      "Train Epoch: 3 [24320/50000 (49%)]\tLoss: 1.856774\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 1.901047\n",
      "Train Epoch: 3 [26880/50000 (54%)]\tLoss: 1.711006\n",
      "Train Epoch: 3 [28160/50000 (56%)]\tLoss: 2.127882\n",
      "Train Epoch: 3 [29440/50000 (59%)]\tLoss: 1.722234\n",
      "Train Epoch: 3 [30720/50000 (61%)]\tLoss: 1.818711\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.846744\n",
      "Train Epoch: 3 [33280/50000 (66%)]\tLoss: 1.815472\n",
      "Train Epoch: 3 [34560/50000 (69%)]\tLoss: 1.995260\n",
      "Train Epoch: 3 [35840/50000 (72%)]\tLoss: 1.929356\n",
      "Train Epoch: 3 [37120/50000 (74%)]\tLoss: 1.778441\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 2.024956\n",
      "Train Epoch: 3 [39680/50000 (79%)]\tLoss: 1.987142\n",
      "Train Epoch: 3 [40960/50000 (82%)]\tLoss: 2.025617\n",
      "Train Epoch: 3 [42240/50000 (84%)]\tLoss: 1.789225\n",
      "Train Epoch: 3 [43520/50000 (87%)]\tLoss: 1.850949\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 1.926630\n",
      "Train Epoch: 3 [46080/50000 (92%)]\tLoss: 1.842491\n",
      "Train Epoch: 3 [47360/50000 (95%)]\tLoss: 1.780740\n",
      "Train Epoch: 3 [48640/50000 (97%)]\tLoss: 2.315696\n",
      "Train Epoch: 3 [31200/50000 (100%)]\tLoss: 1.694162\n",
      "\n",
      "Test set: Avg. loss: 1.8330, Accuracy: 3491/10000 (35%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.820129\n",
      "Train Epoch: 4 [1280/50000 (3%)]\tLoss: 1.899404\n",
      "Train Epoch: 4 [2560/50000 (5%)]\tLoss: 1.841179\n",
      "Train Epoch: 4 [3840/50000 (8%)]\tLoss: 1.748647\n",
      "Train Epoch: 4 [5120/50000 (10%)]\tLoss: 1.784785\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 1.828378\n",
      "Train Epoch: 4 [7680/50000 (15%)]\tLoss: 1.853046\n",
      "Train Epoch: 4 [8960/50000 (18%)]\tLoss: 1.801320\n",
      "Train Epoch: 4 [10240/50000 (20%)]\tLoss: 1.775074\n",
      "Train Epoch: 4 [11520/50000 (23%)]\tLoss: 1.797993\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 1.903873\n",
      "Train Epoch: 4 [14080/50000 (28%)]\tLoss: 1.725823\n",
      "Train Epoch: 4 [15360/50000 (31%)]\tLoss: 2.004297\n",
      "Train Epoch: 4 [16640/50000 (33%)]\tLoss: 1.961323\n",
      "Train Epoch: 4 [17920/50000 (36%)]\tLoss: 1.790800\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 1.933480\n",
      "Train Epoch: 4 [20480/50000 (41%)]\tLoss: 1.829504\n",
      "Train Epoch: 4 [21760/50000 (43%)]\tLoss: 1.793607\n",
      "Train Epoch: 4 [23040/50000 (46%)]\tLoss: 1.923832\n",
      "Train Epoch: 4 [24320/50000 (49%)]\tLoss: 1.677941\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 1.718806\n",
      "Train Epoch: 4 [26880/50000 (54%)]\tLoss: 1.801368\n",
      "Train Epoch: 4 [28160/50000 (56%)]\tLoss: 1.668847\n",
      "Train Epoch: 4 [29440/50000 (59%)]\tLoss: 1.822596\n",
      "Train Epoch: 4 [30720/50000 (61%)]\tLoss: 1.813385\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.877579\n",
      "Train Epoch: 4 [33280/50000 (66%)]\tLoss: 2.152824\n",
      "Train Epoch: 4 [34560/50000 (69%)]\tLoss: 1.889494\n",
      "Train Epoch: 4 [35840/50000 (72%)]\tLoss: 1.933661\n",
      "Train Epoch: 4 [37120/50000 (74%)]\tLoss: 1.756798\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 1.850730\n",
      "Train Epoch: 4 [39680/50000 (79%)]\tLoss: 1.900206\n",
      "Train Epoch: 4 [40960/50000 (82%)]\tLoss: 1.969314\n",
      "Train Epoch: 4 [42240/50000 (84%)]\tLoss: 2.090741\n",
      "Train Epoch: 4 [43520/50000 (87%)]\tLoss: 1.887524\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 1.967529\n",
      "Train Epoch: 4 [46080/50000 (92%)]\tLoss: 1.917219\n",
      "Train Epoch: 4 [47360/50000 (95%)]\tLoss: 1.829508\n",
      "Train Epoch: 4 [48640/50000 (97%)]\tLoss: 2.074061\n",
      "Train Epoch: 4 [31200/50000 (100%)]\tLoss: 1.843927\n",
      "\n",
      "Test set: Avg. loss: 1.8717, Accuracy: 3276/10000 (33%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.939130\n",
      "Train Epoch: 5 [1280/50000 (3%)]\tLoss: 1.646263\n",
      "Train Epoch: 5 [2560/50000 (5%)]\tLoss: 1.697029\n",
      "Train Epoch: 5 [3840/50000 (8%)]\tLoss: 1.769722\n",
      "Train Epoch: 5 [5120/50000 (10%)]\tLoss: 1.783538\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 1.680206\n",
      "Train Epoch: 5 [7680/50000 (15%)]\tLoss: 1.721791\n",
      "Train Epoch: 5 [8960/50000 (18%)]\tLoss: 1.685753\n",
      "Train Epoch: 5 [10240/50000 (20%)]\tLoss: 1.640829\n",
      "Train Epoch: 5 [11520/50000 (23%)]\tLoss: 1.688018\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 1.807545\n",
      "Train Epoch: 5 [14080/50000 (28%)]\tLoss: 1.748854\n",
      "Train Epoch: 5 [15360/50000 (31%)]\tLoss: 1.774585\n",
      "Train Epoch: 5 [16640/50000 (33%)]\tLoss: 1.772600\n",
      "Train Epoch: 5 [17920/50000 (36%)]\tLoss: 1.759334\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 1.699772\n",
      "Train Epoch: 5 [20480/50000 (41%)]\tLoss: 1.769366\n",
      "Train Epoch: 5 [21760/50000 (43%)]\tLoss: 1.600168\n",
      "Train Epoch: 5 [23040/50000 (46%)]\tLoss: 1.690064\n",
      "Train Epoch: 5 [24320/50000 (49%)]\tLoss: 1.638581\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 1.653111\n",
      "Train Epoch: 5 [26880/50000 (54%)]\tLoss: 1.667120\n",
      "Train Epoch: 5 [28160/50000 (56%)]\tLoss: 1.668559\n",
      "Train Epoch: 5 [29440/50000 (59%)]\tLoss: 1.751117\n",
      "Train Epoch: 5 [30720/50000 (61%)]\tLoss: 1.692888\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1.808442\n",
      "Train Epoch: 5 [33280/50000 (66%)]\tLoss: 1.699888\n",
      "Train Epoch: 5 [34560/50000 (69%)]\tLoss: 1.717252\n",
      "Train Epoch: 5 [35840/50000 (72%)]\tLoss: 1.664284\n",
      "Train Epoch: 5 [37120/50000 (74%)]\tLoss: 1.722378\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 1.710162\n",
      "Train Epoch: 5 [39680/50000 (79%)]\tLoss: 1.829019\n",
      "Train Epoch: 5 [40960/50000 (82%)]\tLoss: 1.815956\n",
      "Train Epoch: 5 [42240/50000 (84%)]\tLoss: 1.845269\n",
      "Train Epoch: 5 [43520/50000 (87%)]\tLoss: 1.667322\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 1.627875\n",
      "Train Epoch: 5 [46080/50000 (92%)]\tLoss: 1.709872\n",
      "Train Epoch: 5 [47360/50000 (95%)]\tLoss: 1.756295\n",
      "Train Epoch: 5 [48640/50000 (97%)]\tLoss: 1.508948\n",
      "Train Epoch: 5 [31200/50000 (100%)]\tLoss: 1.624209\n",
      "\n",
      "Test set: Avg. loss: 1.7085, Accuracy: 3993/10000 (40%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.668390\n",
      "Train Epoch: 6 [1280/50000 (3%)]\tLoss: 1.604499\n",
      "Train Epoch: 6 [2560/50000 (5%)]\tLoss: 1.638316\n",
      "Train Epoch: 6 [3840/50000 (8%)]\tLoss: 1.853273\n",
      "Train Epoch: 6 [5120/50000 (10%)]\tLoss: 1.694816\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 1.688826\n",
      "Train Epoch: 6 [7680/50000 (15%)]\tLoss: 1.732137\n",
      "Train Epoch: 6 [8960/50000 (18%)]\tLoss: 1.561391\n",
      "Train Epoch: 6 [10240/50000 (20%)]\tLoss: 1.755007\n",
      "Train Epoch: 6 [11520/50000 (23%)]\tLoss: 1.631400\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 1.718195\n",
      "Train Epoch: 6 [14080/50000 (28%)]\tLoss: 1.785033\n",
      "Train Epoch: 6 [15360/50000 (31%)]\tLoss: 1.634602\n",
      "Train Epoch: 6 [16640/50000 (33%)]\tLoss: 1.747110\n",
      "Train Epoch: 6 [17920/50000 (36%)]\tLoss: 1.604878\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 1.680603\n",
      "Train Epoch: 6 [20480/50000 (41%)]\tLoss: 1.720028\n",
      "Train Epoch: 6 [21760/50000 (43%)]\tLoss: 1.654195\n",
      "Train Epoch: 6 [23040/50000 (46%)]\tLoss: 1.778756\n",
      "Train Epoch: 6 [24320/50000 (49%)]\tLoss: 1.826006\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 1.699825\n",
      "Train Epoch: 6 [26880/50000 (54%)]\tLoss: 1.673563\n",
      "Train Epoch: 6 [28160/50000 (56%)]\tLoss: 1.741926\n",
      "Train Epoch: 6 [29440/50000 (59%)]\tLoss: 1.653961\n",
      "Train Epoch: 6 [30720/50000 (61%)]\tLoss: 1.589217\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 1.695832\n",
      "Train Epoch: 6 [33280/50000 (66%)]\tLoss: 1.681001\n",
      "Train Epoch: 6 [34560/50000 (69%)]\tLoss: 1.782671\n",
      "Train Epoch: 6 [35840/50000 (72%)]\tLoss: 1.702353\n",
      "Train Epoch: 6 [37120/50000 (74%)]\tLoss: 1.746990\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 1.800966\n",
      "Train Epoch: 6 [39680/50000 (79%)]\tLoss: 1.694504\n",
      "Train Epoch: 6 [40960/50000 (82%)]\tLoss: 1.755325\n",
      "Train Epoch: 6 [42240/50000 (84%)]\tLoss: 1.895624\n",
      "Train Epoch: 6 [43520/50000 (87%)]\tLoss: 1.702202\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 1.871748\n",
      "Train Epoch: 6 [46080/50000 (92%)]\tLoss: 1.707734\n",
      "Train Epoch: 6 [47360/50000 (95%)]\tLoss: 1.731674\n",
      "Train Epoch: 6 [48640/50000 (97%)]\tLoss: 1.664533\n",
      "Train Epoch: 6 [31200/50000 (100%)]\tLoss: 1.605574\n",
      "\n",
      "Test set: Avg. loss: 1.6844, Accuracy: 4050/10000 (40%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 1.594412\n",
      "Train Epoch: 7 [1280/50000 (3%)]\tLoss: 1.629467\n",
      "Train Epoch: 7 [2560/50000 (5%)]\tLoss: 1.783844\n",
      "Train Epoch: 7 [3840/50000 (8%)]\tLoss: 1.565820\n",
      "Train Epoch: 7 [5120/50000 (10%)]\tLoss: 1.664849\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 1.647919\n",
      "Train Epoch: 7 [7680/50000 (15%)]\tLoss: 1.751792\n",
      "Train Epoch: 7 [8960/50000 (18%)]\tLoss: 1.765447\n",
      "Train Epoch: 7 [10240/50000 (20%)]\tLoss: 1.730999\n",
      "Train Epoch: 7 [11520/50000 (23%)]\tLoss: 1.555232\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 1.783136\n",
      "Train Epoch: 7 [14080/50000 (28%)]\tLoss: 1.676508\n",
      "Train Epoch: 7 [15360/50000 (31%)]\tLoss: 1.587663\n",
      "Train Epoch: 7 [16640/50000 (33%)]\tLoss: 1.634191\n",
      "Train Epoch: 7 [17920/50000 (36%)]\tLoss: 1.565460\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 1.707405\n",
      "Train Epoch: 7 [20480/50000 (41%)]\tLoss: 1.743305\n",
      "Train Epoch: 7 [21760/50000 (43%)]\tLoss: 1.717787\n",
      "Train Epoch: 7 [23040/50000 (46%)]\tLoss: 1.760965\n",
      "Train Epoch: 7 [24320/50000 (49%)]\tLoss: 1.700035\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 1.741041\n",
      "Train Epoch: 7 [26880/50000 (54%)]\tLoss: 1.614803\n",
      "Train Epoch: 7 [28160/50000 (56%)]\tLoss: 1.587938\n",
      "Train Epoch: 7 [29440/50000 (59%)]\tLoss: 1.676440\n",
      "Train Epoch: 7 [30720/50000 (61%)]\tLoss: 1.642782\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 1.743860\n",
      "Train Epoch: 7 [33280/50000 (66%)]\tLoss: 1.678995\n",
      "Train Epoch: 7 [34560/50000 (69%)]\tLoss: 1.605425\n",
      "Train Epoch: 7 [35840/50000 (72%)]\tLoss: 1.695572\n",
      "Train Epoch: 7 [37120/50000 (74%)]\tLoss: 1.630933\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 1.681743\n",
      "Train Epoch: 7 [39680/50000 (79%)]\tLoss: 1.656118\n",
      "Train Epoch: 7 [40960/50000 (82%)]\tLoss: 1.642392\n",
      "Train Epoch: 7 [42240/50000 (84%)]\tLoss: 1.603181\n",
      "Train Epoch: 7 [43520/50000 (87%)]\tLoss: 1.628186\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 1.753629\n",
      "Train Epoch: 7 [46080/50000 (92%)]\tLoss: 1.623770\n",
      "Train Epoch: 7 [47360/50000 (95%)]\tLoss: 1.666756\n",
      "Train Epoch: 7 [48640/50000 (97%)]\tLoss: 1.642694\n",
      "Train Epoch: 7 [31200/50000 (100%)]\tLoss: 2.044445\n",
      "\n",
      "Test set: Avg. loss: 1.6775, Accuracy: 4135/10000 (41%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 1.617682\n",
      "Train Epoch: 8 [1280/50000 (3%)]\tLoss: 1.761372\n",
      "Train Epoch: 8 [2560/50000 (5%)]\tLoss: 1.797339\n",
      "Train Epoch: 8 [3840/50000 (8%)]\tLoss: 1.686174\n",
      "Train Epoch: 8 [5120/50000 (10%)]\tLoss: 1.617827\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 1.742317\n",
      "Train Epoch: 8 [7680/50000 (15%)]\tLoss: 1.651203\n",
      "Train Epoch: 8 [8960/50000 (18%)]\tLoss: 1.732929\n",
      "Train Epoch: 8 [10240/50000 (20%)]\tLoss: 1.635067\n",
      "Train Epoch: 8 [11520/50000 (23%)]\tLoss: 1.831579\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 1.700196\n",
      "Train Epoch: 8 [14080/50000 (28%)]\tLoss: 1.707526\n",
      "Train Epoch: 8 [15360/50000 (31%)]\tLoss: 1.692240\n",
      "Train Epoch: 8 [16640/50000 (33%)]\tLoss: 1.462173\n",
      "Train Epoch: 8 [17920/50000 (36%)]\tLoss: 1.514880\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 1.797258\n",
      "Train Epoch: 8 [20480/50000 (41%)]\tLoss: 1.719310\n",
      "Train Epoch: 8 [21760/50000 (43%)]\tLoss: 1.678641\n",
      "Train Epoch: 8 [23040/50000 (46%)]\tLoss: 1.731021\n",
      "Train Epoch: 8 [24320/50000 (49%)]\tLoss: 1.536389\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 1.777139\n",
      "Train Epoch: 8 [26880/50000 (54%)]\tLoss: 1.771680\n",
      "Train Epoch: 8 [28160/50000 (56%)]\tLoss: 1.905840\n",
      "Train Epoch: 8 [29440/50000 (59%)]\tLoss: 1.670913\n",
      "Train Epoch: 8 [30720/50000 (61%)]\tLoss: 1.724876\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 1.595545\n",
      "Train Epoch: 8 [33280/50000 (66%)]\tLoss: 1.727225\n",
      "Train Epoch: 8 [34560/50000 (69%)]\tLoss: 1.729058\n",
      "Train Epoch: 8 [35840/50000 (72%)]\tLoss: 1.755754\n",
      "Train Epoch: 8 [37120/50000 (74%)]\tLoss: 1.808507\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 1.641939\n",
      "Train Epoch: 8 [39680/50000 (79%)]\tLoss: 1.623228\n",
      "Train Epoch: 8 [40960/50000 (82%)]\tLoss: 1.717083\n",
      "Train Epoch: 8 [42240/50000 (84%)]\tLoss: 1.733355\n",
      "Train Epoch: 8 [43520/50000 (87%)]\tLoss: 1.599253\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 1.644869\n",
      "Train Epoch: 8 [46080/50000 (92%)]\tLoss: 1.784942\n",
      "Train Epoch: 8 [47360/50000 (95%)]\tLoss: 1.697452\n",
      "Train Epoch: 8 [48640/50000 (97%)]\tLoss: 1.704076\n",
      "Train Epoch: 8 [31200/50000 (100%)]\tLoss: 1.704406\n",
      "\n",
      "Test set: Avg. loss: 1.6744, Accuracy: 4117/10000 (41%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 1.599017\n",
      "Train Epoch: 9 [1280/50000 (3%)]\tLoss: 1.720217\n",
      "Train Epoch: 9 [2560/50000 (5%)]\tLoss: 1.588847\n",
      "Train Epoch: 9 [3840/50000 (8%)]\tLoss: 1.710279\n",
      "Train Epoch: 9 [5120/50000 (10%)]\tLoss: 1.622936\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 1.607522\n",
      "Train Epoch: 9 [7680/50000 (15%)]\tLoss: 1.612960\n",
      "Train Epoch: 9 [8960/50000 (18%)]\tLoss: 1.740358\n",
      "Train Epoch: 9 [10240/50000 (20%)]\tLoss: 1.552477\n",
      "Train Epoch: 9 [11520/50000 (23%)]\tLoss: 1.563814\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 1.691238\n",
      "Train Epoch: 9 [14080/50000 (28%)]\tLoss: 1.666751\n",
      "Train Epoch: 9 [15360/50000 (31%)]\tLoss: 1.616729\n",
      "Train Epoch: 9 [16640/50000 (33%)]\tLoss: 1.684394\n",
      "Train Epoch: 9 [17920/50000 (36%)]\tLoss: 1.528928\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 1.633909\n",
      "Train Epoch: 9 [20480/50000 (41%)]\tLoss: 1.639302\n",
      "Train Epoch: 9 [21760/50000 (43%)]\tLoss: 1.803423\n",
      "Train Epoch: 9 [23040/50000 (46%)]\tLoss: 1.565076\n",
      "Train Epoch: 9 [24320/50000 (49%)]\tLoss: 1.732001\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 1.640633\n",
      "Train Epoch: 9 [26880/50000 (54%)]\tLoss: 1.686913\n",
      "Train Epoch: 9 [28160/50000 (56%)]\tLoss: 1.850779\n",
      "Train Epoch: 9 [29440/50000 (59%)]\tLoss: 1.645562\n",
      "Train Epoch: 9 [30720/50000 (61%)]\tLoss: 1.684112\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 1.528336\n",
      "Train Epoch: 9 [33280/50000 (66%)]\tLoss: 1.584737\n",
      "Train Epoch: 9 [34560/50000 (69%)]\tLoss: 1.671435\n",
      "Train Epoch: 9 [35840/50000 (72%)]\tLoss: 1.526576\n",
      "Train Epoch: 9 [37120/50000 (74%)]\tLoss: 1.673961\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 1.644045\n",
      "Train Epoch: 9 [39680/50000 (79%)]\tLoss: 1.536753\n",
      "Train Epoch: 9 [40960/50000 (82%)]\tLoss: 1.581584\n",
      "Train Epoch: 9 [42240/50000 (84%)]\tLoss: 1.709343\n",
      "Train Epoch: 9 [43520/50000 (87%)]\tLoss: 1.543882\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 1.666877\n",
      "Train Epoch: 9 [46080/50000 (92%)]\tLoss: 1.571361\n",
      "Train Epoch: 9 [47360/50000 (95%)]\tLoss: 1.507269\n",
      "Train Epoch: 9 [48640/50000 (97%)]\tLoss: 1.487773\n",
      "Train Epoch: 9 [31200/50000 (100%)]\tLoss: 1.703911\n",
      "\n",
      "Test set: Avg. loss: 1.6346, Accuracy: 4231/10000 (42%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.530087\n",
      "Train Epoch: 10 [1280/50000 (3%)]\tLoss: 1.570235\n",
      "Train Epoch: 10 [2560/50000 (5%)]\tLoss: 1.538717\n",
      "Train Epoch: 10 [3840/50000 (8%)]\tLoss: 1.652966\n",
      "Train Epoch: 10 [5120/50000 (10%)]\tLoss: 1.617509\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 1.600273\n",
      "Train Epoch: 10 [7680/50000 (15%)]\tLoss: 1.578532\n",
      "Train Epoch: 10 [8960/50000 (18%)]\tLoss: 1.657111\n",
      "Train Epoch: 10 [10240/50000 (20%)]\tLoss: 1.579614\n",
      "Train Epoch: 10 [11520/50000 (23%)]\tLoss: 1.661316\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 1.710509\n",
      "Train Epoch: 10 [14080/50000 (28%)]\tLoss: 1.535920\n",
      "Train Epoch: 10 [15360/50000 (31%)]\tLoss: 1.717250\n",
      "Train Epoch: 10 [16640/50000 (33%)]\tLoss: 1.551544\n",
      "Train Epoch: 10 [17920/50000 (36%)]\tLoss: 1.657995\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 1.652929\n",
      "Train Epoch: 10 [20480/50000 (41%)]\tLoss: 1.602152\n",
      "Train Epoch: 10 [21760/50000 (43%)]\tLoss: 1.584836\n",
      "Train Epoch: 10 [23040/50000 (46%)]\tLoss: 1.558144\n",
      "Train Epoch: 10 [24320/50000 (49%)]\tLoss: 1.653565\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 1.623799\n",
      "Train Epoch: 10 [26880/50000 (54%)]\tLoss: 1.782178\n",
      "Train Epoch: 10 [28160/50000 (56%)]\tLoss: 1.529311\n",
      "Train Epoch: 10 [29440/50000 (59%)]\tLoss: 1.685721\n",
      "Train Epoch: 10 [30720/50000 (61%)]\tLoss: 1.843387\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 1.638960\n",
      "Train Epoch: 10 [33280/50000 (66%)]\tLoss: 1.603516\n",
      "Train Epoch: 10 [34560/50000 (69%)]\tLoss: 1.553257\n",
      "Train Epoch: 10 [35840/50000 (72%)]\tLoss: 1.576546\n",
      "Train Epoch: 10 [37120/50000 (74%)]\tLoss: 1.668909\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 1.643231\n",
      "Train Epoch: 10 [39680/50000 (79%)]\tLoss: 1.739504\n",
      "Train Epoch: 10 [40960/50000 (82%)]\tLoss: 1.725322\n",
      "Train Epoch: 10 [42240/50000 (84%)]\tLoss: 1.528107\n",
      "Train Epoch: 10 [43520/50000 (87%)]\tLoss: 1.602490\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 1.531958\n",
      "Train Epoch: 10 [46080/50000 (92%)]\tLoss: 1.699221\n",
      "Train Epoch: 10 [47360/50000 (95%)]\tLoss: 1.763601\n",
      "Train Epoch: 10 [48640/50000 (97%)]\tLoss: 1.681078\n",
      "Train Epoch: 10 [31200/50000 (100%)]\tLoss: 1.714889\n",
      "\n",
      "Test set: Avg. loss: 1.6307, Accuracy: 4286/10000 (43%)\n",
      "\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 1.604787\n",
      "Train Epoch: 11 [1280/50000 (3%)]\tLoss: 1.515944\n",
      "Train Epoch: 11 [2560/50000 (5%)]\tLoss: 1.616691\n",
      "Train Epoch: 11 [3840/50000 (8%)]\tLoss: 1.559658\n",
      "Train Epoch: 11 [5120/50000 (10%)]\tLoss: 1.526289\n",
      "Train Epoch: 11 [6400/50000 (13%)]\tLoss: 1.522827\n",
      "Train Epoch: 11 [7680/50000 (15%)]\tLoss: 1.578589\n",
      "Train Epoch: 11 [8960/50000 (18%)]\tLoss: 1.531665\n",
      "Train Epoch: 11 [10240/50000 (20%)]\tLoss: 1.627806\n",
      "Train Epoch: 11 [11520/50000 (23%)]\tLoss: 1.587288\n",
      "Train Epoch: 11 [12800/50000 (26%)]\tLoss: 1.650582\n",
      "Train Epoch: 11 [14080/50000 (28%)]\tLoss: 1.624392\n",
      "Train Epoch: 11 [15360/50000 (31%)]\tLoss: 1.824146\n",
      "Train Epoch: 11 [16640/50000 (33%)]\tLoss: 1.527684\n",
      "Train Epoch: 11 [17920/50000 (36%)]\tLoss: 1.577976\n",
      "Train Epoch: 11 [19200/50000 (38%)]\tLoss: 1.628051\n",
      "Train Epoch: 11 [20480/50000 (41%)]\tLoss: 1.614224\n",
      "Train Epoch: 11 [21760/50000 (43%)]\tLoss: 1.469755\n",
      "Train Epoch: 11 [23040/50000 (46%)]\tLoss: 1.569278\n",
      "Train Epoch: 11 [24320/50000 (49%)]\tLoss: 1.493049\n",
      "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 1.567741\n",
      "Train Epoch: 11 [26880/50000 (54%)]\tLoss: 1.653285\n",
      "Train Epoch: 11 [28160/50000 (56%)]\tLoss: 1.601324\n",
      "Train Epoch: 11 [29440/50000 (59%)]\tLoss: 1.470383\n",
      "Train Epoch: 11 [30720/50000 (61%)]\tLoss: 1.677134\n",
      "Train Epoch: 11 [32000/50000 (64%)]\tLoss: 1.622104\n",
      "Train Epoch: 11 [33280/50000 (66%)]\tLoss: 1.600435\n",
      "Train Epoch: 11 [34560/50000 (69%)]\tLoss: 1.581756\n",
      "Train Epoch: 11 [35840/50000 (72%)]\tLoss: 1.695180\n",
      "Train Epoch: 11 [37120/50000 (74%)]\tLoss: 1.657358\n",
      "Train Epoch: 11 [38400/50000 (77%)]\tLoss: 1.583642\n",
      "Train Epoch: 11 [39680/50000 (79%)]\tLoss: 1.665053\n",
      "Train Epoch: 11 [40960/50000 (82%)]\tLoss: 1.561409\n",
      "Train Epoch: 11 [42240/50000 (84%)]\tLoss: 1.525566\n",
      "Train Epoch: 11 [43520/50000 (87%)]\tLoss: 1.561236\n",
      "Train Epoch: 11 [44800/50000 (90%)]\tLoss: 1.637784\n",
      "Train Epoch: 11 [46080/50000 (92%)]\tLoss: 1.620420\n",
      "Train Epoch: 11 [47360/50000 (95%)]\tLoss: 1.511452\n",
      "Train Epoch: 11 [48640/50000 (97%)]\tLoss: 1.397439\n",
      "Train Epoch: 11 [31200/50000 (100%)]\tLoss: 1.648879\n",
      "\n",
      "Test set: Avg. loss: 1.6113, Accuracy: 4266/10000 (43%)\n",
      "\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 1.471721\n",
      "Train Epoch: 12 [1280/50000 (3%)]\tLoss: 1.630249\n",
      "Train Epoch: 12 [2560/50000 (5%)]\tLoss: 1.664766\n",
      "Train Epoch: 12 [3840/50000 (8%)]\tLoss: 1.557956\n",
      "Train Epoch: 12 [5120/50000 (10%)]\tLoss: 1.698899\n",
      "Train Epoch: 12 [6400/50000 (13%)]\tLoss: 1.555348\n",
      "Train Epoch: 12 [7680/50000 (15%)]\tLoss: 1.609375\n",
      "Train Epoch: 12 [8960/50000 (18%)]\tLoss: 1.628726\n",
      "Train Epoch: 12 [10240/50000 (20%)]\tLoss: 1.593320\n",
      "Train Epoch: 12 [11520/50000 (23%)]\tLoss: 1.479956\n",
      "Train Epoch: 12 [12800/50000 (26%)]\tLoss: 1.655116\n",
      "Train Epoch: 12 [14080/50000 (28%)]\tLoss: 1.548141\n",
      "Train Epoch: 12 [15360/50000 (31%)]\tLoss: 1.551717\n",
      "Train Epoch: 12 [16640/50000 (33%)]\tLoss: 1.512344\n",
      "Train Epoch: 12 [17920/50000 (36%)]\tLoss: 1.550796\n",
      "Train Epoch: 12 [19200/50000 (38%)]\tLoss: 1.610348\n",
      "Train Epoch: 12 [20480/50000 (41%)]\tLoss: 1.509344\n",
      "Train Epoch: 12 [21760/50000 (43%)]\tLoss: 1.601352\n",
      "Train Epoch: 12 [23040/50000 (46%)]\tLoss: 1.529210\n",
      "Train Epoch: 12 [24320/50000 (49%)]\tLoss: 1.650346\n",
      "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 1.580641\n",
      "Train Epoch: 12 [26880/50000 (54%)]\tLoss: 1.601037\n",
      "Train Epoch: 12 [28160/50000 (56%)]\tLoss: 1.641002\n",
      "Train Epoch: 12 [29440/50000 (59%)]\tLoss: 1.586160\n",
      "Train Epoch: 12 [30720/50000 (61%)]\tLoss: 1.531092\n",
      "Train Epoch: 12 [32000/50000 (64%)]\tLoss: 1.665529\n",
      "Train Epoch: 12 [33280/50000 (66%)]\tLoss: 1.647149\n",
      "Train Epoch: 12 [34560/50000 (69%)]\tLoss: 1.730353\n",
      "Train Epoch: 12 [35840/50000 (72%)]\tLoss: 1.667820\n",
      "Train Epoch: 12 [37120/50000 (74%)]\tLoss: 1.693098\n",
      "Train Epoch: 12 [38400/50000 (77%)]\tLoss: 1.655420\n",
      "Train Epoch: 12 [39680/50000 (79%)]\tLoss: 1.608645\n",
      "Train Epoch: 12 [40960/50000 (82%)]\tLoss: 1.681402\n",
      "Train Epoch: 12 [42240/50000 (84%)]\tLoss: 1.492752\n",
      "Train Epoch: 12 [43520/50000 (87%)]\tLoss: 1.598654\n",
      "Train Epoch: 12 [44800/50000 (90%)]\tLoss: 1.609536\n",
      "Train Epoch: 12 [46080/50000 (92%)]\tLoss: 1.671253\n",
      "Train Epoch: 12 [47360/50000 (95%)]\tLoss: 1.503229\n",
      "Train Epoch: 12 [48640/50000 (97%)]\tLoss: 1.619797\n",
      "Train Epoch: 12 [31200/50000 (100%)]\tLoss: 1.465216\n",
      "\n",
      "Test set: Avg. loss: 1.6038, Accuracy: 4329/10000 (43%)\n",
      "\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 1.617275\n",
      "Train Epoch: 13 [1280/50000 (3%)]\tLoss: 1.812954\n",
      "Train Epoch: 13 [2560/50000 (5%)]\tLoss: 1.645495\n",
      "Train Epoch: 13 [3840/50000 (8%)]\tLoss: 1.643744\n",
      "Train Epoch: 13 [5120/50000 (10%)]\tLoss: 1.580713\n",
      "Train Epoch: 13 [6400/50000 (13%)]\tLoss: 1.616479\n",
      "Train Epoch: 13 [7680/50000 (15%)]\tLoss: 1.681909\n",
      "Train Epoch: 13 [8960/50000 (18%)]\tLoss: 1.571467\n",
      "Train Epoch: 13 [10240/50000 (20%)]\tLoss: 1.758124\n",
      "Train Epoch: 13 [11520/50000 (23%)]\tLoss: 1.674687\n",
      "Train Epoch: 13 [12800/50000 (26%)]\tLoss: 1.726979\n",
      "Train Epoch: 13 [14080/50000 (28%)]\tLoss: 1.489247\n",
      "Train Epoch: 13 [15360/50000 (31%)]\tLoss: 1.489109\n",
      "Train Epoch: 13 [16640/50000 (33%)]\tLoss: 1.508727\n",
      "Train Epoch: 13 [17920/50000 (36%)]\tLoss: 1.610581\n",
      "Train Epoch: 13 [19200/50000 (38%)]\tLoss: 1.513849\n",
      "Train Epoch: 13 [20480/50000 (41%)]\tLoss: 1.658583\n",
      "Train Epoch: 13 [21760/50000 (43%)]\tLoss: 1.646087\n",
      "Train Epoch: 13 [23040/50000 (46%)]\tLoss: 1.553367\n",
      "Train Epoch: 13 [24320/50000 (49%)]\tLoss: 1.611529\n",
      "Train Epoch: 13 [25600/50000 (51%)]\tLoss: 1.683641\n",
      "Train Epoch: 13 [26880/50000 (54%)]\tLoss: 1.615185\n",
      "Train Epoch: 13 [28160/50000 (56%)]\tLoss: 1.589617\n",
      "Train Epoch: 13 [29440/50000 (59%)]\tLoss: 1.454769\n",
      "Train Epoch: 13 [30720/50000 (61%)]\tLoss: 1.566316\n",
      "Train Epoch: 13 [32000/50000 (64%)]\tLoss: 1.611046\n",
      "Train Epoch: 13 [33280/50000 (66%)]\tLoss: 1.539044\n",
      "Train Epoch: 13 [34560/50000 (69%)]\tLoss: 1.692899\n",
      "Train Epoch: 13 [35840/50000 (72%)]\tLoss: 1.433999\n",
      "Train Epoch: 13 [37120/50000 (74%)]\tLoss: 1.576327\n",
      "Train Epoch: 13 [38400/50000 (77%)]\tLoss: 1.553681\n",
      "Train Epoch: 13 [39680/50000 (79%)]\tLoss: 1.760518\n",
      "Train Epoch: 13 [40960/50000 (82%)]\tLoss: 1.642749\n",
      "Train Epoch: 13 [42240/50000 (84%)]\tLoss: 1.699937\n",
      "Train Epoch: 13 [43520/50000 (87%)]\tLoss: 1.460226\n",
      "Train Epoch: 13 [44800/50000 (90%)]\tLoss: 1.446923\n",
      "Train Epoch: 13 [46080/50000 (92%)]\tLoss: 1.528120\n",
      "Train Epoch: 13 [47360/50000 (95%)]\tLoss: 1.612907\n",
      "Train Epoch: 13 [48640/50000 (97%)]\tLoss: 1.551838\n",
      "Train Epoch: 13 [31200/50000 (100%)]\tLoss: 1.510340\n",
      "\n",
      "Test set: Avg. loss: 1.6090, Accuracy: 4336/10000 (43%)\n",
      "\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 1.605714\n",
      "Train Epoch: 14 [1280/50000 (3%)]\tLoss: 1.717142\n",
      "Train Epoch: 14 [2560/50000 (5%)]\tLoss: 1.657316\n",
      "Train Epoch: 14 [3840/50000 (8%)]\tLoss: 1.517928\n",
      "Train Epoch: 14 [5120/50000 (10%)]\tLoss: 1.549905\n",
      "Train Epoch: 14 [6400/50000 (13%)]\tLoss: 1.760233\n",
      "Train Epoch: 14 [7680/50000 (15%)]\tLoss: 1.518845\n",
      "Train Epoch: 14 [8960/50000 (18%)]\tLoss: 1.542449\n",
      "Train Epoch: 14 [10240/50000 (20%)]\tLoss: 1.695882\n",
      "Train Epoch: 14 [11520/50000 (23%)]\tLoss: 1.557413\n",
      "Train Epoch: 14 [12800/50000 (26%)]\tLoss: 1.435238\n",
      "Train Epoch: 14 [14080/50000 (28%)]\tLoss: 1.540343\n",
      "Train Epoch: 14 [15360/50000 (31%)]\tLoss: 1.581612\n",
      "Train Epoch: 14 [16640/50000 (33%)]\tLoss: 1.744841\n",
      "Train Epoch: 14 [17920/50000 (36%)]\tLoss: 1.542101\n",
      "Train Epoch: 14 [19200/50000 (38%)]\tLoss: 1.530019\n",
      "Train Epoch: 14 [20480/50000 (41%)]\tLoss: 1.735137\n",
      "Train Epoch: 14 [21760/50000 (43%)]\tLoss: 1.445013\n",
      "Train Epoch: 14 [23040/50000 (46%)]\tLoss: 1.516363\n",
      "Train Epoch: 14 [24320/50000 (49%)]\tLoss: 1.591826\n",
      "Train Epoch: 14 [25600/50000 (51%)]\tLoss: 1.510275\n",
      "Train Epoch: 14 [26880/50000 (54%)]\tLoss: 1.632016\n",
      "Train Epoch: 14 [28160/50000 (56%)]\tLoss: 1.439883\n",
      "Train Epoch: 14 [29440/50000 (59%)]\tLoss: 1.349641\n",
      "Train Epoch: 14 [30720/50000 (61%)]\tLoss: 1.552019\n",
      "Train Epoch: 14 [32000/50000 (64%)]\tLoss: 1.501522\n",
      "Train Epoch: 14 [33280/50000 (66%)]\tLoss: 1.440466\n",
      "Train Epoch: 14 [34560/50000 (69%)]\tLoss: 1.501142\n",
      "Train Epoch: 14 [35840/50000 (72%)]\tLoss: 1.655361\n",
      "Train Epoch: 14 [37120/50000 (74%)]\tLoss: 1.611241\n",
      "Train Epoch: 14 [38400/50000 (77%)]\tLoss: 1.594985\n",
      "Train Epoch: 14 [39680/50000 (79%)]\tLoss: 1.671434\n",
      "Train Epoch: 14 [40960/50000 (82%)]\tLoss: 1.701619\n",
      "Train Epoch: 14 [42240/50000 (84%)]\tLoss: 1.407555\n",
      "Train Epoch: 14 [43520/50000 (87%)]\tLoss: 1.528687\n",
      "Train Epoch: 14 [44800/50000 (90%)]\tLoss: 1.558902\n",
      "Train Epoch: 14 [46080/50000 (92%)]\tLoss: 1.561106\n",
      "Train Epoch: 14 [47360/50000 (95%)]\tLoss: 1.531501\n",
      "Train Epoch: 14 [48640/50000 (97%)]\tLoss: 1.667822\n",
      "Train Epoch: 14 [31200/50000 (100%)]\tLoss: 1.537630\n",
      "\n",
      "Test set: Avg. loss: 1.5909, Accuracy: 4388/10000 (44%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_step =10\n",
    "leak = 0.99\n",
    "\n",
    "data = \"cifar10\"\n",
    "toy = VGG_5(time_step,leak,data).cuda()\n",
    "\n",
    "if data == \"cifar10\":\n",
    "    train_loader = train_loader_cifar10\n",
    "    input_dim = 3\n",
    "    pre_linear_dim = 8\n",
    "    test_loader = test_loader_cifar10\n",
    "elif data == \"mnist\":\n",
    "    train_loader = train_loader_mnist\n",
    "    input_dim = 1\n",
    "    pre_linear_dim = 7\n",
    "    test_loader = test_loader_mnist\n",
    "# vgg = VGG_5(time_step)\n",
    "# vgg =vgg.cuda()\n",
    "# print(\"weight\",toy.fc_1.weight)\n",
    "# torch.nn.init.normal_(toy.fc_1.weight, mean=0.0, std=0.1)\n",
    "# toy.fc_1.weight.data = quant(toy.fc_1.weight,2**4)\n",
    "# torch.nn.init.normal_(toy.fc_2.weight, mean=0.0, std=0.1)\n",
    "# toy.fc_2.weight.data = quant(toy.fc_2.weight,2**4)\n",
    "# torch.nn.init.normal_(toy.fc_out.weight, mean=0.0, std=0.1)\n",
    "# toy.fc_out.weight.data = quant(toy.fc_out.weight,2**4)\n",
    "# print(\"quantized weight\",toy.fc_1.weight)\n",
    "lr = 0.006\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# test(toy,data,test_loader)\n",
    "running_loss = 0.0\n",
    "for epoch in range(15):\n",
    "    if epoch > 2:\n",
    "        lr = 0.004\n",
    "    if epoch > 4:\n",
    "        lr = 0.002\n",
    "    if epoch>8:\n",
    "        lr = 0.001\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # print(torch.mean(data))\n",
    "        with torch.no_grad():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "            out = toy(data)\n",
    "#             print(\"memory after fwd\",torch.cuda.memory_allocated()/10000000)\n",
    "        out = Variable(out,requires_grad=True)\n",
    "\n",
    "        # err = loss(out,target,reduction='sum')\n",
    "        err = F.cross_entropy(out, target,reduction='mean')\n",
    "        err.backward()\n",
    "\n",
    "        # exp = torch.exp(out)\n",
    "        # exp_sum = torch.sum(torch.exp(out),1, keepdim=True)   \n",
    "        # target = F.one_hot(target, num_classes=10)\n",
    "        # #L = -1*torch.sum((target*torch.log((exp/exp_sum))),1, keepdim=True)\n",
    "        # du_out = exp/exp_sum\n",
    "        # du_out = (du_out - target)/batch_size_train\n",
    "        du_out = out.grad\n",
    "\n",
    "        bp_VGG5(toy,leak,time_step,du_out,lr,0.5,pre_linear_dim,input_dim)\n",
    "#             print(\"memory after bp\",torch.cuda.memory_allocated()/10000000)\n",
    "\n",
    "        # bp_MLP(toy,leak,time_step,du_out,toy.s_regs_inp,lr,0.5)\n",
    "\n",
    "\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), err.item()))\n",
    "            \n",
    "        # print statistics\n",
    "        # running_loss += err.item()\n",
    "        # if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "        #     print('[%d, %5d] loss: %.3f' %\n",
    "        #           (epoch + 1, i + 1, running_loss / 2000))\n",
    "        #     running_loss = 0.0\n",
    "\n",
    "#             del toy.lif_conv1.s_regs\n",
    "#             del toy.lif_conv1.u_regs\n",
    "#             del toy.lif_conv1.du_regs\n",
    "#             del toy.lif_fc1.du_regs\n",
    "#             del toy.lif_fc1.u_regs\n",
    "#             del toy.lif_fc1.s_regs\n",
    "#             del toy.s_regs_conv\n",
    "#             del toy.s_regs_inp\n",
    "#             del data\n",
    "#             del target\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#             gc.collect()\n",
    "#             print(\"memory after clear\",torch.cuda.memory_allocated()/10000000)\n",
    "\n",
    "    test(toy,data,test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ec3880-e283-4519-8552-3a502e0fae78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3842f727-73a7-4f8d-9f6d-44b90e602b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
