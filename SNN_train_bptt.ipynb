{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251e3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import gc\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b07e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "33e21ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PoissonGen(inp, rescale_fac=2.0):\n",
    "    rand_inp = torch.rand_like(inp)\n",
    "    return torch.mul(torch.le(rand_inp * rescale_fac, torch.abs(inp)).float(), torch.sign(inp))\n",
    "\n",
    "# def spike_function(x):\n",
    "#     x[x>0] = 1\n",
    "#     x[x<=0] = 0\n",
    "#     return x\n",
    "\n",
    "def de_func(U,th):\n",
    "    alpha = 0.3\n",
    "    U = alpha*(1.0 - abs((U-th)/th))\n",
    "    U[U<0]=0\n",
    "    return U\n",
    "\n",
    "def test(toy,data,test_loader):\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    toy = toy.cuda()\n",
    "    for data, target in test_loader:\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "        output = toy(data)\n",
    "        test_loss +=F.cross_entropy(output, target,reduction='mean').item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    \n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "#     with torch.no_grad():\n",
    "#         for data in test_loader:\n",
    "#             toy = toy.cuda()\n",
    "            \n",
    "#             images, labels = data\n",
    "#             images = images.cuda()\n",
    "#             labels = labels.cuda()\n",
    "#             # calculate outputs by running images through the network\n",
    "#             outputs = toy(images)\n",
    "#             # the class with the highest energy is what we choose as prediction\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "#         100 * correct / total))\n",
    "\n",
    "def quant(input, k):\n",
    "    size = input.size()\n",
    "    #mean = torch.mean(input.abs(), 1, keepdim=True)\n",
    "    x = input\n",
    "    #print(x)\n",
    "    xmax = x.abs().max()\n",
    "    num_bits=k\n",
    "    v0 = 1\n",
    "    v1 = 2\n",
    "    v2 = -0.5\n",
    "    y = k #2.**num_bits - 1.\n",
    "    #print(y)\n",
    "    x = x.add(v0).div(v1)\n",
    "    #print(x)\n",
    "    x = x.mul(y).round_()\n",
    "    #print(x)\n",
    "    x = x.div(y)\n",
    "    #print(x)\n",
    "    x = x.add(v2)\n",
    "    #print(x)\n",
    "    x = x.mul(v1)\n",
    "    #print(x)\n",
    "    input = x\n",
    "    return input\n",
    "\n",
    "def conv_weight_update(dH,X,pad):\n",
    "    shap = dH.shape[1]\n",
    "    batch = dH.shape[0]\n",
    "    shap_X = X.shape[1]\n",
    "#     dH = torch.sum(dH,0)\n",
    "#     dH = dH/batch\n",
    "#     dH = torch.unsqueeze(dH,1)\n",
    "    dH = torch.repeat_interleave(dH,X.shape[1],1)\n",
    "    dH = dH.view(1,batch*dH.shape[1],dH.shape[-1],dH.shape[-1])\n",
    "#     X = torch.sum(X,0)\n",
    "#     X = torch.unsqueeze(X,0)\n",
    "    X = X.repeat(1,shap,1,1)\n",
    "    X = X.view(1,batch*X.shape[1],X.shape[-1],X.shape[-1])\n",
    "    dw_conv = F.conv2d(X,dH,padding=pad,groups=X.shape[1])\n",
    "    \n",
    "    dw_conv = torch.sum(dw_conv,0)\n",
    "    dw_conv = dw_conv.view(shap,shap_X,dw_conv.shape[-1],dw_conv.shape[-1])\n",
    "    return dw_conv/batch\n",
    "\n",
    "def conv_dx_update(dH,W,pad):\n",
    "\n",
    "    W = torch.transpose(W,0,1)\n",
    "    W = torch.flip(W,[-1,-2]) # W = C*3*3\n",
    "    dx_conv = F.conv2d(dH,W,padding=1)\n",
    "    \n",
    "    return dx_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d766c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, time_step,leak):\n",
    "        super(model, self).__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(28*28,256,bias=False)\n",
    "        self.fc_2 = nn.Linear(256,256,bias=False)\n",
    "        self.fc_out = nn.Linear(256,10,bias=False)\n",
    "        \n",
    "        self.lif1 = LIF(time_step,leak)\n",
    "        self.lif2 = LIF(time_step,leak)\n",
    "        self.time_step = time_step\n",
    "        self.s_regs_inp = None\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        inp = inp.view(inp.shape[0],-1)\n",
    "        size = inp.shape\n",
    "        self.s_regs_inp = torch.zeros(self.time_step,*size, device=device)\n",
    "        u_out = 0\n",
    "        \n",
    "        for t in range(self.time_step):\n",
    "            \n",
    "            spike_inp = PoissonGen(inp)\n",
    "            self.s_regs_inp[t] += spike_inp \n",
    "            \n",
    "            x = self.fc_1(spike_inp)\n",
    "            #x = quant(x,2**4)\n",
    "            x = self.lif1(x, t)\n",
    "            x = self.fc_2(x)\n",
    "            #x = quant(x,2**4)\n",
    "            x = self.lif2(x, t)\n",
    "            x = self.fc_out(x)\n",
    "            u_out = u_out + x\n",
    "        return u_out/self.time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8d72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,time_step,leak):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(28*28,512,bias=False)\n",
    "        self.fc_out = nn.Linear(512,10,bias=False)\n",
    "        self.lif1 = LIF(time_step,leak)\n",
    "        self.time_step = time_step\n",
    "        self.s_regs_inp = None\n",
    "        \n",
    "    def forward(self, inp):\n",
    "#         print(\"size is:\", (inp.view(inp.shape[0],1,28,28)).shape)\n",
    "        inp = inp.view(inp.shape[0],-1)\n",
    "        size = inp.shape\n",
    "        \n",
    "        self.s_regs_inp = torch.zeros(self.time_step,*size, device=device)\n",
    "        u_out = 0\n",
    "        \n",
    "        for t in range(self.time_step):\n",
    "            spike_inp = PoissonGen(inp)\n",
    "            self.s_regs_inp[t] += spike_inp \n",
    "            x = self.fc_1(spike_inp)\n",
    "            #x = quant(x,2**4)\n",
    "            x = self.lif1(x, t)\n",
    "            x = self.fc_out(x)\n",
    "            u_out = u_out + x\n",
    "        return u_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28765ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_5(nn.Module):\n",
    "    def __init__(self,time_step, leak,data):\n",
    "        super(VGG_5, self).__init__()\n",
    "        \n",
    "        if data == \"cifar10\":\n",
    "            input_dim = 3\n",
    "            pre_linear_dim = 8\n",
    "        elif data == \"mnist\":\n",
    "            input_dim = 1\n",
    "            pre_linear_dim = 7\n",
    "        \n",
    "        self.time_step = time_step\n",
    "        self.s_regs_inp = None\n",
    "        self.s_regs_conv = None\n",
    "        self.conv1 = nn.Conv2d(input_dim, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv_lif1 = LIF(time_step, leak)\n",
    "        # self.conv1a = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        # self.conv_lif1a = LIF(time_step, leak)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2,return_indices=True)\n",
    "        self.pool1_ind = []\n",
    "        self.unpool1 = nn.MaxUnpool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv_lif2 = LIF(time_step, leak)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv_lif3 = LIF(time_step, leak)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2,return_indices=True)\n",
    "        self.pool2_ind = []\n",
    "        self.unpool2 = nn.MaxUnpool2d(kernel_size=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * pre_linear_dim * pre_linear_dim, 1024, bias=False)\n",
    "        self.fc_lif1 = LIF(time_step,leak)\n",
    "        self.fc_out = nn.Linear(1024, 10, bias=False)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "\n",
    "        size = inp.shape\n",
    "        self.s_regs_inp = torch.zeros(self.time_step,*size, device=device)\n",
    "        self.pool1_ind = []\n",
    "        self.pool2_ind = []\n",
    "        u_out = 0\n",
    "        \n",
    "        for t in range(self.time_step):\n",
    "            spike_inp = PoissonGen(inp)\n",
    "            self.s_regs_inp[t] += spike_inp \n",
    "            x = self.conv1(spike_inp)\n",
    "            x = self.conv_lif1(x,t)\n",
    "            # x = self.conv1a(x)\n",
    "            # x = self.conv_lif1a(x,t)\n",
    "            x,indices = self.pool1(x)\n",
    "            self.pool1_ind.append(indices)\n",
    "            x = self.conv2(x)\n",
    "            x = self.conv_lif2(x,t)\n",
    "            x = self.conv3(x)\n",
    "            x = self.conv_lif3(x,t)\n",
    "            x,indices = self.pool2(x)\n",
    "            x = x.view(x.shape[0],-1)\n",
    "            \n",
    "            if t == 0:\n",
    "                self.s_regs_conv = torch.zeros(self.time_step,*x.shape, device=device)\n",
    "            self.pool2_ind.append(indices)\n",
    "            self.s_regs_conv[t] += x\n",
    "            \n",
    "            x = self.fc1(x)\n",
    "            x = self.fc_lif1(x,t)\n",
    "            x = self.fc_out(x)\n",
    "            u_out = u_out + x\n",
    "        return u_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37c149d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_VGG5(vgg,leak,time_step,du_out,l_r,mom,lamb,th,pre_linear_dim,input_dim):\n",
    "    \n",
    "#     batch = du_out.shape[0]\n",
    "    \n",
    "    ## Update weight in FCs, time T\n",
    "    du_fc1 = torch.matmul(du_out,vgg.fc_out.weight)*de_func(vgg.fc_lif1.u_regs[-1],th)\n",
    "    vgg.fc_lif1.du_regs[-1] += du_fc1\n",
    "    w_conv_1 = torch.matmul(torch.transpose(du_fc1,0,1),vgg.s_regs_conv[-1])\n",
    "    vgg.fc1.weight.data -= l_r*(w_conv_1 + lamb*torch.abs(vgg.fc1.weight))    \n",
    "    w_1_out = torch.matmul(torch.transpose(du_out,0,1),vgg.fc_lif1.s_regs[-1])\n",
    "    vgg.fc_out.weight.data -= l_r*(w_1_out + lamb*torch.abs(vgg.fc_out.weight))\n",
    "    \n",
    "    ## Update du in pool2, time T\n",
    "    dx_pool2 = torch.matmul(du_fc1,vgg.fc1.weight)\n",
    "    dx_pool2 = dx_pool2.view(dx_pool2.shape[0],128,pre_linear_dim,pre_linear_dim)\n",
    "    du_pool2 = vgg.unpool2(dx_pool2,vgg.pool2_ind[-1])\n",
    "    \n",
    "    ## Update du and dw in conv3, time T\n",
    "    du_conv3 = du_pool2*de_func(toy.conv_lif3.u_regs[-1],th)\n",
    "    vgg.conv_lif3.du_regs[-1] += du_conv3 \n",
    "    dW_conv3 = conv_weight_update(du_pool2.type(torch.float),vgg.conv_lif2.s_regs[-1].type(torch.float),1)\n",
    "#     dW_conv3 = torch.sum(dW_conv3,0)\n",
    "#     dW_conv3 = dW_conv3.view(128,64,dW_conv3.shape[-1],dW_conv3.shape[-1])\n",
    "    vgg.conv3.weight.data -=l_r*(dW_conv3 + lamb*torch.abs(vgg.conv3.weight))\n",
    "    \n",
    "    ## Update du and dw in conv2, time T\n",
    "    \n",
    "    du_conv2 = conv_dx_update(du_conv3,vgg.conv3.weight,'same')*de_func(toy.conv_lif2.u_regs[-1],th)\n",
    "    vgg.conv_lif2.du_regs[-1] += du_conv2\n",
    "    dW_conv2 = conv_weight_update(du_conv2.type(torch.float),F.max_pool2d(vgg.conv_lif1.s_regs[-1].type(torch.float),kernel_size=2),1)\n",
    "#     dW_conv2 = torch.sum(dW_conv2,0)\n",
    "#     dW_conv2 = dW_conv2.view(64,64,dW_conv2.shape[-1],dW_conv2.shape[-1])\n",
    "    vgg.conv2.weight.data -=l_r*(dW_conv2 + lamb*torch.abs(vgg.conv2.weight))\n",
    "    \n",
    "    ## Update du in pool2, time t\n",
    "    du_pool1 = vgg.unpool1(conv_dx_update(du_conv2,vgg.conv2.weight,'same'),vgg.pool1_ind[-1])\n",
    "    \n",
    "    # du_conv1a = du_pool1*de_func(toy.conv_lif1a.u_regs[-1],th)\n",
    "    # vgg.conv_lif1a.du_regs[-1] += du_conv1a\n",
    "    # dW_conv1a = conv_weight_update(du_pool1.type(torch.float),vgg.conv_lif1.s_regs[-1].type(torch.float),1)\n",
    "    # dW_conv1a = torch.sum(dW_conv1a,0)\n",
    "    # dW_conv1a = dW_conv1a.view(64,32,dW_conv1a.shape[-1],dW_conv1a.shape[-1])\n",
    "    # vgg.conv1a.weight.data -=l_r*dW_conv1a\n",
    "    \n",
    "    du_conv1 = du_pool1*de_func(toy.conv_lif1.u_regs[-1],th)\n",
    "    vgg.conv_lif1.du_regs[-1] += du_conv1\n",
    "    dW_conv1 = conv_weight_update(du_conv1.type(torch.float),vgg.s_regs_inp[-1].type(torch.float),1)\n",
    "#     dW_conv1 = torch.sum(dW_conv1,0)\n",
    "#     dW_conv1 = dW_conv1.view(64,input_dim,dW_conv1.shape[-1],dW_conv1.shape[-1])\n",
    "    vgg.conv1.weight.data -=l_r*(dW_conv1 + lamb*torch.abs(vgg.conv1.weight))\n",
    "    \n",
    "    prev_fc1 = w_conv_1\n",
    "    prev_fc_out = w_1_out\n",
    "    prev_conv3 = dW_conv3\n",
    "    prev_conv2 = dW_conv2\n",
    "    prev_conv1 = dW_conv1\n",
    "\n",
    "\n",
    "    for t in range(time_step-2,-1,-1):\n",
    "        \n",
    "        ds_fc1 = torch.matmul(du_out,vgg.fc_out.weight)+vgg.fc_lif1.du_regs[t+1]*(-leak*vgg.fc_lif1.du_regs[t])\n",
    "        du_fc1 = (ds_fc1)*de_func(vgg.fc_lif1.du_regs[t],th) + vgg.fc_lif1.du_regs[t+1]*leak*(1-vgg.fc_lif1.s_regs[t])\n",
    "        vgg.fc_lif1.du_regs[t] += du_fc1\n",
    "        w_conv_1 = torch.matmul(torch.transpose(du_fc1,0,1),vgg.s_regs_conv[t]) + mom*prev_fc1\n",
    "        prev_fc1 = w_conv_1\n",
    "        vgg.fc1.weight.data -= l_r*(w_conv_1 + lamb*torch.abs(vgg.fc1.weight)) \n",
    "        w_1_out = torch.matmul(torch.transpose(du_out,0,1),vgg.fc_lif1.s_regs[t]) + mom*prev_fc_out\n",
    "        prev_fc_out = w_1_out\n",
    "        vgg.fc_out.weight.data -= l_r*(w_1_out + lamb*torch.abs(vgg.fc_out.weight))\n",
    "        \n",
    "        \n",
    "        dx_pool2 = torch.matmul(du_fc1,vgg.fc1.weight)\n",
    "        dx_pool2 = dx_pool2.view(dx_pool2.shape[0],128,pre_linear_dim,pre_linear_dim)\n",
    "        du_pool2 = vgg.unpool2(dx_pool2,vgg.pool2_ind[t])\n",
    "        ds_conv3 = du_pool2+vgg.conv_lif3.du_regs[t+1]*(-leak*vgg.conv_lif3.du_regs[t])\n",
    "        du_conv3 = ds_conv3*de_func(toy.conv_lif3.u_regs[t],th) + vgg.conv_lif3.du_regs[t+1]*leak*(1-vgg.conv_lif3.s_regs[t])\n",
    "        vgg.conv_lif3.du_regs[t] += du_conv3 \n",
    "        dW_conv3 = conv_weight_update(du_pool2.type(torch.float),vgg.conv_lif2.s_regs[t].type(torch.float),1) + mom*prev_conv3\n",
    "        prev_conv3 = dW_conv3\n",
    "#         dW_conv3 = torch.sum(dW_conv3,0)\n",
    "#         dW_conv3 = dW_conv3.view(128,64,dW_conv3.shape[-1],dW_conv3.shape[-1])\n",
    "        vgg.conv3.weight.data -=l_r*(dW_conv3 + lamb*torch.abs(vgg.conv3.weight))\n",
    "        \n",
    "        ds_conv2 = conv_dx_update(du_conv3,vgg.conv3.weight,'same')+vgg.conv_lif2.du_regs[t+1]*(-leak*vgg.conv_lif2.du_regs[t])\n",
    "        du_conv2 = ds_conv2*de_func(toy.conv_lif2.u_regs[t],th) + vgg.conv_lif2.du_regs[t+1]*leak*(1-vgg.conv_lif2.s_regs[t])\n",
    "        vgg.conv_lif2.du_regs[t] += du_conv2 \n",
    "        dW_conv2 = conv_weight_update(du_conv2.type(torch.float),F.max_pool2d(vgg.conv_lif1.s_regs[t].type(torch.float),kernel_size=2),1) + mom*prev_conv2\n",
    "        prev_conv2 = dW_conv2\n",
    "#         dW_conv2 = torch.sum(dW_conv2,0)\n",
    "#         dW_conv2 = dW_conv2.view(64,64,dW_conv2.shape[-1],dW_conv2.shape[-1])\n",
    "        vgg.conv2.weight.data -=l_r*(dW_conv2 + lamb*torch.abs(vgg.conv2.weight))\n",
    "        du_pool1 = vgg.unpool1(conv_dx_update(du_conv2,vgg.conv2.weight,'same'),vgg.pool1_ind[t])\n",
    "        \n",
    "        \n",
    "#         ds_conv1a = du_pool1+vgg.conv_lif1a.du_regs[t+1]*(-leak*vgg.conv_lif1a.du_regs[t])\n",
    "#         du_conv1a = ds_conv1a*de_func(toy.conv_lif1a.u_regs[t],th) + vgg.conv_lif1a.du_regs[t+1]*leak*(1-vgg.conv_lif1a.s_regs[t])\n",
    "#         vgg.conv_lif1a.du_regs[t] += du_conv1a\n",
    "#         dW_conv1a = conv_weight_update(du_pool1.type(torch.float),vgg.conv_lif1.s_regs[t].type(torch.float),1)\n",
    "#         dW_conv1a = torch.sum(dW_conv1a,0)\n",
    "#         dW_conv1a = dW_conv1a.view(64,32,dW_conv1a.shape[-1],dW_conv1a.shape[-1])\n",
    "#         vgg.conv1a.weight.data -=l_r*dW_conv1a\n",
    "        ds_conv1 = du_pool1 + vgg.conv_lif1.du_regs[t+1]*(-leak*vgg.conv_lif1.du_regs[t])\n",
    "        du_conv1 = ds_conv1*de_func(toy.conv_lif1.u_regs[t],th) + vgg.conv_lif1.du_regs[t+1]*leak*(1-vgg.conv_lif1.s_regs[t])\n",
    "        vgg.conv_lif1.du_regs[t] += du_conv1\n",
    "        dW_conv1 = conv_weight_update(du_conv1.type(torch.float),vgg.s_regs_inp[t].type(torch.float),1) + mom*prev_conv1\n",
    "        prev_conv1 = dW_conv1\n",
    "#         dW_conv1 = torch.sum(dW_conv1,0)\n",
    "#         dW_conv1 = dW_conv1.view(64,input_dim,dW_conv1.shape[-1],dW_conv1.shape[-1])\n",
    "        vgg.conv1.weight.data -=l_r*(dW_conv1 + lamb*torch.abs(vgg.conv1.weight))\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5802d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     du_pool2 = torch.sum(du_pool2,0)\n",
    "#     du_pool2 = torch.unsqueeze(du_pool2,1)\n",
    "    \n",
    "    ## Update du in conv3, time T\n",
    "#     d_conv3 = nn.Conv2d(128, 128, stride=1, kernel_size=f, padding=f-1, bias=False)\n",
    "    \n",
    "    ## Update weight in Conv3, time T\n",
    "#     f = du_pool2.shape[-1]\n",
    "#     d_conv3 = nn.Conv2d(128, 128, stride=1, padding=1, kernel_size=f, bias=False)\n",
    "#     d_conv3.weight.data = du_pool2.type(torch.float)\n",
    "#     dW_conv3 = d_conv3(vgg.conv_lif2.s_regs[-1].type(torch.float))\n",
    "#     dW_conv3 = torch.sum(dW_conv3,0)\n",
    "#     dW_conv3 = torch.unsqueeze(dW_conv3,1)\n",
    "#     vgg.conv3.weight.data -= l_r*dW_conv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7adf32d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_1(nn.Module):\n",
    "    def __init__(self,time_step,leak):\n",
    "        super(VGG_1, self).__init__()\n",
    "        \n",
    "        self.time_step = time_step\n",
    "        self.s_regs_inp = None\n",
    "        self.s_regs_conv = None\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1, bias=False)\n",
    "        \n",
    "#         self.deconv1 = nn.Conv2d()\n",
    "        self.lif_conv1 = LIF(time_step,leak)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2,return_indices=True)\n",
    "        self.pool1_ind = []\n",
    "        self.unpool1 = nn.MaxUnpool2d(kernel_size=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(16 * 14 * 14, 512, bias=False)\n",
    "        self.lif_fc1 = LIF(time_step,leak)\n",
    "        self.fc_out = nn.Linear(512, 10, bias=False)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "\n",
    "        size = inp.shape\n",
    "        self.s_regs_inp = torch.zeros(self.time_step,*size, device=device)\n",
    "        self.pool1_ind = []\n",
    "\n",
    "        u_out = 0\n",
    "        for t in range(self.time_step):\n",
    "            spike_inp = PoissonGen(inp)\n",
    "            self.s_regs_inp[t] += spike_inp\n",
    "            x = self.conv1(spike_inp)\n",
    "            x = self.lif_conv1(x,t)\n",
    "            x, indices = self.pool1(x)\n",
    "            x= x.view(x.shape[0],-1)\n",
    "            \n",
    "            if t == 0:\n",
    "                self.s_regs_conv = torch.zeros(self.time_step,*x.shape, device=device)\n",
    "            self.pool1_ind.append(indices)\n",
    "            self.s_regs_conv[t] += x\n",
    "            \n",
    "            x = self.fc1(x)\n",
    "            x = self.lif_fc1(x,t)\n",
    "            \n",
    "            x = self.fc_out(x)\n",
    "            u_out = u_out + x\n",
    "\n",
    "        return u_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31ffe4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_VGG1(vgg,leak,time_step,du_out,l_r,th):\n",
    "   \n",
    "    ## First fc\n",
    "    du_fc1 = torch.matmul(du_out,vgg.fc_out.weight)*de_func(vgg.lif_fc1.u_regs[-1],th)\n",
    "    vgg.lif_fc1.du_regs[-1] += du_fc1\n",
    "       \n",
    "    ## Update weight\n",
    "    w_conv_1 = torch.matmul(torch.transpose(du_fc1,0,1),vgg.s_regs_conv[-1])\n",
    "    vgg.fc1.weight.data -= l_r*w_conv_1\n",
    "     \n",
    "    w_1_out = torch.matmul(torch.transpose(du_out,0,1),vgg.lif_fc1.s_regs[-1])\n",
    "    vgg.fc_out.weight.data -= l_r*w_1_out\n",
    "    \n",
    "    dx_pool1 = torch.matmul(du_fc1,vgg.fc1.weight)\n",
    "    dx_pool1 = dx_pool1.view(dx_pool1.shape[0],16,14,14)\n",
    "    du_pool1 = vgg.unpool1(dx_pool1,vgg.pool1_ind[-1])\n",
    "    du_pool1 = torch.sum(du_pool1,0)\n",
    "    du_pool1 = torch.unsqueeze(du_pool1,1)\n",
    "    f = du_pool1.shape[-1]\n",
    "    d_conv1_w = nn.Conv2d(1, 16, stride=1, padding=1,kernel_size=f, bias=False)\n",
    "    d_conv1_w.weight.data = du_pool1.type(torch.float)\n",
    "    dW = d_conv1_w(vgg.s_regs_inp[-1].type(torch.float))\n",
    "    dW = torch.sum(dW,0)\n",
    "    dW = torch.unsqueeze(dW,1)\n",
    "\n",
    "    vgg.conv1.weight.data -= l_r*dW\n",
    "    \n",
    "    for t in range(time_step-2,-1,-1):\n",
    "        \n",
    "        ds_fc1 = torch.matmul(du_out,vgg.fc_out.weight)+vgg.lif_fc1.du_regs[t+1]*(-leak*vgg.lif_fc1.du_regs[t])\n",
    "        du_fc1 = (ds_fc1)*de_func(vgg.lif_fc1.du_regs[t],th) + vgg.lif_fc1.du_regs[t+1]*leak*(1-vgg.lif_fc1.s_regs[t])\n",
    "        vgg.lif_fc1.du_regs[t] += du_fc1\n",
    "        \n",
    "        w_conv_1 = torch.matmul(torch.transpose(du_fc1,0,1),vgg.s_regs_conv[t])\n",
    "        vgg.fc1.weight.data -= l_r*w_conv_1\n",
    "        \n",
    "        \n",
    "        dx_pool1 = torch.matmul(du_fc1,vgg.fc1.weight)\n",
    "        dx_pool1 = dx_pool1.view(dx_pool1.shape[0],16,14,14)\n",
    "        du_pool1 = vgg.unpool1(dx_pool1,vgg.pool1_ind[t])\n",
    "        du_pool1 = torch.sum(du_pool1,0)\n",
    "        du_pool1 = torch.unsqueeze(du_pool1,1)\n",
    "        f = du_pool1.shape[-1]\n",
    "        d_conv1_w = nn.Conv2d(1, 16, stride=1, padding=1,kernel_size=f, bias=False)\n",
    "        d_conv1_w.weight.data = du_pool1.type(torch.float)\n",
    "        dW = d_conv1_w(vgg.s_regs_inp[t].type(torch.float))\n",
    "        dW = torch.sum(dW,0)\n",
    "        dW = torch.unsqueeze(dW,1)\n",
    "\n",
    "        vgg.conv1.weight.data -= l_r*dW\n",
    "    \n",
    "    \n",
    "    return 0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e6f78f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIF(nn.Module):\n",
    "    def __init__(self, time_step,leak):\n",
    "        super(LIF, self).__init__()\n",
    "        \n",
    "        self.u_regs = None\n",
    "        self.du_regs = None\n",
    "        self.s_regs = None\n",
    "        self.leak = leak\n",
    "        self.time_step = time_step\n",
    "        self.thresh = 0.5\n",
    "        \n",
    "    def forward(self,inp,t):\n",
    "        \n",
    "#         print(\"memory before clear\",torch.cuda.memory_allocated())\n",
    "        if t == 0:\n",
    "            size = inp.shape\n",
    "            self.u_regs = torch.zeros(self.time_step,*size, device=device)\n",
    "            self.du_regs = torch.zeros(self.time_step,*size, device=device)\n",
    "#             err = torch.normal(0, 0.1,(1,1)).cuda()\n",
    "#             inp = inp + err\n",
    "#             self.u_regs[0] = quant(inp,2**4)\n",
    "            self.u_regs[0] = inp\n",
    "            self.s_regs = torch.zeros(self.time_step,*size, device=device)\n",
    "\n",
    "            spike = inp.gt(self.thresh).float()\n",
    "\n",
    "            self.s_regs[0] = spike\n",
    "            \n",
    "        else:\n",
    "#             err = torch.normal(0, 0.1,(1,1))\n",
    "#             inp = inp + err\n",
    "#             self.u_regs[t] = quant(self.leak * self.u_regs[t-1] * (1 - self.s_regs[t-1]) + (1-self.leak)*inp, 2**4)\n",
    "            self.u_regs[t] = self.leak*self.u_regs[t-1]*(1-self.s_regs[t-1]) + inp\n",
    "\n",
    "            spike = self.u_regs[t].gt(self.thresh).float()\n",
    "\n",
    "            self.s_regs[t] = spike\n",
    "            \n",
    "#         print(\"memory after clear\",torch.cuda.memory_allocated())\n",
    "#         torch.cuda.empty_cache()\n",
    "#         gc.collect()\n",
    "        return spike\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8fc2dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Back propagation for MLP\n",
    "def bp_MLP(toy,leak,time_step,du_out,s_regs_inp,l_r,th):\n",
    "    \n",
    "    ## First fc\n",
    "    du_fc1 = torch.matmul(du_out,toy.fc_out.weight)*de_func(toy.lif1.u_regs[-1],th)\n",
    "    toy.lif1.du_regs[-1] += du_fc1\n",
    "\n",
    "    ## Update weight\n",
    "    w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[-1])\n",
    "#     toy.fc_1.weight.data -= l_r*quant(w_inp_1,2**4)\n",
    "    toy.fc_1.weight.data -= l_r*w_inp_1\n",
    "    \n",
    "    w_1_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif1.s_regs[-1])\n",
    "#     toy.fc_out.weight.data -= l_r*quant(w_1_out,2**4)\n",
    "    toy.fc_out.weight.data -= l_r*w_1_out\n",
    "\n",
    "    for t in range(time_step-2,-1,-1):\n",
    "        \n",
    "        ## First fc\n",
    "        ds_fc1 = torch.matmul(du_out,toy.fc_out.weight)+toy.lif1.du_regs[t+1]*(-leak*toy.lif1.u_regs[t])\n",
    "        du_fc1 = (ds_fc1)*de_func(toy.lif1.u_regs[t],th) + toy.lif1.du_regs[t+1]*leak*(1-toy.lif1.s_regs[t])\n",
    "        toy.lif1.du_regs[t] += du_fc1\n",
    "\n",
    "        ## Update weight\n",
    "        w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[t])\n",
    "#         toy.fc_1.weight.data -= l_r*quant(w_inp_1,2**4)\n",
    "#         print(\"du_size\",du_fc1.shape)\n",
    "#         print(\"s_size\",s_regs_inp[t].shape)\n",
    "#         print(\"dweight shape\",w_inp_1.shape)\n",
    "        toy.fc_1.weight.data -= l_r*w_inp_1\n",
    "\n",
    "        w_1_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif1.s_regs[t])\n",
    "#         toy.fc_out.weight.data -= l_r*quant(w_1_out,2**4)\n",
    "        toy.fc_out.weight.data -= l_r*w_1_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b08084b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Back propagation\n",
    "def bp(toy,leak,time_step,du_out,s_regs_inp,l_r,th):\n",
    "    \n",
    "    ## Second fc    \n",
    "    du_fc2 = torch.matmul(du_out,toy.fc_out.weight)*de_func(toy.lif2.u_regs[-1],th)    \n",
    "    toy.lif2.du_regs[-1] = toy.lif2.du_regs[-1] + du_fc2\n",
    "    \n",
    "    ## First fc\n",
    "    du_fc1 = torch.matmul(du_fc2,toy.fc_2.weight)*de_func(toy.lif1.u_regs[-1],th)\n",
    "    toy.lif1.du_regs[-1] += du_fc1\n",
    "\n",
    "    \n",
    "    ## Update weight\n",
    "    w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[-1])\n",
    "    toy.fc_1.weight.data -= l_r*quant(w_inp_1,2**4)\n",
    "    #toy.fc_1.weight.data -= l_r*w_inp_1\n",
    "\n",
    "    w_1_2 = torch.matmul(torch.transpose(du_fc2,0,1),toy.lif1.s_regs[-1])\n",
    "    toy.fc_2.weight.data -= l_r*quant(w_1_2,2**4)\n",
    "    #toy.fc_2.weight.data -= l_r*w_1_2\n",
    "\n",
    "    w_2_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif2.s_regs[-1])\n",
    "    toy.fc_out.weight.data -= l_r*quant(w_2_out,2**4)\n",
    "    #toy.fc_out.weight.data -= l_r*w_2_out\n",
    "\n",
    "    for t in range(time_step-2,-1,-1):\n",
    "\n",
    "        ds_fc2 = torch.matmul(du_out,toy.fc_out.weight)+toy.lif2.du_regs[t+1]*(-leak*toy.lif2.u_regs[t])\n",
    "        du_fc2 = (ds_fc2)*de_func(toy.lif2.u_regs[t],th) + toy.lif2.du_regs[t+1]*leak*(1-toy.lif2.s_regs[t])\n",
    "        toy.lif2.du_regs[t] += du_fc2\n",
    "        \n",
    "        ## First fc\n",
    "        ds_fc1 = torch.matmul(du_fc2,toy.fc_2.weight)+toy.lif1.du_regs[t+1]*(-leak*toy.lif1.u_regs[t])\n",
    "        du_fc1 = (ds_fc1)*de_func(toy.lif1.u_regs[t],th) + toy.lif1.du_regs[t+1]*leak*(1-toy.lif1.s_regs[t])\n",
    "        toy.lif1.du_regs[t] += du_fc1\n",
    "\n",
    "        ## Update weight\n",
    "        w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[t])\n",
    "        toy.fc_1.weight.data -= l_r*quant(w_inp_1,2**4)\n",
    "        \n",
    "        #toy.fc_1.weight.data -= l_r*w_inp_1\n",
    "\n",
    "        w_1_2 = torch.matmul(torch.transpose(du_fc2,0,1),toy.lif1.s_regs[t])\n",
    "        toy.fc_2.weight.data -= l_r*quant(w_1_2,2**4)\n",
    "        #toy.fc_2.weight.data -= l_r*w_1_2\n",
    "\n",
    "        w_2_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif2.s_regs[t])\n",
    "        toy.fc_out.weight.data -= l_r*quant(w_2_out,2**4)\n",
    "        #toy.fc_out.weight.data -= l_r*w_2_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c3de0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "batch_size_train = 128\n",
    "batch_size_test = 1000\n",
    "\n",
    "train_loader_mnist = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./mnist', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_mnist = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./mnist', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edee34dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# import ssl\n",
    "\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# response = urllib.request.urlopen('https://www.python.org')\n",
    "# print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6839e4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./cifar10', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader_cifar10 = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./cifar10', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "test_loader_cifar10 = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b70c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3bfbdde-e623-4836-8e97-3bf5c7487a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sparsity(toy):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2ce9ca7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.404629\n",
      "Train Epoch: 0 [320/50000 (1%)]\tLoss: 2.215982\n",
      "Train Epoch: 0 [640/50000 (1%)]\tLoss: 2.305000\n",
      "Train Epoch: 0 [960/50000 (2%)]\tLoss: 2.121412\n",
      "Train Epoch: 0 [1280/50000 (3%)]\tLoss: 2.241268\n",
      "Train Epoch: 0 [1600/50000 (3%)]\tLoss: 1.959307\n",
      "Train Epoch: 0 [1920/50000 (4%)]\tLoss: 2.050189\n",
      "Train Epoch: 0 [2240/50000 (4%)]\tLoss: 2.166176\n",
      "Train Epoch: 0 [2560/50000 (5%)]\tLoss: 2.287834\n",
      "Train Epoch: 0 [2880/50000 (6%)]\tLoss: 2.168468\n",
      "Train Epoch: 0 [3200/50000 (6%)]\tLoss: 1.963343\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-dea4adb5fe5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mdu_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mbp_VGG5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleak\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdu_out\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmom\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlamb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpre_linear_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;31m#             print(\"memory after bp\",torch.cuda.memory_allocated()/10000000)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-f82f7f9f65f4>\u001b[0m in \u001b[0;36mbp_VGG5\u001b[1;34m(vgg, leak, time_step, du_out, l_r, mom, lamb, th, pre_linear_dim, input_dim)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;31m#         vgg.conv1a.weight.data -=l_r*dW_conv1a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0mds_conv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdu_pool1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_lif1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdu_regs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mleak\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_lif1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdu_regs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[0mdu_conv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_conv1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mde_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_lif1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mu_regs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_lif1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdu_regs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mleak\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_lif1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms_regs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m         \u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_lif1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdu_regs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdu_conv1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mdW_conv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_weight_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdu_conv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms_regs_inp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmom\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mprev_conv1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-6aa90361ba5f>\u001b[0m in \u001b[0;36mde_func\u001b[1;34m(U, th)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mde_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mU\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mU\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_step = 8\n",
    "leak = 0.99\n",
    "\n",
    "data = \"cifar10\"\n",
    "toy = VGG_5(time_step,leak,data).cuda()\n",
    "\n",
    "if data == \"cifar10\":\n",
    "    train_loader = train_loader_cifar10\n",
    "    input_dim = 3\n",
    "    pre_linear_dim = 8\n",
    "    test_loader = test_loader_cifar10\n",
    "elif data == \"mnist\":\n",
    "    train_loader = train_loader_mnist\n",
    "    input_dim = 1\n",
    "    pre_linear_dim = 7\n",
    "    test_loader = test_loader_mnist\n",
    "# vgg = VGG_5(time_step)\n",
    "# vgg =vgg.cuda()\n",
    "# print(\"weight\",toy.fc_1.weight)\n",
    "# torch.nn.init.normal_(toy.fc_1.weight, mean=0.0, std=0.1)\n",
    "# toy.fc_1.weight.data = quant(toy.fc_1.weight,2**4)\n",
    "# torch.nn.init.normal_(toy.fc_2.weight, mean=0.0, std=0.1)\n",
    "# toy.fc_2.weight.data = quant(toy.fc_2.weight,2**4)\n",
    "# torch.nn.init.normal_(toy.fc_out.weight, mean=0.0, std=0.1)\n",
    "# toy.fc_out.weight.data = quant(toy.fc_out.weight,2**4)\n",
    "# print(\"quantized weight\",toy.fc_1.weight)\n",
    "lr = 0.001\n",
    "mom = 0.9\n",
    "lamb = 0.0001\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# test(toy,data,test_loader)\n",
    "running_loss = 0.0\n",
    "for epoch in range(15):\n",
    "#     if epoch > 2:\n",
    "#         lr = 0.004\n",
    "#     if epoch > 4:\n",
    "#         lr = 0.002\n",
    "#     if epoch>8:\n",
    "#         lr = 0.001\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # print(torch.mean(data))\n",
    "        with torch.no_grad():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "            out = toy(data)\n",
    "#             print(\"memory after fwd\",torch.cuda.memory_allocated()/10000000)\n",
    "        out = Variable(out,requires_grad=True)\n",
    "\n",
    "        # err = loss(out,target,reduction='sum')\n",
    "#         L2_reg = torch.tensor(0.,requires_grad=True)\n",
    "#         for name, param in toy.named_parameters():\n",
    "#             if 'weight' in name:\n",
    "#                 L2_reg = L2_reg + torch.sum(param.pow(2)) / 2\n",
    "        err = F.cross_entropy(out, target,reduction='mean')\n",
    "        err.backward()\n",
    "\n",
    "        # exp = torch.exp(out)\n",
    "        # exp_sum = torch.sum(torch.exp(out),1, keepdim=True)   \n",
    "        # target = F.one_hot(target, num_classes=10)\n",
    "        # #L = -1*torch.sum((target*torch.log((exp/exp_sum))),1, keepdim=True)\n",
    "        # du_out = exp/exp_sum\n",
    "        # du_out = (du_out - target)/batch_size_train\n",
    "        du_out = out.grad\n",
    "\n",
    "        bp_VGG5(toy,leak,time_step,du_out,lr,mom,lamb,0.5,pre_linear_dim,input_dim)\n",
    "#             print(\"memory after bp\",torch.cuda.memory_allocated()/10000000)\n",
    "\n",
    "        # bp_MLP(toy,leak,time_step,du_out,toy.s_regs_inp,lr,0.5)\n",
    "\n",
    "\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), err.item()))\n",
    "            \n",
    "        # print statistics\n",
    "        # running_loss += err.item()\n",
    "        # if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "        #     print('[%d, %5d] loss: %.3f' %\n",
    "        #           (epoch + 1, i + 1, running_loss / 2000))\n",
    "        #     running_loss = 0.0\n",
    "\n",
    "#             del toy.lif_conv1.s_regs\n",
    "#             del toy.lif_conv1.u_regs\n",
    "#             del toy.lif_conv1.du_regs\n",
    "#             del toy.lif_fc1.du_regs\n",
    "#             del toy.lif_fc1.u_regs\n",
    "#             del toy.lif_fc1.s_regs\n",
    "#             del toy.s_regs_conv\n",
    "#             del toy.s_regs_inp\n",
    "#             del data\n",
    "#             del target\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#             gc.collect()\n",
    "#             print(\"memory after clear\",torch.cuda.memory_allocated()/10000000)\n",
    "\n",
    "    test(toy,data,test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ec3880-e283-4519-8552-3a502e0fae78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3842f727-73a7-4f8d-9f6d-44b90e602b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG_5(1,0.5,\"cifar10\").cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db95058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class test(nn.Module):\n",
    "#     def __init__(self,time_step, leak, data):\n",
    "#         super(test, self).__init__()\n",
    "        \n",
    "#         if data == \"cifar10\":\n",
    "#             input_dim = 3\n",
    "#             pre_linear_dim = 8\n",
    "#         elif data == \"mnist\":\n",
    "#             input_dim = 1\n",
    "#             pre_linear_dim = 7\n",
    "            \n",
    "#         self.conv1 = nn.Conv2d(input_dim, 64, kernel_size=3, padding=1, bias=False)\n",
    "#         self.conv_lif1 = LIF(time_step, leak)\n",
    "#         self.pool1 = nn.MaxPool2d(kernel_size=2,return_indices=True)\n",
    "#         self.pool1_ind = []\n",
    "#         self.unpool1 = nn.MaxUnpool2d(kernel_size=2)\n",
    "        \n",
    "#         self.fc1 = nn.Linear(128 * pre_linear_dim * pre_linear_dim, 1024, bias=False)\n",
    "#         self.fc_lif1 = LIF(time_step,leak)\n",
    "#         self.fc_out = nn.Linear(1024, 10, bias=False)\n",
    "        \n",
    "        \n",
    "#         def forward(self, inp):\n",
    "\n",
    "#         size = inp.shape\n",
    "#         self.s_regs_inp = torch.zeros(self.time_step,*size, device=device)\n",
    "#         self.pool1_ind = []\n",
    "#         u_out = 0\n",
    "        \n",
    "#         for t in range(self.time_step):\n",
    "#             spike_inp = PoissonGen(inp)\n",
    "#             self.s_regs_inp[t] += spike_inp \n",
    "#             x = self.conv1(spike_inp)\n",
    "#             x = self.conv_lif1(x,t)\n",
    "#             x,indices = self.pool1(x)\n",
    "#             self.pool1_ind.append(indices)\n",
    "#             x = x.view(x.shape[0],-1)\n",
    "            \n",
    "#             if t == 0:\n",
    "#                 self.s_regs_conv = torch.zeros(self.time_step,*x.shape, device=device)\n",
    "#             self.s_regs_conv[t] += x\n",
    "            \n",
    "#             x = self.fc1(x)\n",
    "#             x = self.fc_lif1(x,t)\n",
    "#             x = self.fc_out(x)\n",
    "#             u_out = u_out + x\n",
    "#         return u_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "7ed613b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(4,1,4,4,device=device,requires_grad=True)\n",
    "w = torch.randn(2,1,3,3,device=device,requires_grad=True)\n",
    "conv1 = F.conv2d(a,w,padding=1)\n",
    "# conv1 = Variable(conv1,requires_grad=True)\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, return_indices=True)\n",
    "unpool1 = nn.MaxUnpool2d(kernel_size=2)\n",
    "x,indices = pool1(conv1)\n",
    "# x = Variable(x,requires_grad=True)\n",
    "err = torch.rand_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "22aa40b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.backward(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "21c934e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "du_pool = unpool1(err,indices)\n",
    "# dw0 = conv_weight_update(torch.unsqueeze(du_pool[0],0),torch.unsqueeze(a[0],0),1)\n",
    "# dw1 = conv_weight_update(torch.unsqueeze(du_pool[1],0),torch.unsqueeze(a[1],0),1)\n",
    "# dw2 = conv_weight_update(torch.unsqueeze(du_pool[2],0),torch.unsqueeze(a[2],0),1)\n",
    "# dw3 = conv_weight_update(torch.unsqueeze(du_pool[3],0),torch.unsqueeze(a[3],0),1)\n",
    "# dw = (dw0+dw1+dw2+dw3)\n",
    "# dww = conv_weight_update(du_pool,a,1)\n",
    "dx = conv_dx_update(du_pool,w,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "3b9959e1-73b8-401f-adb8-8793fddb8bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 4, 4])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "du_pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "65288bf9-0bfe-49ce-b13c-abc64b047c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[2.6056, 3.2645, 3.6455],\n",
       "          [2.7099, 1.5400, 3.5414],\n",
       "          [3.5173, 3.7435, 3.5242]]],\n",
       "\n",
       "\n",
       "        [[[3.1123, 3.0722, 2.8452],\n",
       "          [3.2613, 3.9904, 6.9552],\n",
       "          [2.5672, 3.7917, 4.1968]]]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "3d8bda84-b111-4995-8550-ca2b24319b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[2.6056, 3.2645, 3.6455],\n",
       "          [2.7099, 1.5400, 3.5414],\n",
       "          [3.5173, 3.7435, 3.5242]]],\n",
       "\n",
       "\n",
       "        [[[3.1123, 3.0722, 2.8452],\n",
       "          [3.2613, 3.9904, 6.9552],\n",
       "          [2.5672, 3.7917, 4.1968]]]], device='cuda:0')"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "1a17286e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3842e-07, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(dw-w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "7a00016e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4342e-07, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(dx-a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "102ed3fc-f159-48b3-acaa-0f87c6056912",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.ones(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "1f0938dd-044a-4df9-81f8-5c8bdfea4b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "c[1] = c[1]/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "fd058f84-2ce2-4427-9930-1434ba6c96f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(4,2,4,4,device=device,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "29b07c18-13fd-45c8-b09b-44632f6005a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 4, 4])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "c8b74aa8-d5f5-4332-b70b-5669cae97c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(1,8,4,4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040a2245-05a0-4906-9075-227b89403c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.view"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
