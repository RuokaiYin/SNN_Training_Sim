{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251e3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b07e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33e21ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PoissonGen(inp, rescale_fac=2.0):\n",
    "    rand_inp = torch.rand_like(inp)\n",
    "    return torch.mul(torch.le(rand_inp * rescale_fac, torch.abs(inp)).float(), torch.sign(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d766c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(28*28,6,bias=False)\n",
    "        self.fc_2 = nn.Linear(6,4,bias=False)\n",
    "        self.fc_out = nn.Linear(4,10,bias=False)\n",
    "        self.lif1 = LIF()\n",
    "        self.lif2 = LIF()\n",
    "        self.time_step = 11\n",
    "        self.s_regs_inp = None\n",
    "        \n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m,nn.Linear):\n",
    "#                 n = m.weight.size(1)\n",
    "#                 m.weight.data.normal_(0,5.0/float(n))\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        inp = inp.view(inp.shape[0],-1)\n",
    "        size = inp.shape\n",
    "        self.s_regs_inp = torch.zeros(self.time_step,*size)\n",
    "        u_out = 0\n",
    "        \n",
    "        for t in range(self.time_step):\n",
    "            \n",
    "            spike_inp = PoissonGen(inp)\n",
    "            self.s_regs_inp[t].data += spike_inp \n",
    "            \n",
    "            x = self.fc_1(spike_inp)\n",
    "            x = self.lif1(x, t)\n",
    "            x = self.fc_2(x)\n",
    "            x = self.lif2(x, t)\n",
    "            x = self.fc_out(x)\n",
    "            u_out = u_out + x\n",
    "        return u_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e6f78f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LIF, self).__init__()\n",
    "        \n",
    "        self.u_regs = None\n",
    "        self.du_regs = None\n",
    "        self.s_regs = None\n",
    "        self.leak = 0.8\n",
    "        self.time_step = 20\n",
    "        self.thresh = 0.5\n",
    "        \n",
    "    def forward(self,inp,t):\n",
    "        if t == 0:\n",
    "            size = inp.shape\n",
    "            self.u_regs = torch.zeros(self.time_step,*size)\n",
    "            self.du_regs = torch.zeros(self.time_step,*size)\n",
    "            self.u_regs[0] = inp\n",
    "            self.s_regs = torch.zeros(self.time_step,*size)\n",
    "\n",
    "            vol = inp - self.thresh\n",
    "\n",
    "            spike = spike_function(vol, k=1)\n",
    "\n",
    "            self.s_regs[0] = spike\n",
    "        else:\n",
    "            self.u_regs[t] = self.leak * self.u_regs[t-1] * (1 - self.s_regs[t-1]) + inp\n",
    "\n",
    "            vol = self.u_regs[t] - self.thresh\n",
    "\n",
    "            spike = spike_function(vol, k=1)\n",
    "\n",
    "            self.s_regs[t] = spike\n",
    "\n",
    "        return spike\n",
    "\n",
    "        \n",
    "\n",
    "def spike_function(x, k):\n",
    "\n",
    "    x[x>0] = 1\n",
    "    x[x<=0] = 0\n",
    "\n",
    "    return x\n",
    "\n",
    "def de_func(U,th):\n",
    "    alpha = 0.3\n",
    "    U = alpha*(1.0 - abs((U-th)/th))\n",
    "    U[U<0]=0\n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08084b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Back propagation\n",
    "def bp(toy,leak,time_step,du_out,s_regs_inp,l_r,th):\n",
    "    \n",
    "    ## Second fc    \n",
    "    du_fc2 = torch.matmul(du_out,toy.fc_out.weight)*de_func(toy.lif2.u_regs[-1],th)    \n",
    "    toy.lif2.du_regs[-1].data += du_fc2\n",
    "\n",
    "    ## First fc\n",
    "    du_fc1 = torch.matmul(du_fc2,toy.fc_2.weight)*de_func(toy.lif1.u_regs[-1],th)\n",
    "    toy.lif1.du_regs[-1].data += du_fc1\n",
    "\n",
    "    ## Update weight\n",
    "    w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[-1])\n",
    "    toy.fc_1.weight.data += w_inp_1\n",
    "\n",
    "    w_1_2 = torch.matmul(torch.transpose(du_fc2,0,1),toy.lif1.s_regs[-1])\n",
    "    toy.fc_2.weight.data += w_1_2\n",
    "\n",
    "    w_2_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif2.s_regs[-1])\n",
    "    toy.fc_out.weight.data += w_2_out\n",
    "\n",
    "    for t in range(time_step-2,-1,-1):\n",
    "        ds_fc2 = torch.matmul(du_out,toy.fc_out.weight)+toy.lif2.du_regs[t+1]*(-leak*toy.lif2.u_regs[t])\n",
    "        du_fc2 = (ds_fc2)*de_func(toy.lif2.u_regs[t],th) + toy.lif2.du_regs[t+1]*leak*(1-toy.lif2.s_regs[t])\n",
    "        toy.lif2.du_regs[t].data += du_fc2\n",
    "        \n",
    "        ## First fc\n",
    "        ds_fc1 = torch.matmul(du_fc2,toy.fc_2.weight)+toy.lif1.du_regs[t+1]*(-leak*toy.lif1.u_regs[t])\n",
    "        du_fc1 = (ds_fc1)*de_func(toy.lif1.u_regs[t],th) + toy.lif1.du_regs[t+1]*leak*(1-toy.lif1.s_regs[t])\n",
    "        toy.lif1.du_regs[t].data += du_fc1\n",
    "\n",
    "        ## Update weight\n",
    "\n",
    "        w_inp_1 = torch.matmul(torch.transpose(du_fc1,0,1),s_regs_inp[t])\n",
    "        toy.fc_1.weight.data -= l_r*w_inp_1\n",
    "\n",
    "        w_1_2 = torch.matmul(torch.transpose(du_fc2,0,1),toy.lif1.s_regs[t])\n",
    "        toy.fc_2.weight.data -= l_r*w_1_2\n",
    "\n",
    "        w_2_out = torch.matmul(torch.transpose(du_out,0,1),toy.lif2.s_regs[t])\n",
    "        toy.fc_out.weight.data -= l_r*w_2_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3c7c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47609ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(10 + 1)]\n",
    "log_interval = 10\n",
    "\n",
    "def test():\n",
    "  \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "      output = toy(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2ce9ca7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ruokai\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: -0.0283, Accuracy: 962/10000 (10%)\n",
      "\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.492788\n",
      "Train Epoch: 0 [640/60000 (1%)]\tLoss: 2.311037\n",
      "Train Epoch: 0 [1280/60000 (2%)]\tLoss: 2.228962\n",
      "Train Epoch: 0 [1920/60000 (3%)]\tLoss: 2.054328\n",
      "Train Epoch: 0 [2560/60000 (4%)]\tLoss: 2.098115\n",
      "Train Epoch: 0 [3200/60000 (5%)]\tLoss: 1.951510\n",
      "Train Epoch: 0 [3840/60000 (6%)]\tLoss: 1.774833\n",
      "Train Epoch: 0 [4480/60000 (7%)]\tLoss: 1.814049\n",
      "Train Epoch: 0 [5120/60000 (9%)]\tLoss: 1.730332\n",
      "Train Epoch: 0 [5760/60000 (10%)]\tLoss: 1.831321\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.569038\n",
      "Train Epoch: 0 [7040/60000 (12%)]\tLoss: 1.485887\n",
      "Train Epoch: 0 [7680/60000 (13%)]\tLoss: 1.373265\n",
      "Train Epoch: 0 [8320/60000 (14%)]\tLoss: 1.439399\n",
      "Train Epoch: 0 [8960/60000 (15%)]\tLoss: 1.437452\n",
      "Train Epoch: 0 [9600/60000 (16%)]\tLoss: 1.561408\n",
      "Train Epoch: 0 [10240/60000 (17%)]\tLoss: 1.534433\n",
      "Train Epoch: 0 [10880/60000 (18%)]\tLoss: 1.252344\n",
      "Train Epoch: 0 [11520/60000 (19%)]\tLoss: 1.357207\n",
      "Train Epoch: 0 [12160/60000 (20%)]\tLoss: 1.323164\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.107006\n",
      "Train Epoch: 0 [13440/60000 (22%)]\tLoss: 1.421125\n",
      "Train Epoch: 0 [14080/60000 (23%)]\tLoss: 1.246050\n",
      "Train Epoch: 0 [14720/60000 (25%)]\tLoss: 1.427297\n",
      "Train Epoch: 0 [15360/60000 (26%)]\tLoss: 1.544367\n",
      "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 0.968360\n",
      "Train Epoch: 0 [16640/60000 (28%)]\tLoss: 1.314468\n",
      "Train Epoch: 0 [17280/60000 (29%)]\tLoss: 1.086813\n",
      "Train Epoch: 0 [17920/60000 (30%)]\tLoss: 1.290764\n",
      "Train Epoch: 0 [18560/60000 (31%)]\tLoss: 1.063393\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.159100\n",
      "Train Epoch: 0 [19840/60000 (33%)]\tLoss: 1.088775\n",
      "Train Epoch: 0 [20480/60000 (34%)]\tLoss: 0.859412\n",
      "Train Epoch: 0 [21120/60000 (35%)]\tLoss: 1.001863\n",
      "Train Epoch: 0 [21760/60000 (36%)]\tLoss: 1.026959\n",
      "Train Epoch: 0 [22400/60000 (37%)]\tLoss: 1.122520\n",
      "Train Epoch: 0 [23040/60000 (38%)]\tLoss: 0.987905\n",
      "Train Epoch: 0 [23680/60000 (39%)]\tLoss: 1.104184\n",
      "Train Epoch: 0 [24320/60000 (41%)]\tLoss: 0.848932\n",
      "Train Epoch: 0 [24960/60000 (42%)]\tLoss: 1.094928\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.922591\n",
      "Train Epoch: 0 [26240/60000 (44%)]\tLoss: 0.781541\n",
      "Train Epoch: 0 [26880/60000 (45%)]\tLoss: 0.993298\n",
      "Train Epoch: 0 [27520/60000 (46%)]\tLoss: 1.327654\n",
      "Train Epoch: 0 [28160/60000 (47%)]\tLoss: 1.200692\n",
      "Train Epoch: 0 [28800/60000 (48%)]\tLoss: 1.207149\n",
      "Train Epoch: 0 [29440/60000 (49%)]\tLoss: 0.930751\n",
      "Train Epoch: 0 [30080/60000 (50%)]\tLoss: 1.047147\n",
      "Train Epoch: 0 [30720/60000 (51%)]\tLoss: 0.970101\n",
      "Train Epoch: 0 [31360/60000 (52%)]\tLoss: 1.015324\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.017886\n",
      "Train Epoch: 0 [32640/60000 (54%)]\tLoss: 1.240941\n",
      "Train Epoch: 0 [33280/60000 (55%)]\tLoss: 1.163251\n",
      "Train Epoch: 0 [33920/60000 (57%)]\tLoss: 0.994125\n",
      "Train Epoch: 0 [34560/60000 (58%)]\tLoss: 0.853954\n",
      "Train Epoch: 0 [35200/60000 (59%)]\tLoss: 1.323262\n",
      "Train Epoch: 0 [35840/60000 (60%)]\tLoss: 1.076500\n",
      "Train Epoch: 0 [36480/60000 (61%)]\tLoss: 1.372771\n",
      "Train Epoch: 0 [37120/60000 (62%)]\tLoss: 0.992830\n",
      "Train Epoch: 0 [37760/60000 (63%)]\tLoss: 1.109416\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.929765\n",
      "Train Epoch: 0 [39040/60000 (65%)]\tLoss: 1.271299\n",
      "Train Epoch: 0 [39680/60000 (66%)]\tLoss: 0.979101\n",
      "Train Epoch: 0 [40320/60000 (67%)]\tLoss: 1.108644\n",
      "Train Epoch: 0 [40960/60000 (68%)]\tLoss: 1.118564\n",
      "Train Epoch: 0 [41600/60000 (69%)]\tLoss: 1.095181\n",
      "Train Epoch: 0 [42240/60000 (70%)]\tLoss: 1.053793\n",
      "Train Epoch: 0 [42880/60000 (71%)]\tLoss: 0.837144\n",
      "Train Epoch: 0 [43520/60000 (72%)]\tLoss: 1.303130\n",
      "Train Epoch: 0 [44160/60000 (74%)]\tLoss: 1.004434\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.285479\n",
      "Train Epoch: 0 [45440/60000 (76%)]\tLoss: 1.468545\n",
      "Train Epoch: 0 [46080/60000 (77%)]\tLoss: 1.169209\n",
      "Train Epoch: 0 [46720/60000 (78%)]\tLoss: 1.051917\n",
      "Train Epoch: 0 [47360/60000 (79%)]\tLoss: 1.079878\n",
      "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 1.019726\n",
      "Train Epoch: 0 [48640/60000 (81%)]\tLoss: 0.867688\n",
      "Train Epoch: 0 [49280/60000 (82%)]\tLoss: 0.796230\n",
      "Train Epoch: 0 [49920/60000 (83%)]\tLoss: 1.038409\n",
      "Train Epoch: 0 [50560/60000 (84%)]\tLoss: 1.051859\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.064998\n",
      "Train Epoch: 0 [51840/60000 (86%)]\tLoss: 0.887529\n",
      "Train Epoch: 0 [52480/60000 (87%)]\tLoss: 1.350520\n",
      "Train Epoch: 0 [53120/60000 (88%)]\tLoss: 0.973964\n",
      "Train Epoch: 0 [53760/60000 (90%)]\tLoss: 1.483448\n",
      "Train Epoch: 0 [54400/60000 (91%)]\tLoss: 1.208227\n",
      "Train Epoch: 0 [55040/60000 (92%)]\tLoss: 1.036138\n",
      "Train Epoch: 0 [55680/60000 (93%)]\tLoss: 1.124592\n",
      "Train Epoch: 0 [56320/60000 (94%)]\tLoss: 0.860240\n",
      "Train Epoch: 0 [56960/60000 (95%)]\tLoss: 1.164110\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.056865\n",
      "Train Epoch: 0 [58240/60000 (97%)]\tLoss: 1.169818\n",
      "Train Epoch: 0 [58880/60000 (98%)]\tLoss: 0.788573\n",
      "Train Epoch: 0 [59520/60000 (99%)]\tLoss: 1.139137\n",
      "err: tensor(1.0124, grad_fn=<NllLossBackward>)\n",
      "\n",
      "Test set: Avg. loss: -4.3779, Accuracy: 6141/10000 (61%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.012280\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.968037\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.047292\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 1.107785\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.986104\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.935953\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.976022\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.809516\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.028655\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.810918\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.113358\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.980844\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.045832\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.090966\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.893947\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.848811\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.834163\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.864461\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.074199\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.931750\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.568580\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.868992\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.954025\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.061708\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.668799\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.959816\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.791774\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 1.071404\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.942501\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 1.287078\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.911102\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 1.238522\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.137629\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.832730\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.796817\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 1.174217\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.739221\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.600477\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.904213\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.802398\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.962455\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.633412\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.916551\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.875970\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.862589\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.900266\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.884040\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.955884\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 1.149233\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.799543\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.859419\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.669155\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.633371\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.695711\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.618923\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.964586\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.797637\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.598111\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.605415\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.728633\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.537886\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.756823\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.642736\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 1.332814\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.785098\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.799772\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.610739\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 1.100168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.891258\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 1.149889\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.964901\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.706216\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.687149\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.587199\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 1.148082\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.845749\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 1.042078\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.702906\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 1.048540\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.638946\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.739415\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.919763\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 1.163134\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.794037\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.733852\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.825500\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.700078\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.544064\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 1.071521\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 1.047334\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.565929\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.731851\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.744112\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.760983\n",
      "err: tensor(0.9408, grad_fn=<NllLossBackward>)\n",
      "\n",
      "Test set: Avg. loss: -6.2335, Accuracy: 7762/10000 (78%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.603885\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 1.267984\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.819702\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.554677\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.599281\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.844759\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.687818\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.625687\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.785115\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.539876\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.757115\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 1.216045\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.752143\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.746216\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.590604\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.660394\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.762448\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.583729\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.927800\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 1.073762\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.591135\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.918557\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.857302\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.663628\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.667299\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.561399\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.478909\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.597517\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.646370\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.607357\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.561618\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.504596\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.867520\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.738728\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.760541\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.592279\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.724106\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.678650\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.789361\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.811187\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.576712\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 1.015985\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.804242\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.676619\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.670568\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 1.052229\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.983992\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.604831\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.674437\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.636548\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.532882\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.795357\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 1.113866\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.767886\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.649416\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.691435\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.795641\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.444104\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.723031\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 1.115562\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.652623\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.532499\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.739210\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.537147\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.828337\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.477622\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.474671\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.621753\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.960627\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.775533\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.579867\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.632520\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.826389\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.819093\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.959645\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.821593\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.852545\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.809786\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.988374\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.625011\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.843069\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.680148\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.751497\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.631107\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.689068\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.627184\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.849322\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.434126\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.550710\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.838976\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.614001\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.488054\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.837516\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.448485\n",
      "err: tensor(0.4236, grad_fn=<NllLossBackward>)\n",
      "\n",
      "Test set: Avg. loss: -6.4402, Accuracy: 8006/10000 (80%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.627286\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.632394\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.839824\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.780574\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.892940\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.783289\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.466889\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.931865\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.747109\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.691913\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.768017\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.713567\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.744216\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.577948\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.849125\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.764781\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.925531\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.585015\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.732736\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.586317\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.576732\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.954027\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.699015\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.649298\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.852886\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.711183\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.653461\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.668914\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.761366\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.653959\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.485067\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.781272\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.638510\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.712604\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.881394\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.761052\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.366074\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.676686\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.510095\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.692365\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.715764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.925632\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.725154\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.685832\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.620466\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.533664\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.478068\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.729895\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.519451\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.466126\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.482456\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 1.088239\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.830299\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.778499\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 1.032915\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.751515\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.610946\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.594293\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.693250\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.486782\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.717883\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.620086\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.840334\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.415381\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.394156\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.664583\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.666459\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.898686\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.550625\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.459482\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.713367\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.557944\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.746672\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.867791\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.372891\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.575660\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.771701\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.531251\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.879292\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.978066\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.933651\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.457100\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.708381\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.516562\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.524813\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.708260\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.767959\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.846848\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.738810\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.573099\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.890410\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.481479\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.703566\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.705819\n",
      "err: tensor(0.5681, grad_fn=<NllLossBackward>)\n",
      "\n",
      "Test set: Avg. loss: -6.3908, Accuracy: 7948/10000 (79%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.798589\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.771296\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.518541\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.514261\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.512655\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.697811\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.594837\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.548768\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.653015\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.669271\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.704046\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.538730\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.588331\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.761978\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 1.231016\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.762934\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.580499\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.584442\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.851362\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.880136\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.728478\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.902267\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.578971\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.594172\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.777491\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.596066\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.552375\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.882670\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.720120\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.705894\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.669050\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.812633\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.898884\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.542934\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.634454\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.740833\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.375635\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.719882\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.936345\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.666163\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.948285\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.379502\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.869006\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.631970\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.717525\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.452902\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.811545\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.626203\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.646003\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.947937\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.368154\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.588943\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.574529\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.794124\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.408010\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.783495\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.570206\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.835916\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.718383\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.596689\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.678706\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.640174\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.572806\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.597057\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.613338\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.657614\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.798040\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.671019\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.731690\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.494061\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.585036\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.666275\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.624192\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.620210\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.752982\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.404287\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.585789\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.731147\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.759988\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.673961\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.657740\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.686588\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.789068\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.722555\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.614603\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.457393\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.771127\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.611927\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.621796\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.791520\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.840047\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.734637\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.865422\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.764519\n",
      "err: tensor(0.4522, grad_fn=<NllLossBackward>)\n",
      "\n",
      "Test set: Avg. loss: -6.2469, Accuracy: 8060/10000 (81%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.568526\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.565344\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.897870\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.878974\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.624696\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.762307\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.842086\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.367565\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.447722\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.540709\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.576743\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.717118\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.351831\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.631050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.589673\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.839469\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.623544\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.503080\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.426878\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.730309\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.659384\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.861463\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.600604\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.413244\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.554226\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.754626\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.646560\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.740247\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.584524\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.711029\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.700689\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.643275\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.923715\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.855866\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.895229\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.955323\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.437773\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.593318\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.460003\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.640945\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.817477\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.447147\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.884157\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 1.014890\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.764884\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.529589\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.650580\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.494785\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.872440\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.790024\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.816927\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.893252\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.593001\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.906806\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.635597\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.904953\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.830452\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.459516\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.693397\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.849229\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.914472\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.882720\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.741406\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.804797\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.560674\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.623562\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.784209\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.714345\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.647837\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.582197\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.772659\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.501943\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.557127\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.508908\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.494817\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.578498\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.704948\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 1.115827\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.780560\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.669268\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.857618\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.577197\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.703129\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.620354\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.864552\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.732091\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.521513\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.955557\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.933705\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.607057\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.840901\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.788636\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.526858\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.861138\n",
      "err: tensor(0.6169, grad_fn=<NllLossBackward>)\n",
      "\n",
      "Test set: Avg. loss: -6.3897, Accuracy: 7910/10000 (79%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.855258\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.443371\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.887778\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.837358\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.459530\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.438576\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.432302\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.526914\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.434074\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.555773\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.447205\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.435028\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.563841\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.945749\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.560315\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.405492\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.480846\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.729161\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.582220\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.560685\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.536042\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.462094\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.953268\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.839161\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.652989\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.710097\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.481589\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.656166\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.852907\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.416614\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 1.000750\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.598787\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.730180\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.413712\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.666196\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.633581\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.788416\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.601227\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.652955\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.789100\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.749199\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.616901\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.799780\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.428754\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.728625\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.608473\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.538252\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.936881\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.702688\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.613683\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.752729\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.649844\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.791081\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.545005\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.622981\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.898280\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.675832\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.710626\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.535344\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.644885\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.666356\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.459612\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.637645\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.839393\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.637326\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.673087\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.366212\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.531734\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.527122\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.590617\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.600191\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.667591\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.824534\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.779882\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.427810\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.609272\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.357460\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.619150\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.755778\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.535534\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.813482\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.361198\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.890503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.431515\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.639261\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.798065\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.594399\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.577092\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.790812\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.965719\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.437971\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.453263\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.673875\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.490404\n",
      "err: tensor(0.4894, grad_fn=<NllLossBackward>)\n",
      "\n",
      "Test set: Avg. loss: -6.4254, Accuracy: 8141/10000 (81%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.627609\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.528418\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 1.110731\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.642094\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.714544\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.658243\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.677901\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.544499\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.655060\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.791425\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.542757\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.567440\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.572526\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.990706\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.577125\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.560090\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.306386\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.472702\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.817227\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.580770\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.637697\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.763528\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.945861\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.740261\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.488189\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.768145\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.903672\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.692647\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.498680\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.704093\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.502617\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.739082\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.982854\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.813656\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.706254\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.640912\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.669774\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.572589\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.580739\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.492374\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.541240\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.565931\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.630480\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.704523\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.761204\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.604293\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.547035\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.646731\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.651668\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.614788\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.583475\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.507146\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.598299\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.721445\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.638391\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.721201\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.407578\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.729881\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.923588\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.614760\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.516110\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.300790\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.565391\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.650031\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.855632\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.910890\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.841392\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.560095\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.593428\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.503392\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.560727\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.707584\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.667872\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.738404\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.681156\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.563568\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.893652\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.734143\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.806879\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.525160\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.701699\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.523024\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.629253\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 1.023112\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.709515\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.878033\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.940999\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.708863\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.837435\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.586412\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.600774\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.575911\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.807359\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.933331\n",
      "err: tensor(0.3461, grad_fn=<NllLossBackward>)\n",
      "\n",
      "Test set: Avg. loss: -6.0381, Accuracy: 8079/10000 (81%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.533586\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.605817\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.676305\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.654299\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.642254\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.695421\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.750966\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.419114\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.633981\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.573543\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.647852\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.543807\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.615612\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.883488\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.482947\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 1.187575\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.652669\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.580282\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.748541\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.675469\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.665903\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.621809\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.432314\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.826836\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.977344\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.459044\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.814896\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.799456\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.939425\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.493427\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.709816\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.441241\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.624536\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.466120\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.488378\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.819808\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.721095\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.668069\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 1.022535\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.535112\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.498003\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.474745\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.637095\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.674111\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.522764\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.541064\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.754617\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 1.066639\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.953543\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.760338\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.865977\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.486143\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 1.129031\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.671260\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.619237\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.592265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.866317\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.580645\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.741342\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.884768\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.814571\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.544525\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.638941\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.681608\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.955467\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.552356\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.574248\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.512479\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.913130\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.756427\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.611737\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.787470\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.613218\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.592552\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.900148\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.599045\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.587959\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.539973\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.548861\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.586901\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.746420\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 1.129458\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.964832\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.587613\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.732963\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.923617\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.581422\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.825194\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.898877\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.387102\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.696224\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.783940\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 1.110226\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.822668\n",
      "err: tensor(0.5651, grad_fn=<NllLossBackward>)\n",
      "\n",
      "Test set: Avg. loss: -5.9685, Accuracy: 8105/10000 (81%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.644382\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.816015\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.801787\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.510430\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.562063\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.393015\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.568958\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.836235\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.814136\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.518551\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.739739\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.571157\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.588994\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.910121\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.666588\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.616649\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.719148\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 1.037163\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.823140\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.667130\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.708739\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.794469\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.534490\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.928488\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.567131\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.438157\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.418139\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.543178\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.690110\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.671213\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.710163\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.730748\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.990649\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 1.217154\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.888137\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.825619\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.549156\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.588039\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.907869\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.814057\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.594051\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.777827\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.910719\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.655093\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.521821\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.798633\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 1.095743\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.659315\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.762731\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.653169\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 1.016856\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.840104\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.597132\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.625382\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.503245\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.520977\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.681262\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.506711\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.547814\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.646577\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.840313\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.492653\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.387170\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.799861\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 1.053286\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.816662\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.549153\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.505505\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.699160\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.821610\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.746836\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.859791\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.689392\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.843295\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 1.031833\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.577830\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.891298\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.590364\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.600634\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.730399\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.708760\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.652619\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.655234\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.527188\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.439366\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.398984\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.616592\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.590151\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.607572\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.947295\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.785769\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.752541\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.506685\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.805100\n",
      "err: tensor(0.8591, grad_fn=<NllLossBackward>)\n",
      "\n",
      "Test set: Avg. loss: -6.4707, Accuracy: 8144/10000 (81%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toy = model()\n",
    "leak = 0.99\n",
    "time_step = 10\n",
    "lr = 0.001\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "test()\n",
    "for epoch in range(10):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        out = toy(data)\n",
    "#         if torch.isnan(out).any():\n",
    "#             print(\"Nan Debug found, out,i=\",batch_idx)\n",
    "            \n",
    "        err = loss(out,target)\n",
    "        exp = torch.exp(out)\n",
    "        exp_sum = torch.sum(torch.exp(out),1, keepdim=True)\n",
    "        target = F.one_hot(target, num_classes=10)\n",
    "        L = -1*torch.sum((target*torch.log((exp/exp_sum))),1, keepdim=True)\n",
    "        \n",
    "        \n",
    "        du_out = exp/exp_sum\n",
    "        du_out = du_out - target\n",
    "        \n",
    "        #print(\"out is:\",out)\n",
    "#         if torch.isnan(du_out).any():\n",
    "#             if torch.isnan(torch.exp(out)).any():\n",
    "#                 print(\"exp has nan\")\n",
    "#             if torch.isnan(target).any():\n",
    "#                 print(\"target has nan\")\n",
    "#             print(\"Nan Debug found, du_out,bp,i=\",batch_idx)\n",
    "#             print(\"du_out: \",du_out)\n",
    "#             print(\"exp sum:\",torch.sum(torch.exp(out),1, keepdim=True))\n",
    "#             print(\"exp:\",torch.exp(out))\n",
    "#             print(\"out\",out)\n",
    "            \n",
    "        \n",
    "        bp(toy,leak,time_step,du_out,toy.s_regs_inp,lr,toy.lif1.thresh)\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), err.item()))\n",
    "            train_losses.append(err.item())\n",
    "            train_counter.append((batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "        #print(\"error is: \", err) \n",
    "#         if torch.isnan(err):\n",
    "#             print(\"Nan is found, epoch\",epoch,\"i is\",batch_idx)\n",
    "#             break\n",
    "    print(\"err:\", err)\n",
    "    test()\n",
    "    if torch.isnan(err):\n",
    "        break\n",
    "#l = -1*u_out[0][1]+torch.log(torch.exp(u_out[0][0])+torch.exp(u_out[0][1]))\n",
    "\n",
    "# print(\"err:\",err)\n",
    "# print(\"u_out:\",u_out)\n",
    "#torch.exp(u_out[0][0])/(torch.exp(u_out[0][0])+torch.exp(u_out[0][1])+torch.exp(u_out[0][2]))-0\n",
    "#print(err)\n",
    "# toy.lif2.u_regs.register_hook(print)\n",
    "# err.backward()\n",
    "\n",
    "## Use same Output derivative throught bp\n",
    "\n",
    "\n",
    "#print(du_out)\n",
    "#print(toy.fc_out.weight)\n",
    "\n",
    "#print(toy.fc_out.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73dbb2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'negative log likelihood loss')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea663ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a72fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
